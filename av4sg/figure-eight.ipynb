{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from flow.core.params import (\n",
    "    NetParams,\n",
    "    InitialConfig,\n",
    "    SumoParams,\n",
    "    EnvParams,\n",
    "    VehicleParams,\n",
    ")\n",
    "from flow.controllers import IDMController, ContinuousRouter, RLController\n",
    "from flow.networks.figure_eight import FigureEightNetwork, ADDITIONAL_NET_PARAMS\n",
    "from flow.envs.ring.accel import AccelEnv, ADDITIONAL_ENV_PARAMS\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray.rllib.algorithms.ppo import DEFAULT_CONFIG\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define horizon as a variable to ensure consistent use across notebook\n",
    "HORIZON = 100\n",
    "N_WORKERS = 9  # 1 - number of CPUS\n",
    "N_ROLLOUTS = 1\n",
    "# The algorithm or model to train. This may refer to the name of a built-on\n",
    "# algorithm (e.g. RLLib's DQN or PPO), or a user-defined trainable function or\n",
    "# class registered in the tune registry.)\n",
    "RL_ALG_NAME = \"PPO\"\n",
    "EXP_NAME = \"figure_eight\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = VehicleParams()\n",
    "vehicles.add(\n",
    "    \"human\",\n",
    "    acceleration_controller=(IDMController, {}),\n",
    "    routing_controller=(ContinuousRouter, {}),\n",
    "    num_vehicles=15,\n",
    ")\n",
    "vehicles.add(\n",
    "    veh_id=\"rl\",\n",
    "    acceleration_controller=(RLController, {}),\n",
    "    routing_controller=(ContinuousRouter, {}),\n",
    "    num_vehicles=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified.\n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=EXP_NAME,\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=AccelEnv,\n",
    "    # name of the network class the experiment uses\n",
    "    network=FigureEightNetwork,\n",
    "    # simulator that is used by the experiment\n",
    "    simulator=\"traci\",\n",
    "    # simulation-related parameters\n",
    "    sim=SumoParams(sim_step=0.1, render=False),\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=EnvParams(\n",
    "        # length of one rollout\n",
    "        horizon=HORIZON,\n",
    "        additional_params=ADDITIONAL_ENV_PARAMS,\n",
    "    ),\n",
    "    # network-related parameters (see flow.core.params.NetParams and\n",
    "    # the network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=NetParams(additional_params=ADDITIONAL_NET_PARAMS),\n",
    "    # vehicles to be placed in the network at the start of a rollout\n",
    "    # (see flow.core.vehicles.Vehicles)\n",
    "    veh=vehicles,\n",
    "    # (optional) parameters affecting the positioning of vehicles upon\n",
    "    # initialization/reset (see flow.core.params.InitialConfig)\n",
    "    initial=InitialConfig(spacing=\"uniform\", perturbation=1),\n",
    ")\n",
    "\n",
    "# Call the utility function make_create_env to be able to\n",
    "# register the Flow env for this experiment\n",
    "gym_name, create_env = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 18:08:33,883\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.agents.ppo.ppo::DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.ppo.ppo::PPOConfig(...)` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "# save the flow params for replay\n",
    "flow_json = json.dumps(\n",
    "    flow_params, cls=FlowParamsEncoder, sort_keys=True, indent=4\n",
    ")  # generating a string version of flow_params\n",
    "\n",
    "config = {\n",
    "    **DEFAULT_CONFIG,\n",
    "    \"num_workers\": N_WORKERS,  # number of parallel workers\n",
    "    \"train_batch_size\": HORIZON * N_ROLLOUTS,  # batch size\n",
    "    \"model\": {\n",
    "        **DEFAULT_CONFIG[\"model\"],\n",
    "        \"fcnet_hiddens\": [16, 16],\n",
    "    },  # size of hidden layers in network\n",
    "    \"use_gae\": True,  # using generalized advantage estimation\n",
    "    \"gamma\": 0.999,  # discount rate\n",
    "    \"lambda\": 0.97,\n",
    "    \"sgd_minibatch_size\": min(\n",
    "        16 * 1024, HORIZON * N_ROLLOUTS\n",
    "    ),  # stochastic gradient descent\n",
    "    \"kl_target\": 0.02,  # target KL divergence\n",
    "    \"num_sgd_iter\": 10,  # number of SGD iterations\n",
    "    \"horizon\": HORIZON,  # rollout horizon\n",
    "    \"framework\": \"tf2\",\n",
    "    \"eager_tracing\": True,\n",
    "    \"env_config\": {\n",
    "        \"flow_params\": flow_json,\n",
    "        \"run\": RL_ALG_NAME,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-04 18:08:35,909\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-09-04 18:31:47 (running for 00:23:10.64)<br>Memory usage on this node: 24.2/64.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/10 CPUs, 0/0 GPUs, 0.0/39.5 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/maxdumas/ray_results/figure_eight<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_wor...</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_AccelEnv-v0_1fa80_00000</td><td>TERMINATED</td><td>127.0.0.1:84495</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         1376.71</td><td style=\"text-align: right;\">750600</td><td style=\"text-align: right;\"> 102.781</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">             103.799</td><td style=\"text-align: right;\">             101.938</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=84495)\u001b[0m 2022-09-04 18:08:40,832\tINFO algorithm.py:1860 -- Executing eagerly (framework='tf2'), with eager_tracing=True. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(PPO pid=84495)\u001b[0m 2022-09-04 18:08:40,832\tWARNING ppo.py:350 -- `train_batch_size` (5000) cannot be achieved with your other settings (num_workers=9 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 556.\n",
      "\u001b[2m\u001b[36m(PPO pid=84495)\u001b[0m 2022-09-04 18:08:40,833\tINFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84504)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84504)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84504)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84504)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84504)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84501)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84501)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84501)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84501)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84498)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84498)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84498)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84498)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84498)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84499)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84499)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84499)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84499)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84499)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84506)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84506)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84506)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84506)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84506)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84500)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84500)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84500)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84500)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84500)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84502)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84502)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84502)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84502)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84502)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=84495)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(PPO pid=84495)\u001b[0m \n",
      "\u001b[2m\u001b[36m(PPO pid=84495)\u001b[0m systemMemory: 64.00 GB\n",
      "\u001b[2m\u001b[36m(PPO pid=84495)\u001b[0m maxCacheSize: 24.00 GB\n",
      "\u001b[2m\u001b[36m(PPO pid=84495)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.28, wished=4.50, severity=1.28, time=9.10.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.46, wished=4.50, severity=1.32, time=9.30.\n",
      "\u001b[2m\u001b[36m(PPO pid=84495)\u001b[0m 2022-09-04 18:08:56,594\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 5004\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 5004\n",
      "    num_agent_steps_trained: 5004\n",
      "    num_env_steps_sampled: 5004\n",
      "    num_env_steps_trained: 5004\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-08-58\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.414780616760254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.5099797059956472e-06\n",
      "          policy_loss: -0.10837201029062271\n",
      "          total_loss: 6.024956226348877\n",
      "          vf_explained_var: -4.117786738788709e-05\n",
      "          vf_loss: 6.133327007293701\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 5004\n",
      "    num_agent_steps_trained: 5004\n",
      "    num_env_steps_sampled: 5004\n",
      "    num_env_steps_trained: 5004\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 5004\n",
      "  num_agent_steps_trained: 5004\n",
      "  num_env_steps_sampled: 5004\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 5004\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 70.93333333333334\n",
      "    ram_util_percent: 35.806666666666665\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: .nan\n",
      "    episode_media: {}\n",
      "    episode_reward_max: .nan\n",
      "    episode_reward_mean: .nan\n",
      "    episode_reward_min: .nan\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: []\n",
      "      episode_reward: []\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf: {}\n",
      "  time_since_restore: 10.503030061721802\n",
      "  time_this_iter_s: 10.503030061721802\n",
      "  time_total_s: 10.503030061721802\n",
      "  timers:\n",
      "    learn_throughput: 3524.75\n",
      "    learn_time_ms: 1419.675\n",
      "    synch_weights_time_ms: 8.06\n",
      "    training_iteration_time_ms: 10496.467\n",
      "  timestamp: 1662329338\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5004\n",
      "  training_iteration: 1\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 10008\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 10008\n",
      "    num_agent_steps_trained: 10008\n",
      "    num_env_steps_sampled: 10008\n",
      "    num_env_steps_trained: 10008\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-09-07\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4134509563446045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.918200685073316e-08\n",
      "          policy_loss: 0.015737734735012054\n",
      "          total_loss: 0.01573904976248741\n",
      "          vf_explained_var: 0.30478471517562866\n",
      "          vf_loss: 1.3083891872156528e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 10008\n",
      "    num_agent_steps_trained: 10008\n",
      "    num_env_steps_sampled: 10008\n",
      "    num_env_steps_trained: 10008\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 10008\n",
      "  num_agent_steps_trained: 10008\n",
      "  num_env_steps_sampled: 10008\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 10008\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.86153846153846\n",
      "    ram_util_percent: 36.27692307692308\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: .nan\n",
      "    episode_media: {}\n",
      "    episode_reward_max: .nan\n",
      "    episode_reward_mean: .nan\n",
      "    episode_reward_min: .nan\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: []\n",
      "      episode_reward: []\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf: {}\n",
      "  time_since_restore: 19.769683122634888\n",
      "  time_this_iter_s: 9.266653060913086\n",
      "  time_total_s: 19.769683122634888\n",
      "  timers:\n",
      "    learn_throughput: 5500.382\n",
      "    learn_time_ms: 909.755\n",
      "    synch_weights_time_ms: 8.01\n",
      "    training_iteration_time_ms: 9878.044\n",
      "  timestamp: 1662329347\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10008\n",
      "  training_iteration: 2\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 15012\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 15012\n",
      "    num_agent_steps_trained: 15012\n",
      "    num_env_steps_sampled: 15012\n",
      "    num_env_steps_trained: 15012\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-09-16\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.05000000074505806\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.413574457168579\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.566422428022634e-08\n",
      "          policy_loss: -0.012744992971420288\n",
      "          total_loss: -0.012744896113872528\n",
      "          vf_explained_var: 0.2703380584716797\n",
      "          vf_loss: 9.20188156783297e-08\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 15012\n",
      "    num_agent_steps_trained: 15012\n",
      "    num_env_steps_sampled: 15012\n",
      "    num_env_steps_trained: 15012\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 15012\n",
      "  num_agent_steps_trained: 15012\n",
      "  num_env_steps_sampled: 15012\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 15012\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.13846153846154\n",
      "    ram_util_percent: 36.823076923076925\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: .nan\n",
      "    episode_media: {}\n",
      "    episode_reward_max: .nan\n",
      "    episode_reward_mean: .nan\n",
      "    episode_reward_min: .nan\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: []\n",
      "      episode_reward: []\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf: {}\n",
      "  time_since_restore: 28.9884672164917\n",
      "  time_this_iter_s: 9.218784093856812\n",
      "  time_total_s: 28.9884672164917\n",
      "  timers:\n",
      "    learn_throughput: 6828.494\n",
      "    learn_time_ms: 732.812\n",
      "    synch_weights_time_ms: 7.919\n",
      "    training_iteration_time_ms: 9655.893\n",
      "  timestamp: 1662329356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15012\n",
      "  training_iteration: 3\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 20016\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 20016\n",
      "    num_agent_steps_trained: 20016\n",
      "    num_env_steps_sampled: 20016\n",
      "    num_env_steps_trained: 20016\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-09-25\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.02500000037252903\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4139072895050049\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.866377357690908e-08\n",
      "          policy_loss: 0.0072201453149318695\n",
      "          total_loss: 0.007220177911221981\n",
      "          vf_explained_var: 0.17554792761802673\n",
      "          vf_loss: 3.587488350831336e-08\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 20016\n",
      "    num_agent_steps_trained: 20016\n",
      "    num_env_steps_sampled: 20016\n",
      "    num_env_steps_trained: 20016\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 20016\n",
      "  num_agent_steps_trained: 20016\n",
      "  num_env_steps_sampled: 20016\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 20016\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.53846153846153\n",
      "    ram_util_percent: 36.26923076923078\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: .nan\n",
      "    episode_media: {}\n",
      "    episode_reward_max: .nan\n",
      "    episode_reward_mean: .nan\n",
      "    episode_reward_min: .nan\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: []\n",
      "      episode_reward: []\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf: {}\n",
      "  time_since_restore: 38.360292196273804\n",
      "  time_this_iter_s: 9.371824979782104\n",
      "  time_total_s: 38.360292196273804\n",
      "  timers:\n",
      "    learn_throughput: 7748.786\n",
      "    learn_time_ms: 645.779\n",
      "    synch_weights_time_ms: 7.97\n",
      "    training_iteration_time_ms: 9583.086\n",
      "  timestamp: 1662329365\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20016\n",
      "  training_iteration: 4\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 25020\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 25020\n",
      "    num_agent_steps_trained: 25020\n",
      "    num_env_steps_sampled: 25020\n",
      "    num_env_steps_trained: 25020\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-09-35\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.012500000186264515\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4138906002044678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.8250255468265095e-08\n",
      "          policy_loss: 0.0016626309370622039\n",
      "          total_loss: 0.001662657829001546\n",
      "          vf_explained_var: 0.14595086872577667\n",
      "          vf_loss: 3.010911697742813e-08\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 25020\n",
      "    num_agent_steps_trained: 25020\n",
      "    num_env_steps_sampled: 25020\n",
      "    num_env_steps_trained: 25020\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 25020\n",
      "  num_agent_steps_trained: 25020\n",
      "  num_env_steps_sampled: 25020\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 25020\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.00000000000001\n",
      "    ram_util_percent: 36.33076923076924\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: .nan\n",
      "    episode_media: {}\n",
      "    episode_reward_max: .nan\n",
      "    episode_reward_mean: .nan\n",
      "    episode_reward_min: .nan\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: []\n",
      "      episode_reward: []\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf: {}\n",
      "  time_since_restore: 47.60768532752991\n",
      "  time_this_iter_s: 9.247393131256104\n",
      "  time_total_s: 47.60768532752991\n",
      "  timers:\n",
      "    learn_throughput: 8457.769\n",
      "    learn_time_ms: 591.645\n",
      "    synch_weights_time_ms: 7.991\n",
      "    training_iteration_time_ms: 9514.291\n",
      "  timestamp: 1662329375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25020\n",
      "  training_iteration: 5\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 30024\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 30024\n",
      "    num_agent_steps_trained: 30024\n",
      "    num_env_steps_sampled: 30024\n",
      "    num_env_steps_trained: 30024\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-09-44\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0062500000931322575\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4140530824661255\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.60745358357417e-07\n",
      "          policy_loss: -0.04450484365224838\n",
      "          total_loss: -0.044504813849925995\n",
      "          vf_explained_var: 0.04303020238876343\n",
      "          vf_loss: 2.7908825472877652e-08\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 30024\n",
      "    num_agent_steps_trained: 30024\n",
      "    num_env_steps_sampled: 30024\n",
      "    num_env_steps_trained: 30024\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 30024\n",
      "  num_agent_steps_trained: 30024\n",
      "  num_env_steps_sampled: 30024\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 30024\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.13846153846154\n",
      "    ram_util_percent: 36.392307692307696\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: .nan\n",
      "    episode_media: {}\n",
      "    episode_reward_max: .nan\n",
      "    episode_reward_mean: .nan\n",
      "    episode_reward_min: .nan\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: []\n",
      "      episode_reward: []\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf: {}\n",
      "  time_since_restore: 57.02573537826538\n",
      "  time_this_iter_s: 9.418050050735474\n",
      "  time_total_s: 57.02573537826538\n",
      "  timers:\n",
      "    learn_throughput: 8989.246\n",
      "    learn_time_ms: 556.665\n",
      "    synch_weights_time_ms: 7.936\n",
      "    training_iteration_time_ms: 9497.022\n",
      "  timestamp: 1662329384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30024\n",
      "  training_iteration: 6\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 35028\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 35028\n",
      "    num_agent_steps_trained: 35028\n",
      "    num_env_steps_sampled: 35028\n",
      "    num_env_steps_trained: 35028\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-09-54\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0031250000465661287\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4154719114303589\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.222159640856262e-07\n",
      "          policy_loss: 0.00359351490624249\n",
      "          total_loss: 0.003593543078750372\n",
      "          vf_explained_var: 0.23056967556476593\n",
      "          vf_loss: 2.8754119085760976e-08\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 35028\n",
      "    num_agent_steps_trained: 35028\n",
      "    num_env_steps_sampled: 35028\n",
      "    num_env_steps_trained: 35028\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 35028\n",
      "  num_agent_steps_trained: 35028\n",
      "  num_env_steps_sampled: 35028\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 35028\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.70000000000002\n",
      "    ram_util_percent: 36.45384615384616\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: .nan\n",
      "    episode_media: {}\n",
      "    episode_reward_max: .nan\n",
      "    episode_reward_mean: .nan\n",
      "    episode_reward_min: .nan\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: []\n",
      "      episode_reward: []\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf: {}\n",
      "  time_since_restore: 66.4301552772522\n",
      "  time_this_iter_s: 9.404419898986816\n",
      "  time_total_s: 66.4301552772522\n",
      "  timers:\n",
      "    learn_throughput: 9452.738\n",
      "    learn_time_ms: 529.37\n",
      "    synch_weights_time_ms: 7.917\n",
      "    training_iteration_time_ms: 9482.753\n",
      "  timestamp: 1662329394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35028\n",
      "  training_iteration: 7\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 40032\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 40032\n",
      "    num_agent_steps_trained: 40032\n",
      "    num_env_steps_sampled: 40032\n",
      "    num_env_steps_trained: 40032\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-10-03\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0015625000232830644\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4157146215438843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.27568168753578e-08\n",
      "          policy_loss: 0.019868390634655952\n",
      "          total_loss: 0.01986841671168804\n",
      "          vf_explained_var: 0.3541538417339325\n",
      "          vf_loss: 2.8873396118456185e-08\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 40032\n",
      "    num_agent_steps_trained: 40032\n",
      "    num_env_steps_sampled: 40032\n",
      "    num_env_steps_trained: 40032\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 40032\n",
      "  num_agent_steps_trained: 40032\n",
      "  num_env_steps_sampled: 40032\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 40032\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 83.06923076923076\n",
      "    ram_util_percent: 36.4\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: .nan\n",
      "    episode_media: {}\n",
      "    episode_reward_max: .nan\n",
      "    episode_reward_mean: .nan\n",
      "    episode_reward_min: .nan\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: []\n",
      "      episode_reward: []\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf: {}\n",
      "  time_since_restore: 75.72672009468079\n",
      "  time_this_iter_s: 9.296564817428589\n",
      "  time_total_s: 75.72672009468079\n",
      "  timers:\n",
      "    learn_throughput: 9747.761\n",
      "    learn_time_ms: 513.349\n",
      "    synch_weights_time_ms: 7.891\n",
      "    training_iteration_time_ms: 9458.427\n",
      "  timestamp: 1662329403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40032\n",
      "  training_iteration: 8\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 45036\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 45036\n",
      "    num_agent_steps_trained: 45036\n",
      "    num_env_steps_sampled: 45036\n",
      "    num_env_steps_trained: 45036\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-10-12\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.79093230533132\n",
      "  episode_reward_min: 101.94032551804062\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 9\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0007812500116415322\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4152849912643433\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.645663812472776e-07\n",
      "          policy_loss: -0.03766914829611778\n",
      "          total_loss: -0.037636805325746536\n",
      "          vf_explained_var: 0.0023192614316940308\n",
      "          vf_loss: 3.234641189919785e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 45036\n",
      "    num_agent_steps_trained: 45036\n",
      "    num_env_steps_sampled: 45036\n",
      "    num_env_steps_trained: 45036\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 45036\n",
      "  num_agent_steps_trained: 45036\n",
      "  num_env_steps_sampled: 45036\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 45036\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 82.22307692307693\n",
      "    ram_util_percent: 36.338461538461544\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787729478197313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.590674041576981\n",
      "    mean_inference_ms: 10.883937563214984\n",
      "    mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.79093230533132\n",
      "    episode_reward_min: 101.94032551804062\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11787729478197313\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.590674041576981\n",
      "      mean_inference_ms: 10.883937563214984\n",
      "      mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  time_since_restore: 85.33066511154175\n",
      "  time_this_iter_s: 9.603945016860962\n",
      "  time_total_s: 85.33066511154175\n",
      "  timers:\n",
      "    learn_throughput: 10059.374\n",
      "    learn_time_ms: 497.446\n",
      "    synch_weights_time_ms: 7.925\n",
      "    training_iteration_time_ms: 9473.631\n",
      "  timestamp: 1662329412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45036\n",
      "  training_iteration: 9\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=509.20.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=509.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 50040\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 50040\n",
      "    num_agent_steps_trained: 50040\n",
      "    num_env_steps_sampled: 50040\n",
      "    num_env_steps_trained: 50040\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-10-22\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.79093230533132\n",
      "  episode_reward_min: 101.94032551804062\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0003906250058207661\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.416961908340454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.934244409085295e-07\n",
      "          policy_loss: -0.060349058359861374\n",
      "          total_loss: 5.647315502166748\n",
      "          vf_explained_var: -5.528628753381781e-05\n",
      "          vf_loss: 5.707664489746094\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 50040\n",
      "    num_agent_steps_trained: 50040\n",
      "    num_env_steps_sampled: 50040\n",
      "    num_env_steps_trained: 50040\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 50040\n",
      "  num_agent_steps_trained: 50040\n",
      "  num_env_steps_sampled: 50040\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 50040\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.45714285714287\n",
      "    ram_util_percent: 36.41428571428572\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787729478197313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.590674041576981\n",
      "    mean_inference_ms: 10.883937563214984\n",
      "    mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.79093230533132\n",
      "    episode_reward_min: 101.94032551804062\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11787729478197313\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.590674041576981\n",
      "      mean_inference_ms: 10.883937563214984\n",
      "      mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  time_since_restore: 94.98122000694275\n",
      "  time_this_iter_s: 9.650554895401001\n",
      "  time_total_s: 94.98122000694275\n",
      "  timers:\n",
      "    learn_throughput: 10274.724\n",
      "    learn_time_ms: 487.02\n",
      "    synch_weights_time_ms: 7.964\n",
      "    training_iteration_time_ms: 9490.468\n",
      "  timestamp: 1662329422\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50040\n",
      "  training_iteration: 10\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 55044\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 55044\n",
      "    num_agent_steps_trained: 55044\n",
      "    num_env_steps_sampled: 55044\n",
      "    num_env_steps_trained: 55044\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-10-31\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.79093230533132\n",
      "  episode_reward_min: 101.94032551804062\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.00019531250291038305\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4138916730880737\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.12789086617704e-06\n",
      "          policy_loss: 0.04927437752485275\n",
      "          total_loss: 0.04928169399499893\n",
      "          vf_explained_var: 0.2556731402873993\n",
      "          vf_loss: 7.321342764043948e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 55044\n",
      "    num_agent_steps_trained: 55044\n",
      "    num_env_steps_sampled: 55044\n",
      "    num_env_steps_trained: 55044\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 55044\n",
      "  num_agent_steps_trained: 55044\n",
      "  num_env_steps_sampled: 55044\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 55044\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.26923076923075\n",
      "    ram_util_percent: 36.146153846153844\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787729478197313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.590674041576981\n",
      "    mean_inference_ms: 10.883937563214984\n",
      "    mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.79093230533132\n",
      "    episode_reward_min: 101.94032551804062\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11787729478197313\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.590674041576981\n",
      "      mean_inference_ms: 10.883937563214984\n",
      "      mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  time_since_restore: 104.29220175743103\n",
      "  time_this_iter_s: 9.310981750488281\n",
      "  time_total_s: 104.29220175743103\n",
      "  timers:\n",
      "    learn_throughput: 13127.685\n",
      "    learn_time_ms: 381.179\n",
      "    synch_weights_time_ms: 7.939\n",
      "    training_iteration_time_ms: 9371.221\n",
      "  timestamp: 1662329431\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55044\n",
      "  training_iteration: 11\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 60048\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 60048\n",
      "    num_agent_steps_trained: 60048\n",
      "    num_env_steps_sampled: 60048\n",
      "    num_env_steps_trained: 60048\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-10-41\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.79093230533132\n",
      "  episode_reward_min: 101.94032551804062\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.765625145519152e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4128704071044922\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.239362389555026e-07\n",
      "          policy_loss: -0.05848538130521774\n",
      "          total_loss: -0.05848492309451103\n",
      "          vf_explained_var: 0.22858890891075134\n",
      "          vf_loss: 4.516345484262274e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 60048\n",
      "    num_agent_steps_trained: 60048\n",
      "    num_env_steps_sampled: 60048\n",
      "    num_env_steps_trained: 60048\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 60048\n",
      "  num_agent_steps_trained: 60048\n",
      "  num_env_steps_sampled: 60048\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 60048\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.83333333333333\n",
      "    ram_util_percent: 36.15\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787729478197313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.590674041576981\n",
      "    mean_inference_ms: 10.883937563214984\n",
      "    mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.79093230533132\n",
      "    episode_reward_min: 101.94032551804062\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11787729478197313\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.590674041576981\n",
      "      mean_inference_ms: 10.883937563214984\n",
      "      mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  time_since_restore: 113.49729681015015\n",
      "  time_this_iter_s: 9.205095052719116\n",
      "  time_total_s: 113.49729681015015\n",
      "  timers:\n",
      "    learn_throughput: 13215.804\n",
      "    learn_time_ms: 378.638\n",
      "    synch_weights_time_ms: 7.958\n",
      "    training_iteration_time_ms: 9364.994\n",
      "  timestamp: 1662329441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60048\n",
      "  training_iteration: 12\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 65052\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 65052\n",
      "    num_agent_steps_trained: 65052\n",
      "    num_env_steps_sampled: 65052\n",
      "    num_env_steps_trained: 65052\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.79093230533132\n",
      "  episode_reward_min: 101.94032551804062\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.882812572759576e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4134141206741333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.3058525755459414e-07\n",
      "          policy_loss: 0.033291105180978775\n",
      "          total_loss: 0.033291321247816086\n",
      "          vf_explained_var: 0.2645971477031708\n",
      "          vf_loss: 2.1172797914914554e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 65052\n",
      "    num_agent_steps_trained: 65052\n",
      "    num_env_steps_sampled: 65052\n",
      "    num_env_steps_trained: 65052\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 65052\n",
      "  num_agent_steps_trained: 65052\n",
      "  num_env_steps_sampled: 65052\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 65052\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.62307692307692\n",
      "    ram_util_percent: 36.23846153846154\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787729478197313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.590674041576981\n",
      "    mean_inference_ms: 10.883937563214984\n",
      "    mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.79093230533132\n",
      "    episode_reward_min: 101.94032551804062\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11787729478197313\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.590674041576981\n",
      "      mean_inference_ms: 10.883937563214984\n",
      "      mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  time_since_restore: 122.76148986816406\n",
      "  time_this_iter_s: 9.264193058013916\n",
      "  time_total_s: 122.76148986816406\n",
      "  timers:\n",
      "    learn_throughput: 13286.507\n",
      "    learn_time_ms: 376.623\n",
      "    synch_weights_time_ms: 7.947\n",
      "    training_iteration_time_ms: 9369.445\n",
      "  timestamp: 1662329450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65052\n",
      "  training_iteration: 13\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 70056\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 70056\n",
      "    num_agent_steps_trained: 70056\n",
      "    num_env_steps_sampled: 70056\n",
      "    num_env_steps_trained: 70056\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-10-59\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.79093230533132\n",
      "  episode_reward_min: 101.94032551804062\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.441406286379788e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4135948419570923\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.144373969618755e-07\n",
      "          policy_loss: -0.020200762897729874\n",
      "          total_loss: -0.02020057663321495\n",
      "          vf_explained_var: 0.17272913455963135\n",
      "          vf_loss: 1.8741963003776618e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 70056\n",
      "    num_agent_steps_trained: 70056\n",
      "    num_env_steps_sampled: 70056\n",
      "    num_env_steps_trained: 70056\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 70056\n",
      "  num_agent_steps_trained: 70056\n",
      "  num_env_steps_sampled: 70056\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 70056\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.39230769230768\n",
      "    ram_util_percent: 36.130769230769225\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787729478197313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.590674041576981\n",
      "    mean_inference_ms: 10.883937563214984\n",
      "    mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.79093230533132\n",
      "    episode_reward_min: 101.94032551804062\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11787729478197313\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.590674041576981\n",
      "      mean_inference_ms: 10.883937563214984\n",
      "      mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  time_since_restore: 132.04576873779297\n",
      "  time_this_iter_s: 9.284278869628906\n",
      "  time_total_s: 132.04576873779297\n",
      "  timers:\n",
      "    learn_throughput: 13364.14\n",
      "    learn_time_ms: 374.435\n",
      "    synch_weights_time_ms: 7.868\n",
      "    training_iteration_time_ms: 9360.532\n",
      "  timestamp: 1662329459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70056\n",
      "  training_iteration: 14\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 75060\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 75060\n",
      "    num_agent_steps_trained: 75060\n",
      "    num_env_steps_sampled: 75060\n",
      "    num_env_steps_trained: 75060\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-11-09\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.79093230533132\n",
      "  episode_reward_min: 101.94032551804062\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.220703143189894e-05\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4140069484710693\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1173610801051836e-06\n",
      "          policy_loss: -0.08834529668092728\n",
      "          total_loss: -0.08834511041641235\n",
      "          vf_explained_var: 0.151174858212471\n",
      "          vf_loss: 1.8108548260897805e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 75060\n",
      "    num_agent_steps_trained: 75060\n",
      "    num_env_steps_sampled: 75060\n",
      "    num_env_steps_trained: 75060\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 75060\n",
      "  num_agent_steps_trained: 75060\n",
      "  num_env_steps_sampled: 75060\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 75060\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.04615384615384\n",
      "    ram_util_percent: 36.15384615384615\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787729478197313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.590674041576981\n",
      "    mean_inference_ms: 10.883937563214984\n",
      "    mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.79093230533132\n",
      "    episode_reward_min: 101.94032551804062\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11787729478197313\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.590674041576981\n",
      "      mean_inference_ms: 10.883937563214984\n",
      "      mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  time_since_restore: 141.40996265411377\n",
      "  time_this_iter_s: 9.3641939163208\n",
      "  time_total_s: 141.40996265411377\n",
      "  timers:\n",
      "    learn_throughput: 13466.655\n",
      "    learn_time_ms: 371.584\n",
      "    synch_weights_time_ms: 7.791\n",
      "    training_iteration_time_ms: 9372.223\n",
      "  timestamp: 1662329469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75060\n",
      "  training_iteration: 15\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 80064\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 80064\n",
      "    num_agent_steps_trained: 80064\n",
      "    num_env_steps_sampled: 80064\n",
      "    num_env_steps_trained: 80064\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-11-18\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.79093230533132\n",
      "  episode_reward_min: 101.94032551804062\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.10351571594947e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4155229330062866\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0634556108234392e-07\n",
      "          policy_loss: -0.004600113723427057\n",
      "          total_loss: -0.004599928855895996\n",
      "          vf_explained_var: 0.21139855682849884\n",
      "          vf_loss: 1.8260291767546732e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 80064\n",
      "    num_agent_steps_trained: 80064\n",
      "    num_env_steps_sampled: 80064\n",
      "    num_env_steps_trained: 80064\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 80064\n",
      "  num_agent_steps_trained: 80064\n",
      "  num_env_steps_sampled: 80064\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 80064\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.03076923076924\n",
      "    ram_util_percent: 36.23076923076923\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787729478197313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.590674041576981\n",
      "    mean_inference_ms: 10.883937563214984\n",
      "    mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.79093230533132\n",
      "    episode_reward_min: 101.94032551804062\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11787729478197313\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.590674041576981\n",
      "      mean_inference_ms: 10.883937563214984\n",
      "      mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  time_since_restore: 150.73827171325684\n",
      "  time_this_iter_s: 9.328309059143066\n",
      "  time_total_s: 150.73827171325684\n",
      "  timers:\n",
      "    learn_throughput: 13592.102\n",
      "    learn_time_ms: 368.155\n",
      "    synch_weights_time_ms: 7.771\n",
      "    training_iteration_time_ms: 9363.081\n",
      "  timestamp: 1662329478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80064\n",
      "  training_iteration: 16\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 85068\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 85068\n",
      "    num_agent_steps_trained: 85068\n",
      "    num_env_steps_sampled: 85068\n",
      "    num_env_steps_trained: 85068\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-11-27\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.79093230533132\n",
      "  episode_reward_min: 101.94032551804062\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 9\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.051757857974735e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4161221981048584\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.452689487763564e-07\n",
      "          policy_loss: 0.0325569324195385\n",
      "          total_loss: 0.03255712240934372\n",
      "          vf_explained_var: 0.1671520173549652\n",
      "          vf_loss: 1.8432140791446727e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 85068\n",
      "    num_agent_steps_trained: 85068\n",
      "    num_env_steps_sampled: 85068\n",
      "    num_env_steps_trained: 85068\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 85068\n",
      "  num_agent_steps_trained: 85068\n",
      "  num_env_steps_sampled: 85068\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 85068\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.13076923076923\n",
      "    ram_util_percent: 36.169230769230765\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787729478197313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.590674041576981\n",
      "    mean_inference_ms: 10.883937563214984\n",
      "    mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.79093230533132\n",
      "    episode_reward_min: 101.94032551804062\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11787729478197313\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.590674041576981\n",
      "      mean_inference_ms: 10.883937563214984\n",
      "      mean_raw_obs_processing_ms: 0.2003087996377944\n",
      "  time_since_restore: 160.04102659225464\n",
      "  time_this_iter_s: 9.302754878997803\n",
      "  time_total_s: 160.04102659225464\n",
      "  timers:\n",
      "    learn_throughput: 13540.288\n",
      "    learn_time_ms: 369.564\n",
      "    synch_weights_time_ms: 7.812\n",
      "    training_iteration_time_ms: 9352.847\n",
      "  timestamp: 1662329487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85068\n",
      "  training_iteration: 17\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 90072\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 90072\n",
      "    num_agent_steps_trained: 90072\n",
      "    num_env_steps_sampled: 90072\n",
      "    num_env_steps_trained: 90072\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-11-37\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7876795703186\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 18\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5258789289873675e-06\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.414888858795166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.5970985007006675e-06\n",
      "          policy_loss: 0.055713485926389694\n",
      "          total_loss: 0.05605471879243851\n",
      "          vf_explained_var: 0.10485583543777466\n",
      "          vf_loss: 0.00034124040394090116\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 90072\n",
      "    num_agent_steps_trained: 90072\n",
      "    num_env_steps_sampled: 90072\n",
      "    num_env_steps_trained: 90072\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 90072\n",
      "  num_agent_steps_trained: 90072\n",
      "  num_env_steps_sampled: 90072\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 90072\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.78461538461539\n",
      "    ram_util_percent: 36.176923076923075\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775583409075598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.599260769694271\n",
      "    mean_inference_ms: 10.868144848560915\n",
      "    mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7876795703186\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11775583409075598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.599260769694271\n",
      "      mean_inference_ms: 10.868144848560915\n",
      "      mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  time_since_restore: 169.26931476593018\n",
      "  time_this_iter_s: 9.228288173675537\n",
      "  time_total_s: 169.26931476593018\n",
      "  timers:\n",
      "    learn_throughput: 13688.569\n",
      "    learn_time_ms: 365.56\n",
      "    synch_weights_time_ms: 7.775\n",
      "    training_iteration_time_ms: 9346.092\n",
      "  timestamp: 1662329497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90072\n",
      "  training_iteration: 18\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=1009.30.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=1009.50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 95076\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 95076\n",
      "    num_agent_steps_trained: 95076\n",
      "    num_env_steps_sampled: 95076\n",
      "    num_env_steps_trained: 95076\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-11-46\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7876795703186\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 18\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.629394644936838e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4144999980926514\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.67280927093816e-06\n",
      "          policy_loss: 0.0028420835733413696\n",
      "          total_loss: 5.319384574890137\n",
      "          vf_explained_var: -0.015750834718346596\n",
      "          vf_loss: 5.316542625427246\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 95076\n",
      "    num_agent_steps_trained: 95076\n",
      "    num_env_steps_sampled: 95076\n",
      "    num_env_steps_trained: 95076\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 95076\n",
      "  num_agent_steps_trained: 95076\n",
      "  num_env_steps_sampled: 95076\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 95076\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.53076923076924\n",
      "    ram_util_percent: 36.24615384615385\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775583409075598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.599260769694271\n",
      "    mean_inference_ms: 10.868144848560915\n",
      "    mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7876795703186\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11775583409075598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.599260769694271\n",
      "      mean_inference_ms: 10.868144848560915\n",
      "      mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  time_since_restore: 178.47131180763245\n",
      "  time_this_iter_s: 9.20199704170227\n",
      "  time_total_s: 178.47131180763245\n",
      "  timers:\n",
      "    learn_throughput: 13726.667\n",
      "    learn_time_ms: 364.546\n",
      "    synch_weights_time_ms: 7.705\n",
      "    training_iteration_time_ms: 9305.988\n",
      "  timestamp: 1662329506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95076\n",
      "  training_iteration: 19\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 100080\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 100080\n",
      "    num_agent_steps_trained: 100080\n",
      "    num_env_steps_sampled: 100080\n",
      "    num_env_steps_trained: 100080\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-11-55\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7876795703186\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 18\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.814697322468419e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4106346368789673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.834755256728386e-07\n",
      "          policy_loss: -0.09781473875045776\n",
      "          total_loss: -0.09780583530664444\n",
      "          vf_explained_var: 0.24290792644023895\n",
      "          vf_loss: 8.901110049919225e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 100080\n",
      "    num_agent_steps_trained: 100080\n",
      "    num_env_steps_sampled: 100080\n",
      "    num_env_steps_trained: 100080\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 100080\n",
      "  num_agent_steps_trained: 100080\n",
      "  num_env_steps_sampled: 100080\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 100080\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.56666666666668\n",
      "    ram_util_percent: 36.43333333333334\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775583409075598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.599260769694271\n",
      "    mean_inference_ms: 10.868144848560915\n",
      "    mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7876795703186\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11775583409075598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.599260769694271\n",
      "      mean_inference_ms: 10.868144848560915\n",
      "      mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  time_since_restore: 187.51688289642334\n",
      "  time_this_iter_s: 9.045571088790894\n",
      "  time_total_s: 187.51688289642334\n",
      "  timers:\n",
      "    learn_throughput: 13889.745\n",
      "    learn_time_ms: 360.266\n",
      "    synch_weights_time_ms: 7.616\n",
      "    training_iteration_time_ms: 9245.607\n",
      "  timestamp: 1662329515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100080\n",
      "  training_iteration: 20\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 105084\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 105084\n",
      "    num_agent_steps_trained: 105084\n",
      "    num_env_steps_sampled: 105084\n",
      "    num_env_steps_trained: 105084\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-12-04\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7876795703186\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 18\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9073486612342094e-07\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4110636711120605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.71868530541542e-07\n",
      "          policy_loss: 0.12927044928073883\n",
      "          total_loss: 0.12927131354808807\n",
      "          vf_explained_var: 0.2482893466949463\n",
      "          vf_loss: 8.871864451975853e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 105084\n",
      "    num_agent_steps_trained: 105084\n",
      "    num_env_steps_sampled: 105084\n",
      "    num_env_steps_trained: 105084\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 105084\n",
      "  num_agent_steps_trained: 105084\n",
      "  num_env_steps_sampled: 105084\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 105084\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.86153846153846\n",
      "    ram_util_percent: 36.30769230769231\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775583409075598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.599260769694271\n",
      "    mean_inference_ms: 10.868144848560915\n",
      "    mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7876795703186\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11775583409075598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.599260769694271\n",
      "      mean_inference_ms: 10.868144848560915\n",
      "      mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  time_since_restore: 196.4223656654358\n",
      "  time_this_iter_s: 8.905482769012451\n",
      "  time_total_s: 196.4223656654358\n",
      "  timers:\n",
      "    learn_throughput: 13914.528\n",
      "    learn_time_ms: 359.624\n",
      "    synch_weights_time_ms: 7.591\n",
      "    training_iteration_time_ms: 9205.016\n",
      "  timestamp: 1662329524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105084\n",
      "  training_iteration: 21\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 110088\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 110088\n",
      "    num_agent_steps_trained: 110088\n",
      "    num_env_steps_sampled: 110088\n",
      "    num_env_steps_trained: 110088\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-12-13\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7876795703186\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 18\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.536743306171047e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4079170227050781\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.90122533342219e-06\n",
      "          policy_loss: 0.13035434484481812\n",
      "          total_loss: 0.13035491108894348\n",
      "          vf_explained_var: 0.38608118891716003\n",
      "          vf_loss: 5.55827114112617e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 110088\n",
      "    num_agent_steps_trained: 110088\n",
      "    num_env_steps_sampled: 110088\n",
      "    num_env_steps_trained: 110088\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 110088\n",
      "  num_agent_steps_trained: 110088\n",
      "  num_env_steps_sampled: 110088\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 110088\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 81.93846153846152\n",
      "    ram_util_percent: 36.42307692307693\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775583409075598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.599260769694271\n",
      "    mean_inference_ms: 10.868144848560915\n",
      "    mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7876795703186\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11775583409075598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.599260769694271\n",
      "      mean_inference_ms: 10.868144848560915\n",
      "      mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  time_since_restore: 206.01390981674194\n",
      "  time_this_iter_s: 9.591544151306152\n",
      "  time_total_s: 206.01390981674194\n",
      "  timers:\n",
      "    learn_throughput: 13939.343\n",
      "    learn_time_ms: 358.984\n",
      "    synch_weights_time_ms: 7.487\n",
      "    training_iteration_time_ms: 9243.614\n",
      "  timestamp: 1662329533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110088\n",
      "  training_iteration: 22\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 115092\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 115092\n",
      "    num_agent_steps_trained: 115092\n",
      "    num_env_steps_sampled: 115092\n",
      "    num_env_steps_trained: 115092\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-12-23\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7876795703186\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 18\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.7683716530855236e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4047694206237793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.480321199982427e-06\n",
      "          policy_loss: -0.06687793135643005\n",
      "          total_loss: -0.06687743961811066\n",
      "          vf_explained_var: 0.3015791177749634\n",
      "          vf_loss: 5.00579631079745e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 115092\n",
      "    num_agent_steps_trained: 115092\n",
      "    num_env_steps_sampled: 115092\n",
      "    num_env_steps_trained: 115092\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 115092\n",
      "  num_agent_steps_trained: 115092\n",
      "  num_env_steps_sampled: 115092\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 115092\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.67142857142856\n",
      "    ram_util_percent: 36.49285714285715\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775583409075598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.599260769694271\n",
      "    mean_inference_ms: 10.868144848560915\n",
      "    mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7876795703186\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11775583409075598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.599260769694271\n",
      "      mean_inference_ms: 10.868144848560915\n",
      "      mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  time_since_restore: 215.95842266082764\n",
      "  time_this_iter_s: 9.944512844085693\n",
      "  time_total_s: 215.95842266082764\n",
      "  timers:\n",
      "    learn_throughput: 13856.92\n",
      "    learn_time_ms: 361.119\n",
      "    synch_weights_time_ms: 7.457\n",
      "    training_iteration_time_ms: 9311.615\n",
      "  timestamp: 1662329543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115092\n",
      "  training_iteration: 23\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 120096\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 120096\n",
      "    num_agent_steps_trained: 120096\n",
      "    num_env_steps_sampled: 120096\n",
      "    num_env_steps_trained: 120096\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-12-34\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7876795703186\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 18\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3841858265427618e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.407126784324646\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.932615487021394e-05\n",
      "          policy_loss: -0.15435293316841125\n",
      "          total_loss: -0.15435241162776947\n",
      "          vf_explained_var: 0.11502392590045929\n",
      "          vf_loss: 4.860737590206554e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 120096\n",
      "    num_agent_steps_trained: 120096\n",
      "    num_env_steps_sampled: 120096\n",
      "    num_env_steps_trained: 120096\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 120096\n",
      "  num_agent_steps_trained: 120096\n",
      "  num_env_steps_sampled: 120096\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 120096\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.89999999999999\n",
      "    ram_util_percent: 36.385714285714286\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775583409075598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.599260769694271\n",
      "    mean_inference_ms: 10.868144848560915\n",
      "    mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7876795703186\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11775583409075598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.599260769694271\n",
      "      mean_inference_ms: 10.868144848560915\n",
      "      mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  time_since_restore: 226.1625747680664\n",
      "  time_this_iter_s: 10.20415210723877\n",
      "  time_total_s: 226.1625747680664\n",
      "  timers:\n",
      "    learn_throughput: 13879.844\n",
      "    learn_time_ms: 360.523\n",
      "    synch_weights_time_ms: 7.5\n",
      "    training_iteration_time_ms: 9403.728\n",
      "  timestamp: 1662329554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120096\n",
      "  training_iteration: 24\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 125100\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 125100\n",
      "    num_agent_steps_trained: 125100\n",
      "    num_env_steps_sampled: 125100\n",
      "    num_env_steps_trained: 125100\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-12-44\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7876795703186\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 18\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1920929132713809e-08\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4102146625518799\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.568192023019947e-07\n",
      "          policy_loss: -0.1005520448088646\n",
      "          total_loss: -0.1005515456199646\n",
      "          vf_explained_var: 0.30600643157958984\n",
      "          vf_loss: 4.90002946662571e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 125100\n",
      "    num_agent_steps_trained: 125100\n",
      "    num_env_steps_sampled: 125100\n",
      "    num_env_steps_trained: 125100\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 125100\n",
      "  num_agent_steps_trained: 125100\n",
      "  num_env_steps_sampled: 125100\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 125100\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.11428571428571\n",
      "    ram_util_percent: 36.55\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775583409075598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.599260769694271\n",
      "    mean_inference_ms: 10.868144848560915\n",
      "    mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7876795703186\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11775583409075598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.599260769694271\n",
      "      mean_inference_ms: 10.868144848560915\n",
      "      mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  time_since_restore: 236.128479719162\n",
      "  time_this_iter_s: 9.965904951095581\n",
      "  time_total_s: 236.128479719162\n",
      "  timers:\n",
      "    learn_throughput: 13706.622\n",
      "    learn_time_ms: 365.079\n",
      "    synch_weights_time_ms: 7.489\n",
      "    training_iteration_time_ms: 9463.915\n",
      "  timestamp: 1662329564\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125100\n",
      "  training_iteration: 25\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 130104\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 130104\n",
      "    num_agent_steps_trained: 130104\n",
      "    num_env_steps_sampled: 130104\n",
      "    num_env_steps_trained: 130104\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-12-53\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7876795703186\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 18\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.9604645663569045e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4113473892211914\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.967661885122652e-07\n",
      "          policy_loss: 0.07684728503227234\n",
      "          total_loss: 0.07684779167175293\n",
      "          vf_explained_var: 0.254793643951416\n",
      "          vf_loss: 5.017143962504633e-07\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 130104\n",
      "    num_agent_steps_trained: 130104\n",
      "    num_env_steps_sampled: 130104\n",
      "    num_env_steps_trained: 130104\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 130104\n",
      "  num_agent_steps_trained: 130104\n",
      "  num_env_steps_sampled: 130104\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 130104\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.69230769230768\n",
      "    ram_util_percent: 36.292307692307695\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775583409075598\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.599260769694271\n",
      "    mean_inference_ms: 10.868144848560915\n",
      "    mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7876795703186\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11775583409075598\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.599260769694271\n",
      "      mean_inference_ms: 10.868144848560915\n",
      "      mean_raw_obs_processing_ms: 0.20168700994588085\n",
      "  time_since_restore: 245.64092469215393\n",
      "  time_this_iter_s: 9.512444972991943\n",
      "  time_total_s: 245.64092469215393\n",
      "  timers:\n",
      "    learn_throughput: 13596.565\n",
      "    learn_time_ms: 368.034\n",
      "    synch_weights_time_ms: 7.485\n",
      "    training_iteration_time_ms: 9482.439\n",
      "  timestamp: 1662329573\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130104\n",
      "  training_iteration: 26\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 135108\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 135108\n",
      "    num_agent_steps_trained: 135108\n",
      "    num_env_steps_sampled: 135108\n",
      "    num_env_steps_trained: 135108\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-13-02\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78710092135817\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 27\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.9802322831784522e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4106786251068115\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.676848892719136e-07\n",
      "          policy_loss: 0.0639360323548317\n",
      "          total_loss: 0.0660247802734375\n",
      "          vf_explained_var: 0.0800645723938942\n",
      "          vf_loss: 0.002088748151436448\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 135108\n",
      "    num_agent_steps_trained: 135108\n",
      "    num_env_steps_sampled: 135108\n",
      "    num_env_steps_trained: 135108\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 135108\n",
      "  num_agent_steps_trained: 135108\n",
      "  num_env_steps_sampled: 135108\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 135108\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.83846153846154\n",
      "    ram_util_percent: 36.246153846153845\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11757679494662951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.615168258905931\n",
      "    mean_inference_ms: 10.88755061200513\n",
      "    mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78710092135817\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11757679494662951\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.615168258905931\n",
      "      mean_inference_ms: 10.88755061200513\n",
      "      mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  time_since_restore: 254.9371817111969\n",
      "  time_this_iter_s: 9.296257019042969\n",
      "  time_total_s: 254.9371817111969\n",
      "  timers:\n",
      "    learn_throughput: 13584.879\n",
      "    learn_time_ms: 368.351\n",
      "    synch_weights_time_ms: 7.42\n",
      "    training_iteration_time_ms: 9481.881\n",
      "  timestamp: 1662329582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135108\n",
      "  training_iteration: 27\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=1509.40.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=1509.60.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 140112\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 140112\n",
      "    num_agent_steps_trained: 140112\n",
      "    num_env_steps_sampled: 140112\n",
      "    num_env_steps_trained: 140112\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-13-12\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78710092135817\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 27\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4901161415892261e-09\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.412330985069275\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.72497446127818e-07\n",
      "          policy_loss: -0.0012716141063719988\n",
      "          total_loss: 5.410078525543213\n",
      "          vf_explained_var: -0.00033867955789901316\n",
      "          vf_loss: 5.411350250244141\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 140112\n",
      "    num_agent_steps_trained: 140112\n",
      "    num_env_steps_sampled: 140112\n",
      "    num_env_steps_trained: 140112\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 140112\n",
      "  num_agent_steps_trained: 140112\n",
      "  num_env_steps_sampled: 140112\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 140112\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.02307692307693\n",
      "    ram_util_percent: 36.20769230769231\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11757679494662951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.615168258905931\n",
      "    mean_inference_ms: 10.88755061200513\n",
      "    mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78710092135817\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11757679494662951\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.615168258905931\n",
      "      mean_inference_ms: 10.88755061200513\n",
      "      mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  time_since_restore: 264.2259337902069\n",
      "  time_this_iter_s: 9.28875207901001\n",
      "  time_total_s: 264.2259337902069\n",
      "  timers:\n",
      "    learn_throughput: 13471.313\n",
      "    learn_time_ms: 371.456\n",
      "    synch_weights_time_ms: 7.483\n",
      "    training_iteration_time_ms: 9487.879\n",
      "  timestamp: 1662329592\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140112\n",
      "  training_iteration: 28\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 145116\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 145116\n",
      "    num_agent_steps_trained: 145116\n",
      "    num_env_steps_sampled: 145116\n",
      "    num_env_steps_trained: 145116\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78710092135817\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 27\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.450580707946131e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.407191514968872\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.883764409489231e-07\n",
      "          policy_loss: -0.061241406947374344\n",
      "          total_loss: -0.06119602173566818\n",
      "          vf_explained_var: 0.12381873279809952\n",
      "          vf_loss: 4.5377535570878536e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 145116\n",
      "    num_agent_steps_trained: 145116\n",
      "    num_env_steps_sampled: 145116\n",
      "    num_env_steps_trained: 145116\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 145116\n",
      "  num_agent_steps_trained: 145116\n",
      "  num_env_steps_sampled: 145116\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 145116\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.71538461538462\n",
      "    ram_util_percent: 36.261538461538464\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11757679494662951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.615168258905931\n",
      "    mean_inference_ms: 10.88755061200513\n",
      "    mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78710092135817\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11757679494662951\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.615168258905931\n",
      "      mean_inference_ms: 10.88755061200513\n",
      "      mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  time_since_restore: 273.53794384002686\n",
      "  time_this_iter_s: 9.312010049819946\n",
      "  time_total_s: 273.53794384002686\n",
      "  timers:\n",
      "    learn_throughput: 13390.911\n",
      "    learn_time_ms: 373.686\n",
      "    synch_weights_time_ms: 7.49\n",
      "    training_iteration_time_ms: 9498.891\n",
      "  timestamp: 1662329601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145116\n",
      "  training_iteration: 29\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 150120\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 150120\n",
      "    num_agent_steps_trained: 150120\n",
      "    num_env_steps_sampled: 150120\n",
      "    num_env_steps_trained: 150120\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-13-30\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78710092135817\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 27\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7252903539730653e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4071705341339111\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2521817300003022e-07\n",
      "          policy_loss: -0.01008075661957264\n",
      "          total_loss: -0.010077404789626598\n",
      "          vf_explained_var: 0.26622122526168823\n",
      "          vf_loss: 3.349359303683741e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 150120\n",
      "    num_agent_steps_trained: 150120\n",
      "    num_env_steps_sampled: 150120\n",
      "    num_env_steps_trained: 150120\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 150120\n",
      "  num_agent_steps_trained: 150120\n",
      "  num_env_steps_sampled: 150120\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 150120\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.36923076923077\n",
      "    ram_util_percent: 36.323076923076925\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11757679494662951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.615168258905931\n",
      "    mean_inference_ms: 10.88755061200513\n",
      "    mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78710092135817\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11757679494662951\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.615168258905931\n",
      "      mean_inference_ms: 10.88755061200513\n",
      "      mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  time_since_restore: 282.7689208984375\n",
      "  time_this_iter_s: 9.230977058410645\n",
      "  time_total_s: 282.7689208984375\n",
      "  timers:\n",
      "    learn_throughput: 13266.721\n",
      "    learn_time_ms: 377.184\n",
      "    synch_weights_time_ms: 7.506\n",
      "    training_iteration_time_ms: 9517.414\n",
      "  timestamp: 1662329610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150120\n",
      "  training_iteration: 30\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 155124\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 155124\n",
      "    num_agent_steps_trained: 155124\n",
      "    num_env_steps_sampled: 155124\n",
      "    num_env_steps_trained: 155124\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-13-40\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78710092135817\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 27\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8626451769865326e-10\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4079914093017578\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.808030429463543e-07\n",
      "          policy_loss: 0.015059629455208778\n",
      "          total_loss: 0.015061445534229279\n",
      "          vf_explained_var: 0.21473391354084015\n",
      "          vf_loss: 1.8161165371566312e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 155124\n",
      "    num_agent_steps_trained: 155124\n",
      "    num_env_steps_sampled: 155124\n",
      "    num_env_steps_trained: 155124\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 155124\n",
      "  num_agent_steps_trained: 155124\n",
      "  num_env_steps_sampled: 155124\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 155124\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.0153846153846\n",
      "    ram_util_percent: 36.24615384615385\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11757679494662951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.615168258905931\n",
      "    mean_inference_ms: 10.88755061200513\n",
      "    mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78710092135817\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11757679494662951\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.615168258905931\n",
      "      mean_inference_ms: 10.88755061200513\n",
      "      mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  time_since_restore: 292.16711616516113\n",
      "  time_this_iter_s: 9.398195266723633\n",
      "  time_total_s: 292.16711616516113\n",
      "  timers:\n",
      "    learn_throughput: 13232.131\n",
      "    learn_time_ms: 378.17\n",
      "    synch_weights_time_ms: 7.531\n",
      "    training_iteration_time_ms: 9566.634\n",
      "  timestamp: 1662329620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155124\n",
      "  training_iteration: 31\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 160128\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 160128\n",
      "    num_agent_steps_trained: 160128\n",
      "    num_env_steps_sampled: 160128\n",
      "    num_env_steps_trained: 160128\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-13-49\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78710092135817\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 27\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.313225884932663e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4084680080413818\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0690998806239804e-06\n",
      "          policy_loss: -0.0652351826429367\n",
      "          total_loss: -0.06523352861404419\n",
      "          vf_explained_var: 0.11684534698724747\n",
      "          vf_loss: 1.6525552837265423e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 160128\n",
      "    num_agent_steps_trained: 160128\n",
      "    num_env_steps_sampled: 160128\n",
      "    num_env_steps_trained: 160128\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 160128\n",
      "  num_agent_steps_trained: 160128\n",
      "  num_env_steps_sampled: 160128\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 160128\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.325\n",
      "    ram_util_percent: 36.233333333333334\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11757679494662951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.615168258905931\n",
      "    mean_inference_ms: 10.88755061200513\n",
      "    mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78710092135817\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11757679494662951\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.615168258905931\n",
      "      mean_inference_ms: 10.88755061200513\n",
      "      mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  time_since_restore: 301.30766439437866\n",
      "  time_this_iter_s: 9.14054822921753\n",
      "  time_total_s: 301.30766439437866\n",
      "  timers:\n",
      "    learn_throughput: 13268.146\n",
      "    learn_time_ms: 377.144\n",
      "    synch_weights_time_ms: 7.548\n",
      "    training_iteration_time_ms: 9521.625\n",
      "  timestamp: 1662329629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160128\n",
      "  training_iteration: 32\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 165132\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 165132\n",
      "    num_agent_steps_trained: 165132\n",
      "    num_env_steps_sampled: 165132\n",
      "    num_env_steps_trained: 165132\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-13-58\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78710092135817\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 27\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.6566129424663316e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4089502096176147\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.27528265795263e-07\n",
      "          policy_loss: 0.05677046254277229\n",
      "          total_loss: 0.0567721351981163\n",
      "          vf_explained_var: 0.24945366382598877\n",
      "          vf_loss: 1.6733362144805142e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 165132\n",
      "    num_agent_steps_trained: 165132\n",
      "    num_env_steps_sampled: 165132\n",
      "    num_env_steps_trained: 165132\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 165132\n",
      "  num_agent_steps_trained: 165132\n",
      "  num_env_steps_sampled: 165132\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 165132\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.93846153846154\n",
      "    ram_util_percent: 36.292307692307695\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11757679494662951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.615168258905931\n",
      "    mean_inference_ms: 10.88755061200513\n",
      "    mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78710092135817\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11757679494662951\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.615168258905931\n",
      "      mean_inference_ms: 10.88755061200513\n",
      "      mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  time_since_restore: 310.51683044433594\n",
      "  time_this_iter_s: 9.209166049957275\n",
      "  time_total_s: 310.51683044433594\n",
      "  timers:\n",
      "    learn_throughput: 13362.162\n",
      "    learn_time_ms: 374.49\n",
      "    synch_weights_time_ms: 7.548\n",
      "    training_iteration_time_ms: 9448.169\n",
      "  timestamp: 1662329638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165132\n",
      "  training_iteration: 33\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 170136\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 170136\n",
      "    num_agent_steps_trained: 170136\n",
      "    num_env_steps_sampled: 170136\n",
      "    num_env_steps_trained: 170136\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-14-07\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78710092135817\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 27\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.3283064712331658e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4071917533874512\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2616037515253993e-06\n",
      "          policy_loss: 0.05240865796804428\n",
      "          total_loss: 0.052410323172807693\n",
      "          vf_explained_var: 0.3155468702316284\n",
      "          vf_loss: 1.6705349707990536e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 170136\n",
      "    num_agent_steps_trained: 170136\n",
      "    num_env_steps_sampled: 170136\n",
      "    num_env_steps_trained: 170136\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 170136\n",
      "  num_agent_steps_trained: 170136\n",
      "  num_env_steps_sampled: 170136\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 170136\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.82307692307693\n",
      "    ram_util_percent: 36.276923076923076\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11757679494662951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.615168258905931\n",
      "    mean_inference_ms: 10.88755061200513\n",
      "    mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78710092135817\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11757679494662951\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.615168258905931\n",
      "      mean_inference_ms: 10.88755061200513\n",
      "      mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  time_since_restore: 319.73942136764526\n",
      "  time_this_iter_s: 9.222590923309326\n",
      "  time_total_s: 319.73942136764526\n",
      "  timers:\n",
      "    learn_throughput: 13279.948\n",
      "    learn_time_ms: 376.809\n",
      "    synch_weights_time_ms: 7.588\n",
      "    training_iteration_time_ms: 9350.04\n",
      "  timestamp: 1662329647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170136\n",
      "  training_iteration: 34\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 175140\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 175140\n",
      "    num_agent_steps_trained: 175140\n",
      "    num_env_steps_sampled: 175140\n",
      "    num_env_steps_trained: 175140\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78710092135817\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 27\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1641532356165829e-11\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4063332080841064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.716359913141787e-07\n",
      "          policy_loss: 0.0627739354968071\n",
      "          total_loss: 0.06277559697628021\n",
      "          vf_explained_var: 0.3232962489128113\n",
      "          vf_loss: 1.6502297057741089e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 175140\n",
      "    num_agent_steps_trained: 175140\n",
      "    num_env_steps_sampled: 175140\n",
      "    num_env_steps_trained: 175140\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 175140\n",
      "  num_agent_steps_trained: 175140\n",
      "  num_env_steps_sampled: 175140\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 175140\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.46153846153847\n",
      "    ram_util_percent: 36.292307692307695\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11757679494662951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.615168258905931\n",
      "    mean_inference_ms: 10.88755061200513\n",
      "    mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78710092135817\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11757679494662951\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.615168258905931\n",
      "      mean_inference_ms: 10.88755061200513\n",
      "      mean_raw_obs_processing_ms: 0.2031248324847098\n",
      "  time_since_restore: 328.9921700954437\n",
      "  time_this_iter_s: 9.252748727798462\n",
      "  time_total_s: 328.9921700954437\n",
      "  timers:\n",
      "    learn_throughput: 13356.313\n",
      "    learn_time_ms: 374.654\n",
      "    synch_weights_time_ms: 7.626\n",
      "    training_iteration_time_ms: 9278.75\n",
      "  timestamp: 1662329657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175140\n",
      "  training_iteration: 35\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 180144\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 180144\n",
      "    num_agent_steps_trained: 180144\n",
      "    num_env_steps_sampled: 180144\n",
      "    num_env_steps_trained: 180144\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-14-26\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78666484678386\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 36\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.8207661780829145e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4053242206573486\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0263872809446184e-06\n",
      "          policy_loss: 0.003156491322442889\n",
      "          total_loss: 0.018844859674572945\n",
      "          vf_explained_var: 0.027559122070670128\n",
      "          vf_loss: 0.01568836346268654\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 180144\n",
      "    num_agent_steps_trained: 180144\n",
      "    num_env_steps_sampled: 180144\n",
      "    num_env_steps_trained: 180144\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 180144\n",
      "  num_agent_steps_trained: 180144\n",
      "  num_env_steps_sampled: 180144\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 180144\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.7076923076923\n",
      "    ram_util_percent: 36.307692307692314\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11739604543208687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.618302773757628\n",
      "    mean_inference_ms: 10.887303178456188\n",
      "    mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78666484678386\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11739604543208687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.618302773757628\n",
      "      mean_inference_ms: 10.887303178456188\n",
      "      mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  time_since_restore: 338.25234627723694\n",
      "  time_this_iter_s: 9.260176181793213\n",
      "  time_total_s: 338.25234627723694\n",
      "  timers:\n",
      "    learn_throughput: 13373.068\n",
      "    learn_time_ms: 374.185\n",
      "    synch_weights_time_ms: 7.631\n",
      "    training_iteration_time_ms: 9253.528\n",
      "  timestamp: 1662329666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180144\n",
      "  training_iteration: 36\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=2009.50.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=2009.70.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 185148\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 185148\n",
      "    num_agent_steps_trained: 185148\n",
      "    num_env_steps_sampled: 185148\n",
      "    num_env_steps_trained: 185148\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-14-35\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78666484678386\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 36\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.9103830890414573e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.406970500946045\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.154122845851816e-06\n",
      "          policy_loss: 0.11036480963230133\n",
      "          total_loss: 5.334985733032227\n",
      "          vf_explained_var: -0.0001644134463276714\n",
      "          vf_loss: 5.224621295928955\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 185148\n",
      "    num_agent_steps_trained: 185148\n",
      "    num_env_steps_sampled: 185148\n",
      "    num_env_steps_trained: 185148\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 185148\n",
      "  num_agent_steps_trained: 185148\n",
      "  num_env_steps_sampled: 185148\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 185148\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.53076923076922\n",
      "    ram_util_percent: 36.33076923076924\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11739604543208687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.618302773757628\n",
      "    mean_inference_ms: 10.887303178456188\n",
      "    mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78666484678386\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11739604543208687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.618302773757628\n",
      "      mean_inference_ms: 10.887303178456188\n",
      "      mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  time_since_restore: 347.55936336517334\n",
      "  time_this_iter_s: 9.307017087936401\n",
      "  time_total_s: 347.55936336517334\n",
      "  timers:\n",
      "    learn_throughput: 13430.298\n",
      "    learn_time_ms: 372.59\n",
      "    synch_weights_time_ms: 7.637\n",
      "    training_iteration_time_ms: 9254.524\n",
      "  timestamp: 1662329675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185148\n",
      "  training_iteration: 37\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 190152\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 190152\n",
      "    num_agent_steps_trained: 190152\n",
      "    num_env_steps_sampled: 190152\n",
      "    num_env_steps_trained: 190152\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-14-44\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78666484678386\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 36\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.4551915445207286e-12\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3997390270233154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.823388456363318e-07\n",
      "          policy_loss: -0.09538985788822174\n",
      "          total_loss: -0.09534792602062225\n",
      "          vf_explained_var: 0.30457979440689087\n",
      "          vf_loss: 4.1952345782192424e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 190152\n",
      "    num_agent_steps_trained: 190152\n",
      "    num_env_steps_sampled: 190152\n",
      "    num_env_steps_trained: 190152\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 190152\n",
      "  num_agent_steps_trained: 190152\n",
      "  num_env_steps_sampled: 190152\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 190152\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.76923076923077\n",
      "    ram_util_percent: 36.292307692307695\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11739604543208687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.618302773757628\n",
      "    mean_inference_ms: 10.887303178456188\n",
      "    mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78666484678386\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11739604543208687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.618302773757628\n",
      "      mean_inference_ms: 10.887303178456188\n",
      "      mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  time_since_restore: 356.81618762016296\n",
      "  time_this_iter_s: 9.256824254989624\n",
      "  time_total_s: 356.81618762016296\n",
      "  timers:\n",
      "    learn_throughput: 13557.98\n",
      "    learn_time_ms: 369.082\n",
      "    synch_weights_time_ms: 8.794\n",
      "    training_iteration_time_ms: 9251.687\n",
      "  timestamp: 1662329684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190152\n",
      "  training_iteration: 38\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 195156\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 195156\n",
      "    num_agent_steps_trained: 195156\n",
      "    num_env_steps_sampled: 195156\n",
      "    num_env_steps_trained: 195156\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-14-54\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78666484678386\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 36\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.275957722603643e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3991037607192993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.8581475791943376e-07\n",
      "          policy_loss: 0.051867686212062836\n",
      "          total_loss: 0.05187263339757919\n",
      "          vf_explained_var: 0.20076386630535126\n",
      "          vf_loss: 4.954951691615861e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 195156\n",
      "    num_agent_steps_trained: 195156\n",
      "    num_env_steps_sampled: 195156\n",
      "    num_env_steps_trained: 195156\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 195156\n",
      "  num_agent_steps_trained: 195156\n",
      "  num_env_steps_sampled: 195156\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 195156\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 73.625\n",
      "    ram_util_percent: 36.175000000000004\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11739604543208687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.618302773757628\n",
      "    mean_inference_ms: 10.887303178456188\n",
      "    mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78666484678386\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11739604543208687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.618302773757628\n",
      "      mean_inference_ms: 10.887303178456188\n",
      "      mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  time_since_restore: 366.03795766830444\n",
      "  time_this_iter_s: 9.22177004814148\n",
      "  time_total_s: 366.03795766830444\n",
      "  timers:\n",
      "    learn_throughput: 13655.5\n",
      "    learn_time_ms: 366.446\n",
      "    synch_weights_time_ms: 8.744\n",
      "    training_iteration_time_ms: 9242.558\n",
      "  timestamp: 1662329694\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195156\n",
      "  training_iteration: 39\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 200160\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 200160\n",
      "    num_agent_steps_trained: 200160\n",
      "    num_env_steps_sampled: 200160\n",
      "    num_env_steps_trained: 200160\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-15-03\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78666484678386\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 36\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.6379788613018216e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3982704877853394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.572008963232292e-08\n",
      "          policy_loss: 0.03971303254365921\n",
      "          total_loss: 0.039716411381959915\n",
      "          vf_explained_var: 0.274273157119751\n",
      "          vf_loss: 3.3744304346328136e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 200160\n",
      "    num_agent_steps_trained: 200160\n",
      "    num_env_steps_sampled: 200160\n",
      "    num_env_steps_trained: 200160\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 200160\n",
      "  num_agent_steps_trained: 200160\n",
      "  num_env_steps_sampled: 200160\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 200160\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.66923076923078\n",
      "    ram_util_percent: 36.330769230769235\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11739604543208687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.618302773757628\n",
      "    mean_inference_ms: 10.887303178456188\n",
      "    mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78666484678386\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11739604543208687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.618302773757628\n",
      "      mean_inference_ms: 10.887303178456188\n",
      "      mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  time_since_restore: 375.2350549697876\n",
      "  time_this_iter_s: 9.197097301483154\n",
      "  time_total_s: 375.2350549697876\n",
      "  timers:\n",
      "    learn_throughput: 13740.4\n",
      "    learn_time_ms: 364.182\n",
      "    synch_weights_time_ms: 8.73\n",
      "    training_iteration_time_ms: 9239.139\n",
      "  timestamp: 1662329703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200160\n",
      "  training_iteration: 40\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 205164\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 205164\n",
      "    num_agent_steps_trained: 205164\n",
      "    num_env_steps_sampled: 205164\n",
      "    num_env_steps_trained: 205164\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-15-12\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78666484678386\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 36\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8189894306509108e-13\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3974072933197021\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4348637478178716e-06\n",
      "          policy_loss: 0.042471934109926224\n",
      "          total_loss: 0.0424751415848732\n",
      "          vf_explained_var: 0.18743708729743958\n",
      "          vf_loss: 3.2247153285425156e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 205164\n",
      "    num_agent_steps_trained: 205164\n",
      "    num_env_steps_sampled: 205164\n",
      "    num_env_steps_trained: 205164\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 205164\n",
      "  num_agent_steps_trained: 205164\n",
      "  num_env_steps_sampled: 205164\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 205164\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.23076923076923\n",
      "    ram_util_percent: 36.25384615384616\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11739604543208687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.618302773757628\n",
      "    mean_inference_ms: 10.887303178456188\n",
      "    mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78666484678386\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11739604543208687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.618302773757628\n",
      "      mean_inference_ms: 10.887303178456188\n",
      "      mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  time_since_restore: 384.4414496421814\n",
      "  time_this_iter_s: 9.206394672393799\n",
      "  time_total_s: 384.4414496421814\n",
      "  timers:\n",
      "    learn_throughput: 13678.082\n",
      "    learn_time_ms: 365.841\n",
      "    synch_weights_time_ms: 8.711\n",
      "    training_iteration_time_ms: 9219.981\n",
      "  timestamp: 1662329712\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205164\n",
      "  training_iteration: 41\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 210168\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 210168\n",
      "    num_agent_steps_trained: 210168\n",
      "    num_env_steps_sampled: 210168\n",
      "    num_env_steps_trained: 210168\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-15-21\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78666484678386\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 36\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.094947153254554e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3953828811645508\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.5086112600547494e-06\n",
      "          policy_loss: 0.08004169166088104\n",
      "          total_loss: 0.08004490286111832\n",
      "          vf_explained_var: 0.3208067715167999\n",
      "          vf_loss: 3.221014367227326e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 210168\n",
      "    num_agent_steps_trained: 210168\n",
      "    num_env_steps_sampled: 210168\n",
      "    num_env_steps_trained: 210168\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 210168\n",
      "  num_agent_steps_trained: 210168\n",
      "  num_env_steps_sampled: 210168\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 210168\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.25384615384614\n",
      "    ram_util_percent: 36.284615384615385\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11739604543208687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.618302773757628\n",
      "    mean_inference_ms: 10.887303178456188\n",
      "    mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78666484678386\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11739604543208687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.618302773757628\n",
      "      mean_inference_ms: 10.887303178456188\n",
      "      mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  time_since_restore: 393.72346568107605\n",
      "  time_this_iter_s: 9.282016038894653\n",
      "  time_total_s: 393.72346568107605\n",
      "  timers:\n",
      "    learn_throughput: 13627.735\n",
      "    learn_time_ms: 367.192\n",
      "    synch_weights_time_ms: 8.729\n",
      "    training_iteration_time_ms: 9234.079\n",
      "  timestamp: 1662329721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210168\n",
      "  training_iteration: 42\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 215172\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 215172\n",
      "    num_agent_steps_trained: 215172\n",
      "    num_env_steps_sampled: 215172\n",
      "    num_env_steps_trained: 215172\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-15-31\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78666484678386\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 36\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.547473576627277e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3935879468917847\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2440983709893771e-06\n",
      "          policy_loss: 0.058816999197006226\n",
      "          total_loss: 0.058820150792598724\n",
      "          vf_explained_var: 0.16411109268665314\n",
      "          vf_loss: 3.1518034120381344e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 215172\n",
      "    num_agent_steps_trained: 215172\n",
      "    num_env_steps_sampled: 215172\n",
      "    num_env_steps_trained: 215172\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 215172\n",
      "  num_agent_steps_trained: 215172\n",
      "  num_env_steps_sampled: 215172\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 215172\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.87692307692306\n",
      "    ram_util_percent: 36.20769230769231\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11739604543208687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.618302773757628\n",
      "    mean_inference_ms: 10.887303178456188\n",
      "    mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78666484678386\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11739604543208687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.618302773757628\n",
      "      mean_inference_ms: 10.887303178456188\n",
      "      mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  time_since_restore: 403.02618288993835\n",
      "  time_this_iter_s: 9.302717208862305\n",
      "  time_total_s: 403.02618288993835\n",
      "  timers:\n",
      "    learn_throughput: 13498.552\n",
      "    learn_time_ms: 370.706\n",
      "    synch_weights_time_ms: 8.762\n",
      "    training_iteration_time_ms: 9243.508\n",
      "  timestamp: 1662329731\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215172\n",
      "  training_iteration: 43\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 220176\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 220176\n",
      "    num_agent_steps_trained: 220176\n",
      "    num_env_steps_sampled: 220176\n",
      "    num_env_steps_trained: 220176\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-15-40\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78666484678386\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 36\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2737367883136385e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3931264877319336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.788960836776823e-07\n",
      "          policy_loss: -0.11044351011514664\n",
      "          total_loss: -0.1104404479265213\n",
      "          vf_explained_var: 0.1052631363272667\n",
      "          vf_loss: 3.0370911190402694e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 220176\n",
      "    num_agent_steps_trained: 220176\n",
      "    num_env_steps_sampled: 220176\n",
      "    num_env_steps_trained: 220176\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 220176\n",
      "  num_agent_steps_trained: 220176\n",
      "  num_env_steps_sampled: 220176\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 220176\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.77499999999999\n",
      "    ram_util_percent: 36.208333333333336\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11739604543208687\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.618302773757628\n",
      "    mean_inference_ms: 10.887303178456188\n",
      "    mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78666484678386\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11739604543208687\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.618302773757628\n",
      "      mean_inference_ms: 10.887303178456188\n",
      "      mean_raw_obs_processing_ms: 0.20386609297398336\n",
      "  time_since_restore: 412.08413100242615\n",
      "  time_this_iter_s: 9.057948112487793\n",
      "  time_total_s: 412.08413100242615\n",
      "  timers:\n",
      "    learn_throughput: 13609.353\n",
      "    learn_time_ms: 367.688\n",
      "    synch_weights_time_ms: 8.636\n",
      "    training_iteration_time_ms: 9226.994\n",
      "  timestamp: 1662329740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220176\n",
      "  training_iteration: 44\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 225180\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 225180\n",
      "    num_agent_steps_trained: 225180\n",
      "    num_env_steps_sampled: 225180\n",
      "    num_env_steps_trained: 225180\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-15-49\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7863039182708\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 45\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1368683941568192e-14\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3946430683135986\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3961169997855905e-07\n",
      "          policy_loss: 0.07209224998950958\n",
      "          total_loss: 0.09396214038133621\n",
      "          vf_explained_var: 0.1281842589378357\n",
      "          vf_loss: 0.021869884803891182\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 225180\n",
      "    num_agent_steps_trained: 225180\n",
      "    num_env_steps_sampled: 225180\n",
      "    num_env_steps_trained: 225180\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 225180\n",
      "  num_agent_steps_trained: 225180\n",
      "  num_env_steps_sampled: 225180\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 225180\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.8923076923077\n",
      "    ram_util_percent: 36.26923076923077\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11725829824326148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.616357141128515\n",
      "    mean_inference_ms: 10.883925909286212\n",
      "    mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7863039182708\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11725829824326148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.616357141128515\n",
      "      mean_inference_ms: 10.883925909286212\n",
      "      mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  time_since_restore: 421.2897992134094\n",
      "  time_this_iter_s: 9.205668210983276\n",
      "  time_total_s: 421.2897992134094\n",
      "  timers:\n",
      "    learn_throughput: 13639.767\n",
      "    learn_time_ms: 366.868\n",
      "    synch_weights_time_ms: 8.606\n",
      "    training_iteration_time_ms: 9222.323\n",
      "  timestamp: 1662329749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225180\n",
      "  training_iteration: 45\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=2509.60.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=2509.80.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 230184\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 230184\n",
      "    num_agent_steps_trained: 230184\n",
      "    num_env_steps_sampled: 230184\n",
      "    num_env_steps_trained: 230184\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-15-58\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7863039182708\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.684341970784096e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3997843265533447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.6863805285538547e-06\n",
      "          policy_loss: -0.04143005609512329\n",
      "          total_loss: 5.706910610198975\n",
      "          vf_explained_var: -0.0006967782974243164\n",
      "          vf_loss: 5.748341083526611\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 230184\n",
      "    num_agent_steps_trained: 230184\n",
      "    num_env_steps_sampled: 230184\n",
      "    num_env_steps_trained: 230184\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 230184\n",
      "  num_agent_steps_trained: 230184\n",
      "  num_env_steps_sampled: 230184\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 230184\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.3\n",
      "    ram_util_percent: 36.284615384615385\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11725829824326148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.616357141128515\n",
      "    mean_inference_ms: 10.883925909286212\n",
      "    mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7863039182708\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11725829824326148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.616357141128515\n",
      "      mean_inference_ms: 10.883925909286212\n",
      "      mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  time_since_restore: 430.51495695114136\n",
      "  time_this_iter_s: 9.225157737731934\n",
      "  time_total_s: 430.51495695114136\n",
      "  timers:\n",
      "    learn_throughput: 13623.985\n",
      "    learn_time_ms: 367.293\n",
      "    synch_weights_time_ms: 8.673\n",
      "    training_iteration_time_ms: 9218.922\n",
      "  timestamp: 1662329758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230184\n",
      "  training_iteration: 46\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 235188\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 235188\n",
      "    num_agent_steps_trained: 235188\n",
      "    num_env_steps_sampled: 235188\n",
      "    num_env_steps_trained: 235188\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-16-07\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7863039182708\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.842170985392048e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3920247554779053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.760190111279371e-06\n",
      "          policy_loss: -0.1655329465866089\n",
      "          total_loss: -0.16543710231781006\n",
      "          vf_explained_var: 0.1413135826587677\n",
      "          vf_loss: 9.58669843384996e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 235188\n",
      "    num_agent_steps_trained: 235188\n",
      "    num_env_steps_sampled: 235188\n",
      "    num_env_steps_trained: 235188\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 235188\n",
      "  num_agent_steps_trained: 235188\n",
      "  num_env_steps_sampled: 235188\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 235188\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 81.74166666666667\n",
      "    ram_util_percent: 36.225\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11725829824326148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.616357141128515\n",
      "    mean_inference_ms: 10.883925909286212\n",
      "    mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7863039182708\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11725829824326148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.616357141128515\n",
      "      mean_inference_ms: 10.883925909286212\n",
      "      mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  time_since_restore: 439.59491086006165\n",
      "  time_this_iter_s: 9.079953908920288\n",
      "  time_total_s: 439.59491086006165\n",
      "  timers:\n",
      "    learn_throughput: 13628.441\n",
      "    learn_time_ms: 367.173\n",
      "    synch_weights_time_ms: 8.615\n",
      "    training_iteration_time_ms: 9196.225\n",
      "  timestamp: 1662329767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235188\n",
      "  training_iteration: 47\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 240192\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 240192\n",
      "    num_agent_steps_trained: 240192\n",
      "    num_env_steps_sampled: 240192\n",
      "    num_env_steps_trained: 240192\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-16-17\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7863039182708\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.421085492696024e-15\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3944461345672607\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0542169093241682e-06\n",
      "          policy_loss: -0.002760919975116849\n",
      "          total_loss: -0.002750598592683673\n",
      "          vf_explained_var: 0.2532237470149994\n",
      "          vf_loss: 1.032892851071665e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 240192\n",
      "    num_agent_steps_trained: 240192\n",
      "    num_env_steps_sampled: 240192\n",
      "    num_env_steps_trained: 240192\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 240192\n",
      "  num_agent_steps_trained: 240192\n",
      "  num_env_steps_sampled: 240192\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 240192\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.36923076923077\n",
      "    ram_util_percent: 36.261538461538464\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11725829824326148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.616357141128515\n",
      "    mean_inference_ms: 10.883925909286212\n",
      "    mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7863039182708\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11725829824326148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.616357141128515\n",
      "      mean_inference_ms: 10.883925909286212\n",
      "      mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  time_since_restore: 448.87284660339355\n",
      "  time_this_iter_s: 9.27793574333191\n",
      "  time_total_s: 448.87284660339355\n",
      "  timers:\n",
      "    learn_throughput: 13645.488\n",
      "    learn_time_ms: 366.715\n",
      "    synch_weights_time_ms: 7.381\n",
      "    training_iteration_time_ms: 9198.091\n",
      "  timestamp: 1662329777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240192\n",
      "  training_iteration: 48\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 245196\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 245196\n",
      "    num_agent_steps_trained: 245196\n",
      "    num_env_steps_sampled: 245196\n",
      "    num_env_steps_trained: 245196\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-16-26\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7863039182708\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.10542746348012e-16\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3961353302001953\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.9211978471721523e-06\n",
      "          policy_loss: -0.08387061953544617\n",
      "          total_loss: -0.08386369049549103\n",
      "          vf_explained_var: 0.11585849523544312\n",
      "          vf_loss: 6.942119398445357e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 245196\n",
      "    num_agent_steps_trained: 245196\n",
      "    num_env_steps_sampled: 245196\n",
      "    num_env_steps_trained: 245196\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 245196\n",
      "  num_agent_steps_trained: 245196\n",
      "  num_env_steps_sampled: 245196\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 245196\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.44615384615385\n",
      "    ram_util_percent: 36.292307692307695\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11725829824326148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.616357141128515\n",
      "    mean_inference_ms: 10.883925909286212\n",
      "    mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7863039182708\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11725829824326148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.616357141128515\n",
      "      mean_inference_ms: 10.883925909286212\n",
      "      mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  time_since_restore: 458.1057996749878\n",
      "  time_this_iter_s: 9.232953071594238\n",
      "  time_total_s: 458.1057996749878\n",
      "  timers:\n",
      "    learn_throughput: 13638.866\n",
      "    learn_time_ms: 366.893\n",
      "    synch_weights_time_ms: 7.424\n",
      "    training_iteration_time_ms: 9199.263\n",
      "  timestamp: 1662329786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245196\n",
      "  training_iteration: 49\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 250200\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 250200\n",
      "    num_agent_steps_trained: 250200\n",
      "    num_env_steps_sampled: 250200\n",
      "    num_env_steps_trained: 250200\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7863039182708\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.55271373174006e-16\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3976160287857056\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.948897304122511e-07\n",
      "          policy_loss: 0.005614115856587887\n",
      "          total_loss: 0.005620849318802357\n",
      "          vf_explained_var: 0.35679659247398376\n",
      "          vf_loss: 6.737077455909457e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 250200\n",
      "    num_agent_steps_trained: 250200\n",
      "    num_env_steps_sampled: 250200\n",
      "    num_env_steps_trained: 250200\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 250200\n",
      "  num_agent_steps_trained: 250200\n",
      "  num_env_steps_sampled: 250200\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 250200\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.26153846153846\n",
      "    ram_util_percent: 36.24615384615385\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11725829824326148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.616357141128515\n",
      "    mean_inference_ms: 10.883925909286212\n",
      "    mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7863039182708\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11725829824326148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.616357141128515\n",
      "      mean_inference_ms: 10.883925909286212\n",
      "      mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  time_since_restore: 467.30726742744446\n",
      "  time_this_iter_s: 9.201467752456665\n",
      "  time_total_s: 467.30726742744446\n",
      "  timers:\n",
      "    learn_throughput: 13605.228\n",
      "    learn_time_ms: 367.8\n",
      "    synch_weights_time_ms: 7.454\n",
      "    training_iteration_time_ms: 9199.768\n",
      "  timestamp: 1662329795\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250200\n",
      "  training_iteration: 50\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 255204\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 255204\n",
      "    num_agent_steps_trained: 255204\n",
      "    num_env_steps_sampled: 255204\n",
      "    num_env_steps_trained: 255204\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-16-45\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7863039182708\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.77635686587003e-16\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.39774489402771\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4212881183084392e-07\n",
      "          policy_loss: -0.06932436674833298\n",
      "          total_loss: -0.06931781768798828\n",
      "          vf_explained_var: 0.13859491050243378\n",
      "          vf_loss: 6.563676834048238e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 255204\n",
      "    num_agent_steps_trained: 255204\n",
      "    num_env_steps_sampled: 255204\n",
      "    num_env_steps_trained: 255204\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 255204\n",
      "  num_agent_steps_trained: 255204\n",
      "  num_env_steps_sampled: 255204\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 255204\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.38461538461539\n",
      "    ram_util_percent: 36.23846153846154\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11725829824326148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.616357141128515\n",
      "    mean_inference_ms: 10.883925909286212\n",
      "    mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7863039182708\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11725829824326148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.616357141128515\n",
      "      mean_inference_ms: 10.883925909286212\n",
      "      mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  time_since_restore: 476.63023853302\n",
      "  time_this_iter_s: 9.322971105575562\n",
      "  time_total_s: 476.63023853302\n",
      "  timers:\n",
      "    learn_throughput: 13641.454\n",
      "    learn_time_ms: 366.823\n",
      "    synch_weights_time_ms: 7.47\n",
      "    training_iteration_time_ms: 9211.313\n",
      "  timestamp: 1662329805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255204\n",
      "  training_iteration: 51\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 260208\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 260208\n",
      "    num_agent_steps_trained: 260208\n",
      "    num_env_steps_sampled: 260208\n",
      "    num_env_steps_trained: 260208\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-16-54\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7863039182708\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.88178432935015e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3992226123809814\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.0925526769133285e-06\n",
      "          policy_loss: -0.2758021950721741\n",
      "          total_loss: -0.27579596638679504\n",
      "          vf_explained_var: 0.15678006410598755\n",
      "          vf_loss: 6.216951533133397e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 260208\n",
      "    num_agent_steps_trained: 260208\n",
      "    num_env_steps_sampled: 260208\n",
      "    num_env_steps_trained: 260208\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 260208\n",
      "  num_agent_steps_trained: 260208\n",
      "  num_env_steps_sampled: 260208\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 260208\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.75384615384615\n",
      "    ram_util_percent: 36.23846153846154\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11725829824326148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.616357141128515\n",
      "    mean_inference_ms: 10.883925909286212\n",
      "    mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7863039182708\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11725829824326148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.616357141128515\n",
      "      mean_inference_ms: 10.883925909286212\n",
      "      mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  time_since_restore: 485.8537254333496\n",
      "  time_this_iter_s: 9.22348690032959\n",
      "  time_total_s: 485.8537254333496\n",
      "  timers:\n",
      "    learn_throughput: 13606.0\n",
      "    learn_time_ms: 367.779\n",
      "    synch_weights_time_ms: 7.486\n",
      "    training_iteration_time_ms: 9205.456\n",
      "  timestamp: 1662329814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260208\n",
      "  training_iteration: 52\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 265212\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 265212\n",
      "    num_agent_steps_trained: 265212\n",
      "    num_env_steps_sampled: 265212\n",
      "    num_env_steps_trained: 265212\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-17-03\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7863039182708\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 45\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.440892164675075e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4043099880218506\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.783523637452163e-06\n",
      "          policy_loss: -0.0350448340177536\n",
      "          total_loss: -0.0350383035838604\n",
      "          vf_explained_var: 0.2282876968383789\n",
      "          vf_loss: 6.540954927913845e-06\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 265212\n",
      "    num_agent_steps_trained: 265212\n",
      "    num_env_steps_sampled: 265212\n",
      "    num_env_steps_trained: 265212\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 265212\n",
      "  num_agent_steps_trained: 265212\n",
      "  num_env_steps_sampled: 265212\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 265212\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.68333333333334\n",
      "    ram_util_percent: 36.2\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11725829824326148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.616357141128515\n",
      "    mean_inference_ms: 10.883925909286212\n",
      "    mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7863039182708\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11725829824326148\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.616357141128515\n",
      "      mean_inference_ms: 10.883925909286212\n",
      "      mean_raw_obs_processing_ms: 0.20422317706726273\n",
      "  time_since_restore: 495.08471870422363\n",
      "  time_this_iter_s: 9.230993270874023\n",
      "  time_total_s: 495.08471870422363\n",
      "  timers:\n",
      "    learn_throughput: 13680.335\n",
      "    learn_time_ms: 365.781\n",
      "    synch_weights_time_ms: 7.493\n",
      "    training_iteration_time_ms: 9198.317\n",
      "  timestamp: 1662329823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265212\n",
      "  training_iteration: 53\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 270216\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 270216\n",
      "    num_agent_steps_trained: 270216\n",
      "    num_env_steps_sampled: 270216\n",
      "    num_env_steps_trained: 270216\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-17-13\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7861282275023\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 54\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.2204460823375376e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4072964191436768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.405035838317417e-07\n",
      "          policy_loss: -0.04072218015789986\n",
      "          total_loss: 0.08208556473255157\n",
      "          vf_explained_var: 0.09125333279371262\n",
      "          vf_loss: 0.12280775606632233\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 270216\n",
      "    num_agent_steps_trained: 270216\n",
      "    num_env_steps_sampled: 270216\n",
      "    num_env_steps_trained: 270216\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 270216\n",
      "  num_agent_steps_trained: 270216\n",
      "  num_env_steps_sampled: 270216\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 270216\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.72142857142858\n",
      "    ram_util_percent: 36.414285714285725\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715655222837341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.6132516473047795\n",
      "    mean_inference_ms: 10.878359334779178\n",
      "    mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7861282275023\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11715655222837341\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.6132516473047795\n",
      "      mean_inference_ms: 10.878359334779178\n",
      "      mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  time_since_restore: 504.53468465805054\n",
      "  time_this_iter_s: 9.449965953826904\n",
      "  time_total_s: 504.53468465805054\n",
      "  timers:\n",
      "    learn_throughput: 13561.835\n",
      "    learn_time_ms: 368.977\n",
      "    synch_weights_time_ms: 7.581\n",
      "    training_iteration_time_ms: 9237.438\n",
      "  timestamp: 1662329833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270216\n",
      "  training_iteration: 54\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=3009.90.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=3009.70.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 275220\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 275220\n",
      "    num_agent_steps_trained: 275220\n",
      "    num_env_steps_sampled: 275220\n",
      "    num_env_steps_trained: 275220\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7861282275023\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 54\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1102230411687688e-17\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.413654088973999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.044286232689046e-07\n",
      "          policy_loss: 0.030261129140853882\n",
      "          total_loss: 5.1338911056518555\n",
      "          vf_explained_var: -0.000663244747556746\n",
      "          vf_loss: 5.1036295890808105\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 275220\n",
      "    num_agent_steps_trained: 275220\n",
      "    num_env_steps_sampled: 275220\n",
      "    num_env_steps_trained: 275220\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 275220\n",
      "  num_agent_steps_trained: 275220\n",
      "  num_env_steps_sampled: 275220\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 275220\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 85.56666666666668\n",
      "    ram_util_percent: 36.21666666666667\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715655222837341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.6132516473047795\n",
      "    mean_inference_ms: 10.878359334779178\n",
      "    mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7861282275023\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11715655222837341\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.6132516473047795\n",
      "      mean_inference_ms: 10.878359334779178\n",
      "      mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  time_since_restore: 513.7044167518616\n",
      "  time_this_iter_s: 9.169732093811035\n",
      "  time_total_s: 513.7044167518616\n",
      "  timers:\n",
      "    learn_throughput: 13633.756\n",
      "    learn_time_ms: 367.03\n",
      "    synch_weights_time_ms: 7.532\n",
      "    training_iteration_time_ms: 9233.772\n",
      "  timestamp: 1662329842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275220\n",
      "  training_iteration: 55\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 280224\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 280224\n",
      "    num_agent_steps_trained: 280224\n",
      "    num_env_steps_sampled: 280224\n",
      "    num_env_steps_trained: 280224\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-17-31\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7861282275023\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 54\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.551115205843844e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4042168855667114\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.063785005811951e-07\n",
      "          policy_loss: -0.03598397970199585\n",
      "          total_loss: -0.03584786877036095\n",
      "          vf_explained_var: 0.12273678928613663\n",
      "          vf_loss: 0.00013610281166620553\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 280224\n",
      "    num_agent_steps_trained: 280224\n",
      "    num_env_steps_sampled: 280224\n",
      "    num_env_steps_trained: 280224\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 280224\n",
      "  num_agent_steps_trained: 280224\n",
      "  num_env_steps_sampled: 280224\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 280224\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.73076923076923\n",
      "    ram_util_percent: 36.29230769230771\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715655222837341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.6132516473047795\n",
      "    mean_inference_ms: 10.878359334779178\n",
      "    mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7861282275023\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11715655222837341\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.6132516473047795\n",
      "      mean_inference_ms: 10.878359334779178\n",
      "      mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  time_since_restore: 522.8184616565704\n",
      "  time_this_iter_s: 9.114044904708862\n",
      "  time_total_s: 522.8184616565704\n",
      "  timers:\n",
      "    learn_throughput: 13719.763\n",
      "    learn_time_ms: 364.729\n",
      "    synch_weights_time_ms: 7.418\n",
      "    training_iteration_time_ms: 9222.571\n",
      "  timestamp: 1662329851\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280224\n",
      "  training_iteration: 56\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 285228\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 285228\n",
      "    num_agent_steps_trained: 285228\n",
      "    num_env_steps_sampled: 285228\n",
      "    num_env_steps_trained: 285228\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-17-40\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7861282275023\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 54\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.775557602921922e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4035342931747437\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.515592024745274e-07\n",
      "          policy_loss: 0.09728637337684631\n",
      "          total_loss: 0.09730484336614609\n",
      "          vf_explained_var: 0.3378432095050812\n",
      "          vf_loss: 1.846674786065705e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 285228\n",
      "    num_agent_steps_trained: 285228\n",
      "    num_env_steps_sampled: 285228\n",
      "    num_env_steps_trained: 285228\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 285228\n",
      "  num_agent_steps_trained: 285228\n",
      "  num_env_steps_sampled: 285228\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 285228\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.89230769230768\n",
      "    ram_util_percent: 36.35384615384616\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715655222837341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.6132516473047795\n",
      "    mean_inference_ms: 10.878359334779178\n",
      "    mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7861282275023\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11715655222837341\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.6132516473047795\n",
      "      mean_inference_ms: 10.878359334779178\n",
      "      mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  time_since_restore: 532.0935587882996\n",
      "  time_this_iter_s: 9.275097131729126\n",
      "  time_total_s: 532.0935587882996\n",
      "  timers:\n",
      "    learn_throughput: 13698.206\n",
      "    learn_time_ms: 365.303\n",
      "    synch_weights_time_ms: 7.518\n",
      "    training_iteration_time_ms: 9242.128\n",
      "  timestamp: 1662329860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285228\n",
      "  training_iteration: 57\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 290232\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 290232\n",
      "    num_agent_steps_trained: 290232\n",
      "    num_env_steps_sampled: 290232\n",
      "    num_env_steps_trained: 290232\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-17-49\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7861282275023\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 54\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.387778801460961e-18\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.401235818862915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.8365081966039725e-06\n",
      "          policy_loss: 0.11106546223163605\n",
      "          total_loss: 0.11107907444238663\n",
      "          vf_explained_var: 0.014096545986831188\n",
      "          vf_loss: 1.3609857887786347e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 290232\n",
      "    num_agent_steps_trained: 290232\n",
      "    num_env_steps_sampled: 290232\n",
      "    num_env_steps_trained: 290232\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 290232\n",
      "  num_agent_steps_trained: 290232\n",
      "  num_env_steps_sampled: 290232\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 290232\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.86923076923077\n",
      "    ram_util_percent: 36.415384615384625\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715655222837341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.6132516473047795\n",
      "    mean_inference_ms: 10.878359334779178\n",
      "    mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7861282275023\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11715655222837341\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.6132516473047795\n",
      "      mean_inference_ms: 10.878359334779178\n",
      "      mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  time_since_restore: 541.4126706123352\n",
      "  time_this_iter_s: 9.319111824035645\n",
      "  time_total_s: 541.4126706123352\n",
      "  timers:\n",
      "    learn_throughput: 13632.381\n",
      "    learn_time_ms: 367.067\n",
      "    synch_weights_time_ms: 7.561\n",
      "    training_iteration_time_ms: 9246.135\n",
      "  timestamp: 1662329869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290232\n",
      "  training_iteration: 58\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 295236\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 295236\n",
      "    num_agent_steps_trained: 295236\n",
      "    num_env_steps_sampled: 295236\n",
      "    num_env_steps_trained: 295236\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-17-59\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7861282275023\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 54\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.938894007304805e-19\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3998419046401978\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.4226082473433053e-07\n",
      "          policy_loss: -0.1969439685344696\n",
      "          total_loss: -0.19693176448345184\n",
      "          vf_explained_var: 0.1788104921579361\n",
      "          vf_loss: 1.2218272786412854e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 295236\n",
      "    num_agent_steps_trained: 295236\n",
      "    num_env_steps_sampled: 295236\n",
      "    num_env_steps_trained: 295236\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 295236\n",
      "  num_agent_steps_trained: 295236\n",
      "  num_env_steps_sampled: 295236\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 295236\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.96153846153847\n",
      "    ram_util_percent: 36.307692307692314\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715655222837341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.6132516473047795\n",
      "    mean_inference_ms: 10.878359334779178\n",
      "    mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7861282275023\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11715655222837341\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.6132516473047795\n",
      "      mean_inference_ms: 10.878359334779178\n",
      "      mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  time_since_restore: 550.7853252887726\n",
      "  time_this_iter_s: 9.372654676437378\n",
      "  time_total_s: 550.7853252887726\n",
      "  timers:\n",
      "    learn_throughput: 13533.906\n",
      "    learn_time_ms: 369.738\n",
      "    synch_weights_time_ms: 7.591\n",
      "    training_iteration_time_ms: 9260.076\n",
      "  timestamp: 1662329879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295236\n",
      "  training_iteration: 59\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 300240\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 300240\n",
      "    num_agent_steps_trained: 300240\n",
      "    num_env_steps_sampled: 300240\n",
      "    num_env_steps_trained: 300240\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-18-08\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7861282275023\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 54\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.4694470036524025e-19\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.403240442276001\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.825000233831815e-06\n",
      "          policy_loss: -0.028841117396950722\n",
      "          total_loss: -0.028828401118516922\n",
      "          vf_explained_var: 0.18871361017227173\n",
      "          vf_loss: 1.2725827218673658e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 300240\n",
      "    num_agent_steps_trained: 300240\n",
      "    num_env_steps_sampled: 300240\n",
      "    num_env_steps_trained: 300240\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 300240\n",
      "  num_agent_steps_trained: 300240\n",
      "  num_env_steps_sampled: 300240\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 300240\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.31538461538462\n",
      "    ram_util_percent: 36.346153846153854\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715655222837341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.6132516473047795\n",
      "    mean_inference_ms: 10.878359334779178\n",
      "    mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7861282275023\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11715655222837341\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.6132516473047795\n",
      "      mean_inference_ms: 10.878359334779178\n",
      "      mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  time_since_restore: 560.1086423397064\n",
      "  time_this_iter_s: 9.323317050933838\n",
      "  time_total_s: 560.1086423397064\n",
      "  timers:\n",
      "    learn_throughput: 13522.594\n",
      "    learn_time_ms: 370.047\n",
      "    synch_weights_time_ms: 7.635\n",
      "    training_iteration_time_ms: 9272.19\n",
      "  timestamp: 1662329888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300240\n",
      "  training_iteration: 60\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 305244\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 305244\n",
      "    num_agent_steps_trained: 305244\n",
      "    num_env_steps_sampled: 305244\n",
      "    num_env_steps_trained: 305244\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-18-17\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7861282275023\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 54\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.7347235018262012e-19\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4042415618896484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.865492660930613e-07\n",
      "          policy_loss: 0.08142506331205368\n",
      "          total_loss: 0.08143787831068039\n",
      "          vf_explained_var: 0.3145456910133362\n",
      "          vf_loss: 1.2824277291656472e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 305244\n",
      "    num_agent_steps_trained: 305244\n",
      "    num_env_steps_sampled: 305244\n",
      "    num_env_steps_trained: 305244\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 305244\n",
      "  num_agent_steps_trained: 305244\n",
      "  num_env_steps_sampled: 305244\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 305244\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.8923076923077\n",
      "    ram_util_percent: 36.323076923076925\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715655222837341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.6132516473047795\n",
      "    mean_inference_ms: 10.878359334779178\n",
      "    mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7861282275023\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11715655222837341\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.6132516473047795\n",
      "      mean_inference_ms: 10.878359334779178\n",
      "      mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  time_since_restore: 569.299667596817\n",
      "  time_this_iter_s: 9.191025257110596\n",
      "  time_total_s: 569.299667596817\n",
      "  timers:\n",
      "    learn_throughput: 13580.756\n",
      "    learn_time_ms: 368.463\n",
      "    synch_weights_time_ms: 8.985\n",
      "    training_iteration_time_ms: 9259.213\n",
      "  timestamp: 1662329897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305244\n",
      "  training_iteration: 61\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 310248\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 310248\n",
      "    num_agent_steps_trained: 310248\n",
      "    num_env_steps_sampled: 310248\n",
      "    num_env_steps_trained: 310248\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-18-27\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.7861282275023\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 54\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.673617509131006e-20\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4026387929916382\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.9691683519340586e-06\n",
      "          policy_loss: -0.05208946019411087\n",
      "          total_loss: -0.05207720398902893\n",
      "          vf_explained_var: 0.18873348832130432\n",
      "          vf_loss: 1.2266171324881725e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 310248\n",
      "    num_agent_steps_trained: 310248\n",
      "    num_env_steps_sampled: 310248\n",
      "    num_env_steps_trained: 310248\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 310248\n",
      "  num_agent_steps_trained: 310248\n",
      "  num_env_steps_sampled: 310248\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 310248\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.71666666666665\n",
      "    ram_util_percent: 36.25\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11715655222837341\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.6132516473047795\n",
      "    mean_inference_ms: 10.878359334779178\n",
      "    mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.7861282275023\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11715655222837341\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.6132516473047795\n",
      "      mean_inference_ms: 10.878359334779178\n",
      "      mean_raw_obs_processing_ms: 0.20459883936284132\n",
      "  time_since_restore: 578.4919786453247\n",
      "  time_this_iter_s: 9.19231104850769\n",
      "  time_total_s: 578.4919786453247\n",
      "  timers:\n",
      "    learn_throughput: 13662.155\n",
      "    learn_time_ms: 366.267\n",
      "    synch_weights_time_ms: 8.931\n",
      "    training_iteration_time_ms: 9256.13\n",
      "  timestamp: 1662329907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310248\n",
      "  training_iteration: 62\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 315252\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 315252\n",
      "    num_agent_steps_trained: 315252\n",
      "    num_env_steps_sampled: 315252\n",
      "    num_env_steps_trained: 315252\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-18-36\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78592085034302\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 63\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.336808754565503e-20\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.4033256769180298\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.1480743700740277e-07\n",
      "          policy_loss: 0.08508024364709854\n",
      "          total_loss: 0.18648341298103333\n",
      "          vf_explained_var: 0.030044052749872208\n",
      "          vf_loss: 0.10140317678451538\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 315252\n",
      "    num_agent_steps_trained: 315252\n",
      "    num_env_steps_sampled: 315252\n",
      "    num_env_steps_trained: 315252\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 315252\n",
      "  num_agent_steps_trained: 315252\n",
      "  num_env_steps_sampled: 315252\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 315252\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.26153846153846\n",
      "    ram_util_percent: 36.215384615384615\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11708532382430256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.609976640872842\n",
      "    mean_inference_ms: 10.873431618866745\n",
      "    mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78592085034302\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11708532382430256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.609976640872842\n",
      "      mean_inference_ms: 10.873431618866745\n",
      "      mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  time_since_restore: 587.9828066825867\n",
      "  time_this_iter_s: 9.490828037261963\n",
      "  time_total_s: 587.9828066825867\n",
      "  timers:\n",
      "    learn_throughput: 13698.703\n",
      "    learn_time_ms: 365.29\n",
      "    synch_weights_time_ms: 8.939\n",
      "    training_iteration_time_ms: 9282.055\n",
      "  timestamp: 1662329916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315252\n",
      "  training_iteration: 63\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=3509.80.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=3510.00.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 320256\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 320256\n",
      "    num_agent_steps_trained: 320256\n",
      "    num_env_steps_sampled: 320256\n",
      "    num_env_steps_trained: 320256\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-18-45\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78592085034302\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 63\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.1684043772827515e-20\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.408850908279419\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.466590209834976e-06\n",
      "          policy_loss: 0.08965369313955307\n",
      "          total_loss: 4.758111000061035\n",
      "          vf_explained_var: -1.4153122720017564e-05\n",
      "          vf_loss: 4.66845703125\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 320256\n",
      "    num_agent_steps_trained: 320256\n",
      "    num_env_steps_sampled: 320256\n",
      "    num_env_steps_trained: 320256\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 320256\n",
      "  num_agent_steps_trained: 320256\n",
      "  num_env_steps_sampled: 320256\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 320256\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.66153846153847\n",
      "    ram_util_percent: 36.415384615384625\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11708532382430256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.609976640872842\n",
      "    mean_inference_ms: 10.873431618866745\n",
      "    mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78592085034302\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11708532382430256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.609976640872842\n",
      "      mean_inference_ms: 10.873431618866745\n",
      "      mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  time_since_restore: 597.0481009483337\n",
      "  time_this_iter_s: 9.06529426574707\n",
      "  time_total_s: 597.0481009483337\n",
      "  timers:\n",
      "    learn_throughput: 13695.752\n",
      "    learn_time_ms: 365.369\n",
      "    synch_weights_time_ms: 8.904\n",
      "    training_iteration_time_ms: 9243.631\n",
      "  timestamp: 1662329925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320256\n",
      "  training_iteration: 64\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 325260\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 325260\n",
      "    num_agent_steps_trained: 325260\n",
      "    num_env_steps_sampled: 325260\n",
      "    num_env_steps_trained: 325260\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-18-54\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78592085034302\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 63\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0842021886413758e-20\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.395918607711792\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.163101039011963e-06\n",
      "          policy_loss: 0.04266392067074776\n",
      "          total_loss: 0.04282797500491142\n",
      "          vf_explained_var: 0.288634330034256\n",
      "          vf_loss: 0.00016405817586928606\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 325260\n",
      "    num_agent_steps_trained: 325260\n",
      "    num_env_steps_sampled: 325260\n",
      "    num_env_steps_trained: 325260\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 325260\n",
      "  num_agent_steps_trained: 325260\n",
      "  num_env_steps_sampled: 325260\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 325260\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.53076923076922\n",
      "    ram_util_percent: 36.384615384615394\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11708532382430256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.609976640872842\n",
      "    mean_inference_ms: 10.873431618866745\n",
      "    mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78592085034302\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11708532382430256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.609976640872842\n",
      "      mean_inference_ms: 10.873431618866745\n",
      "      mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  time_since_restore: 606.2456641197205\n",
      "  time_this_iter_s: 9.197563171386719\n",
      "  time_total_s: 606.2456641197205\n",
      "  timers:\n",
      "    learn_throughput: 13649.911\n",
      "    learn_time_ms: 366.596\n",
      "    synch_weights_time_ms: 10.192\n",
      "    training_iteration_time_ms: 9246.437\n",
      "  timestamp: 1662329934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325260\n",
      "  training_iteration: 65\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 330264\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 330264\n",
      "    num_agent_steps_trained: 330264\n",
      "    num_env_steps_sampled: 330264\n",
      "    num_env_steps_trained: 330264\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-19-04\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78592085034302\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 63\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.421010943206879e-21\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3938367366790771\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.130957323577604e-07\n",
      "          policy_loss: 0.06165289878845215\n",
      "          total_loss: 0.061680007725954056\n",
      "          vf_explained_var: 0.28264373540878296\n",
      "          vf_loss: 2.7103724278276786e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 330264\n",
      "    num_agent_steps_trained: 330264\n",
      "    num_env_steps_sampled: 330264\n",
      "    num_env_steps_trained: 330264\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 330264\n",
      "  num_agent_steps_trained: 330264\n",
      "  num_env_steps_sampled: 330264\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 330264\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.18333333333332\n",
      "    ram_util_percent: 36.32500000000001\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11708532382430256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.609976640872842\n",
      "    mean_inference_ms: 10.873431618866745\n",
      "    mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78592085034302\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11708532382430256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.609976640872842\n",
      "      mean_inference_ms: 10.873431618866745\n",
      "      mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  time_since_restore: 615.5527899265289\n",
      "  time_this_iter_s: 9.307125806808472\n",
      "  time_total_s: 615.5527899265289\n",
      "  timers:\n",
      "    learn_throughput: 13580.9\n",
      "    learn_time_ms: 368.459\n",
      "    synch_weights_time_ms: 10.227\n",
      "    training_iteration_time_ms: 9265.813\n",
      "  timestamp: 1662329944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330264\n",
      "  training_iteration: 66\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 335268\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 335268\n",
      "    num_agent_steps_trained: 335268\n",
      "    num_env_steps_sampled: 335268\n",
      "    num_env_steps_trained: 335268\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-19-13\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78592085034302\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 63\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.7105054716034394e-21\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3913921117782593\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.40708515472943e-06\n",
      "          policy_loss: 0.05842543765902519\n",
      "          total_loss: 0.058446742594242096\n",
      "          vf_explained_var: 0.343886137008667\n",
      "          vf_loss: 2.1310839656507596e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 335268\n",
      "    num_agent_steps_trained: 335268\n",
      "    num_env_steps_sampled: 335268\n",
      "    num_env_steps_trained: 335268\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 335268\n",
      "  num_agent_steps_trained: 335268\n",
      "  num_env_steps_sampled: 335268\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 335268\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.29999999999998\n",
      "    ram_util_percent: 36.33076923076924\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11708532382430256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.609976640872842\n",
      "    mean_inference_ms: 10.873431618866745\n",
      "    mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78592085034302\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11708532382430256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.609976640872842\n",
      "      mean_inference_ms: 10.873431618866745\n",
      "      mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  time_since_restore: 624.7627749443054\n",
      "  time_this_iter_s: 9.20998501777649\n",
      "  time_total_s: 624.7627749443054\n",
      "  timers:\n",
      "    learn_throughput: 13622.399\n",
      "    learn_time_ms: 367.336\n",
      "    synch_weights_time_ms: 10.161\n",
      "    training_iteration_time_ms: 9259.303\n",
      "  timestamp: 1662329953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335268\n",
      "  training_iteration: 67\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 340272\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 340272\n",
      "    num_agent_steps_trained: 340272\n",
      "    num_env_steps_sampled: 340272\n",
      "    num_env_steps_trained: 340272\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-19-22\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78592085034302\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 63\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.3552527358017197e-21\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3894902467727661\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0104602097271709e-06\n",
      "          policy_loss: 0.07835720479488373\n",
      "          total_loss: 0.07837794721126556\n",
      "          vf_explained_var: 0.24790067970752716\n",
      "          vf_loss: 2.0740122636198066e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 340272\n",
      "    num_agent_steps_trained: 340272\n",
      "    num_env_steps_sampled: 340272\n",
      "    num_env_steps_trained: 340272\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 340272\n",
      "  num_agent_steps_trained: 340272\n",
      "  num_env_steps_sampled: 340272\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 340272\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.8923076923077\n",
      "    ram_util_percent: 36.346153846153854\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11708532382430256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.609976640872842\n",
      "    mean_inference_ms: 10.873431618866745\n",
      "    mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78592085034302\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11708532382430256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.609976640872842\n",
      "      mean_inference_ms: 10.873431618866745\n",
      "      mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  time_since_restore: 634.0832328796387\n",
      "  time_this_iter_s: 9.320457935333252\n",
      "  time_total_s: 634.0832328796387\n",
      "  timers:\n",
      "    learn_throughput: 13645.267\n",
      "    learn_time_ms: 366.721\n",
      "    synch_weights_time_ms: 10.09\n",
      "    training_iteration_time_ms: 9259.474\n",
      "  timestamp: 1662329962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340272\n",
      "  training_iteration: 68\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 345276\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 345276\n",
      "    num_agent_steps_trained: 345276\n",
      "    num_env_steps_sampled: 345276\n",
      "    num_env_steps_trained: 345276\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-19-32\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78592085034302\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 63\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.776263679008599e-22\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3880770206451416\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.013682885466551e-07\n",
      "          policy_loss: 0.011689970269799232\n",
      "          total_loss: 0.011710105463862419\n",
      "          vf_explained_var: 0.3360794484615326\n",
      "          vf_loss: 2.012383811234031e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 345276\n",
      "    num_agent_steps_trained: 345276\n",
      "    num_env_steps_sampled: 345276\n",
      "    num_env_steps_trained: 345276\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 345276\n",
      "  num_agent_steps_trained: 345276\n",
      "  num_env_steps_sampled: 345276\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 345276\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 71.11538461538463\n",
      "    ram_util_percent: 36.22307692307693\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11708532382430256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.609976640872842\n",
      "    mean_inference_ms: 10.873431618866745\n",
      "    mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78592085034302\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11708532382430256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.609976640872842\n",
      "      mean_inference_ms: 10.873431618866745\n",
      "      mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  time_since_restore: 643.3860867023468\n",
      "  time_this_iter_s: 9.30285382270813\n",
      "  time_total_s: 643.3860867023468\n",
      "  timers:\n",
      "    learn_throughput: 13711.86\n",
      "    learn_time_ms: 364.94\n",
      "    synch_weights_time_ms: 10.092\n",
      "    training_iteration_time_ms: 9252.597\n",
      "  timestamp: 1662329972\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345276\n",
      "  training_iteration: 69\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 350280\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 350280\n",
      "    num_agent_steps_trained: 350280\n",
      "    num_env_steps_sampled: 350280\n",
      "    num_env_steps_trained: 350280\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-19-41\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78592085034302\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 63\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3881318395042993e-22\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3874099254608154\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0357014446071844e-07\n",
      "          policy_loss: 0.02239290438592434\n",
      "          total_loss: 0.02241302654147148\n",
      "          vf_explained_var: 0.3032991290092468\n",
      "          vf_loss: 2.011935066548176e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 350280\n",
      "    num_agent_steps_trained: 350280\n",
      "    num_env_steps_sampled: 350280\n",
      "    num_env_steps_trained: 350280\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 350280\n",
      "  num_agent_steps_trained: 350280\n",
      "  num_env_steps_sampled: 350280\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 350280\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.89999999999999\n",
      "    ram_util_percent: 36.346153846153854\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11708532382430256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.609976640872842\n",
      "    mean_inference_ms: 10.873431618866745\n",
      "    mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78592085034302\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11708532382430256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.609976640872842\n",
      "      mean_inference_ms: 10.873431618866745\n",
      "      mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  time_since_restore: 652.5034029483795\n",
      "  time_this_iter_s: 9.117316246032715\n",
      "  time_total_s: 652.5034029483795\n",
      "  timers:\n",
      "    learn_throughput: 13773.537\n",
      "    learn_time_ms: 363.305\n",
      "    synch_weights_time_ms: 10.023\n",
      "    training_iteration_time_ms: 9231.995\n",
      "  timestamp: 1662329981\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350280\n",
      "  training_iteration: 70\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 355284\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 355284\n",
      "    num_agent_steps_trained: 355284\n",
      "    num_env_steps_sampled: 355284\n",
      "    num_env_steps_trained: 355284\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-19-50\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78592085034302\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 63\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6940659197521496e-22\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.386122465133667\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.7245154140255181e-06\n",
      "          policy_loss: 0.00015100519522093236\n",
      "          total_loss: 0.00017067938460968435\n",
      "          vf_explained_var: 0.1503382921218872\n",
      "          vf_loss: 1.9671040718094446e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 355284\n",
      "    num_agent_steps_trained: 355284\n",
      "    num_env_steps_sampled: 355284\n",
      "    num_env_steps_trained: 355284\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 355284\n",
      "  num_agent_steps_trained: 355284\n",
      "  num_env_steps_sampled: 355284\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 355284\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 81.33333333333333\n",
      "    ram_util_percent: 36.308333333333344\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11708532382430256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.609976640872842\n",
      "    mean_inference_ms: 10.873431618866745\n",
      "    mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78592085034302\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11708532382430256\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.609976640872842\n",
      "      mean_inference_ms: 10.873431618866745\n",
      "      mean_raw_obs_processing_ms: 0.20500847382093879\n",
      "  time_since_restore: 661.5548121929169\n",
      "  time_this_iter_s: 9.051409244537354\n",
      "  time_total_s: 661.5548121929169\n",
      "  timers:\n",
      "    learn_throughput: 13731.319\n",
      "    learn_time_ms: 364.422\n",
      "    synch_weights_time_ms: 8.653\n",
      "    training_iteration_time_ms: 9217.883\n",
      "  timestamp: 1662329990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355284\n",
      "  training_iteration: 71\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 360288\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 360288\n",
      "    num_agent_steps_trained: 360288\n",
      "    num_env_steps_sampled: 360288\n",
      "    num_env_steps_trained: 360288\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-19-59\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78578059919995\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.470329598760748e-23\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3868941068649292\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.7754858794869506e-07\n",
      "          policy_loss: 0.05131441354751587\n",
      "          total_loss: 0.31904423236846924\n",
      "          vf_explained_var: 0.09713403135538101\n",
      "          vf_loss: 0.26772981882095337\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 360288\n",
      "    num_agent_steps_trained: 360288\n",
      "    num_env_steps_sampled: 360288\n",
      "    num_env_steps_trained: 360288\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 360288\n",
      "  num_agent_steps_trained: 360288\n",
      "  num_env_steps_sampled: 360288\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 360288\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.82307692307691\n",
      "    ram_util_percent: 36.36923076923078\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11703204775504829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.605480889325661\n",
      "    mean_inference_ms: 10.868050562956588\n",
      "    mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78578059919995\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11703204775504829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.605480889325661\n",
      "      mean_inference_ms: 10.868050562956588\n",
      "      mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  time_since_restore: 670.7042379379272\n",
      "  time_this_iter_s: 9.149425745010376\n",
      "  time_total_s: 670.7042379379272\n",
      "  timers:\n",
      "    learn_throughput: 13688.955\n",
      "    learn_time_ms: 365.55\n",
      "    synch_weights_time_ms: 8.67\n",
      "    training_iteration_time_ms: 9213.553\n",
      "  timestamp: 1662329999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360288\n",
      "  training_iteration: 72\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=4010.10.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=4009.90.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 365292\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 365292\n",
      "    num_agent_steps_trained: 365292\n",
      "    num_env_steps_sampled: 365292\n",
      "    num_env_steps_trained: 365292\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-20-08\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78578059919995\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.235164799380374e-23\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3979681730270386\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.803788220262504e-06\n",
      "          policy_loss: -0.09125436842441559\n",
      "          total_loss: 5.590971946716309\n",
      "          vf_explained_var: -0.0016925155650824308\n",
      "          vf_loss: 5.682226181030273\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 365292\n",
      "    num_agent_steps_trained: 365292\n",
      "    num_env_steps_sampled: 365292\n",
      "    num_env_steps_trained: 365292\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 365292\n",
      "  num_agent_steps_trained: 365292\n",
      "  num_env_steps_sampled: 365292\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 365292\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.575\n",
      "    ram_util_percent: 36.275000000000006\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11703204775504829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.605480889325661\n",
      "    mean_inference_ms: 10.868050562956588\n",
      "    mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78578059919995\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11703204775504829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.605480889325661\n",
      "      mean_inference_ms: 10.868050562956588\n",
      "      mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  time_since_restore: 679.8383028507233\n",
      "  time_this_iter_s: 9.13406491279602\n",
      "  time_total_s: 679.8383028507233\n",
      "  timers:\n",
      "    learn_throughput: 13741.846\n",
      "    learn_time_ms: 364.143\n",
      "    synch_weights_time_ms: 8.621\n",
      "    training_iteration_time_ms: 9177.841\n",
      "  timestamp: 1662330008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365292\n",
      "  training_iteration: 73\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 370296\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 370296\n",
      "    num_agent_steps_trained: 370296\n",
      "    num_env_steps_sampled: 370296\n",
      "    num_env_steps_trained: 370296\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-20-17\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78578059919995\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.117582399690187e-23\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3812508583068848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.257140855974285e-07\n",
      "          policy_loss: 0.06112076714634895\n",
      "          total_loss: 0.061287760734558105\n",
      "          vf_explained_var: 0.2699105441570282\n",
      "          vf_loss: 0.00016698858235031366\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 370296\n",
      "    num_agent_steps_trained: 370296\n",
      "    num_env_steps_sampled: 370296\n",
      "    num_env_steps_trained: 370296\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 370296\n",
      "  num_agent_steps_trained: 370296\n",
      "  num_env_steps_sampled: 370296\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 370296\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.23076923076925\n",
      "    ram_util_percent: 36.407692307692315\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11703204775504829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.605480889325661\n",
      "    mean_inference_ms: 10.868050562956588\n",
      "    mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78578059919995\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11703204775504829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.605480889325661\n",
      "      mean_inference_ms: 10.868050562956588\n",
      "      mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  time_since_restore: 689.0628678798676\n",
      "  time_this_iter_s: 9.224565029144287\n",
      "  time_total_s: 689.0628678798676\n",
      "  timers:\n",
      "    learn_throughput: 13727.276\n",
      "    learn_time_ms: 364.53\n",
      "    synch_weights_time_ms: 8.714\n",
      "    training_iteration_time_ms: 9193.853\n",
      "  timestamp: 1662330017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370296\n",
      "  training_iteration: 74\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 375300\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 375300\n",
      "    num_agent_steps_trained: 375300\n",
      "    num_env_steps_sampled: 375300\n",
      "    num_env_steps_trained: 375300\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-20-27\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78578059919995\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0587911998450935e-23\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3797433376312256\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.3133348981427844e-07\n",
      "          policy_loss: -0.026084136217832565\n",
      "          total_loss: -0.026046186685562134\n",
      "          vf_explained_var: 0.18044553697109222\n",
      "          vf_loss: 3.794454096350819e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 375300\n",
      "    num_agent_steps_trained: 375300\n",
      "    num_env_steps_sampled: 375300\n",
      "    num_env_steps_trained: 375300\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 375300\n",
      "  num_agent_steps_trained: 375300\n",
      "  num_env_steps_sampled: 375300\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 375300\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.57692307692307\n",
      "    ram_util_percent: 36.42307692307693\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11703204775504829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.605480889325661\n",
      "    mean_inference_ms: 10.868050562956588\n",
      "    mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78578059919995\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11703204775504829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.605480889325661\n",
      "      mean_inference_ms: 10.868050562956588\n",
      "      mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  time_since_restore: 698.423850774765\n",
      "  time_this_iter_s: 9.360982894897461\n",
      "  time_total_s: 698.423850774765\n",
      "  timers:\n",
      "    learn_throughput: 13692.685\n",
      "    learn_time_ms: 365.451\n",
      "    synch_weights_time_ms: 7.496\n",
      "    training_iteration_time_ms: 9210.193\n",
      "  timestamp: 1662330027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375300\n",
      "  training_iteration: 75\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 380304\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 380304\n",
      "    num_agent_steps_trained: 380304\n",
      "    num_env_steps_sampled: 380304\n",
      "    num_env_steps_trained: 380304\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-20-36\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78578059919995\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.293955999225468e-24\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3796353340148926\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2020795736589207e-07\n",
      "          policy_loss: -0.08424919843673706\n",
      "          total_loss: -0.08421751856803894\n",
      "          vf_explained_var: 0.19413550198078156\n",
      "          vf_loss: 3.1671013857703656e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 380304\n",
      "    num_agent_steps_trained: 380304\n",
      "    num_env_steps_sampled: 380304\n",
      "    num_env_steps_trained: 380304\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 380304\n",
      "  num_agent_steps_trained: 380304\n",
      "  num_env_steps_sampled: 380304\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 380304\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.975\n",
      "    ram_util_percent: 36.341666666666676\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11703204775504829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.605480889325661\n",
      "    mean_inference_ms: 10.868050562956588\n",
      "    mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78578059919995\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11703204775504829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.605480889325661\n",
      "      mean_inference_ms: 10.868050562956588\n",
      "      mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  time_since_restore: 707.5591478347778\n",
      "  time_this_iter_s: 9.135297060012817\n",
      "  time_total_s: 707.5591478347778\n",
      "  timers:\n",
      "    learn_throughput: 13701.05\n",
      "    learn_time_ms: 365.227\n",
      "    synch_weights_time_ms: 7.513\n",
      "    training_iteration_time_ms: 9192.988\n",
      "  timestamp: 1662330036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380304\n",
      "  training_iteration: 76\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 385308\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 385308\n",
      "    num_agent_steps_trained: 385308\n",
      "    num_env_steps_sampled: 385308\n",
      "    num_env_steps_trained: 385308\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-20-45\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78578059919995\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.646977999612734e-24\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3817403316497803\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.451450654916698e-06\n",
      "          policy_loss: -0.06609608232975006\n",
      "          total_loss: -0.0660649836063385\n",
      "          vf_explained_var: 0.25191959738731384\n",
      "          vf_loss: 3.108671080553904e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 385308\n",
      "    num_agent_steps_trained: 385308\n",
      "    num_env_steps_sampled: 385308\n",
      "    num_env_steps_trained: 385308\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 385308\n",
      "  num_agent_steps_trained: 385308\n",
      "  num_env_steps_sampled: 385308\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 385308\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.37692307692308\n",
      "    ram_util_percent: 36.30000000000001\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11703204775504829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.605480889325661\n",
      "    mean_inference_ms: 10.868050562956588\n",
      "    mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78578059919995\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11703204775504829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.605480889325661\n",
      "      mean_inference_ms: 10.868050562956588\n",
      "      mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  time_since_restore: 716.6728887557983\n",
      "  time_this_iter_s: 9.113740921020508\n",
      "  time_total_s: 716.6728887557983\n",
      "  timers:\n",
      "    learn_throughput: 13712.806\n",
      "    learn_time_ms: 364.914\n",
      "    synch_weights_time_ms: 7.503\n",
      "    training_iteration_time_ms: 9183.321\n",
      "  timestamp: 1662330045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385308\n",
      "  training_iteration: 77\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 390312\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 390312\n",
      "    num_agent_steps_trained: 390312\n",
      "    num_env_steps_sampled: 390312\n",
      "    num_env_steps_trained: 390312\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-20-55\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78578059919995\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.323488999806367e-24\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3833328485488892\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.891706512604287e-07\n",
      "          policy_loss: 0.069893017411232\n",
      "          total_loss: 0.06992477178573608\n",
      "          vf_explained_var: 0.2481335699558258\n",
      "          vf_loss: 3.176811515004374e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 390312\n",
      "    num_agent_steps_trained: 390312\n",
      "    num_env_steps_sampled: 390312\n",
      "    num_env_steps_trained: 390312\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 390312\n",
      "  num_agent_steps_trained: 390312\n",
      "  num_env_steps_sampled: 390312\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 390312\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.73076923076925\n",
      "    ram_util_percent: 36.27692307692309\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11703204775504829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.605480889325661\n",
      "    mean_inference_ms: 10.868050562956588\n",
      "    mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78578059919995\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11703204775504829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.605480889325661\n",
      "      mean_inference_ms: 10.868050562956588\n",
      "      mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  time_since_restore: 726.1656656265259\n",
      "  time_this_iter_s: 9.492776870727539\n",
      "  time_total_s: 726.1656656265259\n",
      "  timers:\n",
      "    learn_throughput: 13593.598\n",
      "    learn_time_ms: 368.114\n",
      "    synch_weights_time_ms: 7.606\n",
      "    training_iteration_time_ms: 9200.351\n",
      "  timestamp: 1662330055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390312\n",
      "  training_iteration: 78\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 395316\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 395316\n",
      "    num_agent_steps_trained: 395316\n",
      "    num_env_steps_sampled: 395316\n",
      "    num_env_steps_trained: 395316\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-21-05\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78578059919995\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.617444999031835e-25\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3829594850540161\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.388272832831717e-07\n",
      "          policy_loss: -0.10975430905818939\n",
      "          total_loss: -0.10972460359334946\n",
      "          vf_explained_var: 0.09060430526733398\n",
      "          vf_loss: 2.9711663955822587e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 395316\n",
      "    num_agent_steps_trained: 395316\n",
      "    num_env_steps_sampled: 395316\n",
      "    num_env_steps_trained: 395316\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 395316\n",
      "  num_agent_steps_trained: 395316\n",
      "  num_env_steps_sampled: 395316\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 395316\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.54285714285716\n",
      "    ram_util_percent: 36.771428571428565\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11703204775504829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.605480889325661\n",
      "    mean_inference_ms: 10.868050562956588\n",
      "    mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78578059919995\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11703204775504829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.605480889325661\n",
      "      mean_inference_ms: 10.868050562956588\n",
      "      mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  time_since_restore: 736.0930504798889\n",
      "  time_this_iter_s: 9.927384853363037\n",
      "  time_total_s: 736.0930504798889\n",
      "  timers:\n",
      "    learn_throughput: 13544.346\n",
      "    learn_time_ms: 369.453\n",
      "    synch_weights_time_ms: 7.623\n",
      "    training_iteration_time_ms: 9262.832\n",
      "  timestamp: 1662330065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395316\n",
      "  training_iteration: 79\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 400320\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 400320\n",
      "    num_agent_steps_trained: 400320\n",
      "    num_env_steps_sampled: 400320\n",
      "    num_env_steps_trained: 400320\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-21-14\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78578059919995\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 72\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.3087224995159173e-25\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3870863914489746\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.873758992587682e-05\n",
      "          policy_loss: -0.13966822624206543\n",
      "          total_loss: -0.1396387219429016\n",
      "          vf_explained_var: 0.04372743144631386\n",
      "          vf_loss: 2.9488102882169187e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 400320\n",
      "    num_agent_steps_trained: 400320\n",
      "    num_env_steps_sampled: 400320\n",
      "    num_env_steps_trained: 400320\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 400320\n",
      "  num_agent_steps_trained: 400320\n",
      "  num_env_steps_sampled: 400320\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 400320\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.56153846153845\n",
      "    ram_util_percent: 36.46923076923077\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11703204775504829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.605480889325661\n",
      "    mean_inference_ms: 10.868050562956588\n",
      "    mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78578059919995\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11703204775504829\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.605480889325661\n",
      "      mean_inference_ms: 10.868050562956588\n",
      "      mean_raw_obs_processing_ms: 0.20535542487559094\n",
      "  time_since_restore: 745.6704354286194\n",
      "  time_this_iter_s: 9.577384948730469\n",
      "  time_total_s: 745.6704354286194\n",
      "  timers:\n",
      "    learn_throughput: 13625.343\n",
      "    learn_time_ms: 367.257\n",
      "    synch_weights_time_ms: 7.592\n",
      "    training_iteration_time_ms: 9308.886\n",
      "  timestamp: 1662330074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400320\n",
      "  training_iteration: 80\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 405324\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 405324\n",
      "    num_agent_steps_trained: 405324\n",
      "    num_env_steps_sampled: 405324\n",
      "    num_env_steps_trained: 405324\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-21-23\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78726960327619\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 81\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6543612497579586e-25\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3904621601104736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.072218745131977e-07\n",
      "          policy_loss: 0.05616535618901253\n",
      "          total_loss: 0.42776432633399963\n",
      "          vf_explained_var: 0.09510372579097748\n",
      "          vf_loss: 0.3715989589691162\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 405324\n",
      "    num_agent_steps_trained: 405324\n",
      "    num_env_steps_sampled: 405324\n",
      "    num_env_steps_trained: 405324\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 405324\n",
      "  num_agent_steps_trained: 405324\n",
      "  num_env_steps_sampled: 405324\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 405324\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.25384615384615\n",
      "    ram_util_percent: 36.784615384615385\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11700833885218118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.600996923956141\n",
      "    mean_inference_ms: 10.864782666050985\n",
      "    mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78726960327619\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11700833885218118\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.600996923956141\n",
      "      mean_inference_ms: 10.864782666050985\n",
      "      mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  time_since_restore: 754.7710301876068\n",
      "  time_this_iter_s: 9.100594758987427\n",
      "  time_total_s: 754.7710301876068\n",
      "  timers:\n",
      "    learn_throughput: 13639.825\n",
      "    learn_time_ms: 366.867\n",
      "    synch_weights_time_ms: 7.606\n",
      "    training_iteration_time_ms: 9313.858\n",
      "  timestamp: 1662330083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405324\n",
      "  training_iteration: 81\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=4510.00.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=4510.20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 410328\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 410328\n",
      "    num_agent_steps_trained: 410328\n",
      "    num_env_steps_sampled: 410328\n",
      "    num_env_steps_trained: 410328\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-21-32\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78726960327619\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.271806248789793e-26\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3998770713806152\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.865011876769131e-06\n",
      "          policy_loss: 0.11150070279836655\n",
      "          total_loss: 4.671496391296387\n",
      "          vf_explained_var: -0.0002842843532562256\n",
      "          vf_loss: 4.559995651245117\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 410328\n",
      "    num_agent_steps_trained: 410328\n",
      "    num_env_steps_sampled: 410328\n",
      "    num_env_steps_trained: 410328\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 410328\n",
      "  num_agent_steps_trained: 410328\n",
      "  num_env_steps_sampled: 410328\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 410328\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.86666666666666\n",
      "    ram_util_percent: 37.14166666666667\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11700833885218118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.600996923956141\n",
      "    mean_inference_ms: 10.864782666050985\n",
      "    mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78726960327619\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11700833885218118\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.600996923956141\n",
      "      mean_inference_ms: 10.864782666050985\n",
      "      mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  time_since_restore: 763.7965660095215\n",
      "  time_this_iter_s: 9.025535821914673\n",
      "  time_total_s: 763.7965660095215\n",
      "  timers:\n",
      "    learn_throughput: 13734.011\n",
      "    learn_time_ms: 364.351\n",
      "    synch_weights_time_ms: 7.623\n",
      "    training_iteration_time_ms: 9301.405\n",
      "  timestamp: 1662330092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410328\n",
      "  training_iteration: 82\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 415332\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 415332\n",
      "    num_agent_steps_trained: 415332\n",
      "    num_env_steps_sampled: 415332\n",
      "    num_env_steps_trained: 415332\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-21-42\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78726960327619\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.1359031243948966e-26\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3827569484710693\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.1532149452905287e-07\n",
      "          policy_loss: -0.15562297403812408\n",
      "          total_loss: -0.1553640216588974\n",
      "          vf_explained_var: 0.08709022402763367\n",
      "          vf_loss: 0.00025895581347867846\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 415332\n",
      "    num_agent_steps_trained: 415332\n",
      "    num_env_steps_sampled: 415332\n",
      "    num_env_steps_trained: 415332\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 415332\n",
      "  num_agent_steps_trained: 415332\n",
      "  num_env_steps_sampled: 415332\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 415332\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.55\n",
      "    ram_util_percent: 36.81428571428571\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11700833885218118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.600996923956141\n",
      "    mean_inference_ms: 10.864782666050985\n",
      "    mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78726960327619\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11700833885218118\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.600996923956141\n",
      "      mean_inference_ms: 10.864782666050985\n",
      "      mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  time_since_restore: 773.4439771175385\n",
      "  time_this_iter_s: 9.647411108016968\n",
      "  time_total_s: 773.4439771175385\n",
      "  timers:\n",
      "    learn_throughput: 13703.205\n",
      "    learn_time_ms: 365.17\n",
      "    synch_weights_time_ms: 9.011\n",
      "    training_iteration_time_ms: 9352.83\n",
      "  timestamp: 1662330102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415332\n",
      "  training_iteration: 83\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 420336\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 420336\n",
      "    num_agent_steps_trained: 420336\n",
      "    num_env_steps_sampled: 420336\n",
      "    num_env_steps_trained: 420336\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-21-51\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78726960327619\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0679515621974483e-26\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3831475973129272\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.941001290921122e-07\n",
      "          policy_loss: 0.12802544236183167\n",
      "          total_loss: 0.1280829906463623\n",
      "          vf_explained_var: 0.22767695784568787\n",
      "          vf_loss: 5.756630707764998e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 420336\n",
      "    num_agent_steps_trained: 420336\n",
      "    num_env_steps_sampled: 420336\n",
      "    num_env_steps_trained: 420336\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 420336\n",
      "  num_agent_steps_trained: 420336\n",
      "  num_env_steps_sampled: 420336\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 420336\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.61538461538461\n",
      "    ram_util_percent: 36.85384615384615\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11700833885218118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.600996923956141\n",
      "    mean_inference_ms: 10.864782666050985\n",
      "    mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78726960327619\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11700833885218118\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.600996923956141\n",
      "      mean_inference_ms: 10.864782666050985\n",
      "      mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  time_since_restore: 782.8761439323425\n",
      "  time_this_iter_s: 9.432166814804077\n",
      "  time_total_s: 782.8761439323425\n",
      "  timers:\n",
      "    learn_throughput: 13708.105\n",
      "    learn_time_ms: 365.04\n",
      "    synch_weights_time_ms: 9.022\n",
      "    training_iteration_time_ms: 9373.42\n",
      "  timestamp: 1662330111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420336\n",
      "  training_iteration: 84\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 425340\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 425340\n",
      "    num_agent_steps_trained: 425340\n",
      "    num_env_steps_sampled: 425340\n",
      "    num_env_steps_trained: 425340\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-22-01\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78726960327619\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0339757810987241e-26\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.380439043045044\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.069855094712693e-06\n",
      "          policy_loss: 0.05148981139063835\n",
      "          total_loss: 0.05153648927807808\n",
      "          vf_explained_var: 0.19853651523590088\n",
      "          vf_loss: 4.6681510866619647e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 425340\n",
      "    num_agent_steps_trained: 425340\n",
      "    num_env_steps_sampled: 425340\n",
      "    num_env_steps_trained: 425340\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 425340\n",
      "  num_agent_steps_trained: 425340\n",
      "  num_env_steps_sampled: 425340\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 425340\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.33846153846153\n",
      "    ram_util_percent: 36.89999999999999\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11700833885218118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.600996923956141\n",
      "    mean_inference_ms: 10.864782666050985\n",
      "    mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78726960327619\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11700833885218118\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.600996923956141\n",
      "      mean_inference_ms: 10.864782666050985\n",
      "      mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  time_since_restore: 792.1479291915894\n",
      "  time_this_iter_s: 9.271785259246826\n",
      "  time_total_s: 792.1479291915894\n",
      "  timers:\n",
      "    learn_throughput: 13745.523\n",
      "    learn_time_ms: 364.046\n",
      "    synch_weights_time_ms: 9.054\n",
      "    training_iteration_time_ms: 9364.524\n",
      "  timestamp: 1662330121\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425340\n",
      "  training_iteration: 85\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 430344\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 430344\n",
      "    num_agent_steps_trained: 430344\n",
      "    num_env_steps_sampled: 430344\n",
      "    num_env_steps_trained: 430344\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78726960327619\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.169878905493621e-27\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3792829513549805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.2234305769616185e-08\n",
      "          policy_loss: -0.016045259311795235\n",
      "          total_loss: -0.016000453382730484\n",
      "          vf_explained_var: 0.15737871825695038\n",
      "          vf_loss: 4.479498602449894e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 430344\n",
      "    num_agent_steps_trained: 430344\n",
      "    num_env_steps_sampled: 430344\n",
      "    num_env_steps_trained: 430344\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 430344\n",
      "  num_agent_steps_trained: 430344\n",
      "  num_env_steps_sampled: 430344\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 430344\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.10833333333333\n",
      "    ram_util_percent: 36.974999999999994\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11700833885218118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.600996923956141\n",
      "    mean_inference_ms: 10.864782666050985\n",
      "    mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78726960327619\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11700833885218118\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.600996923956141\n",
      "      mean_inference_ms: 10.864782666050985\n",
      "      mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  time_since_restore: 801.1801500320435\n",
      "  time_this_iter_s: 9.032220840454102\n",
      "  time_total_s: 801.1801500320435\n",
      "  timers:\n",
      "    learn_throughput: 13767.357\n",
      "    learn_time_ms: 363.468\n",
      "    synch_weights_time_ms: 9.051\n",
      "    training_iteration_time_ms: 9354.219\n",
      "  timestamp: 1662330130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430344\n",
      "  training_iteration: 86\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 435348\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 435348\n",
      "    num_agent_steps_trained: 435348\n",
      "    num_env_steps_sampled: 435348\n",
      "    num_env_steps_trained: 435348\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-22-19\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78726960327619\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.5849394527468104e-27\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3790581226348877\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.5104453154890507e-07\n",
      "          policy_loss: 0.0477980300784111\n",
      "          total_loss: 0.04784294590353966\n",
      "          vf_explained_var: 0.29071933031082153\n",
      "          vf_loss: 4.4913638703292236e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 435348\n",
      "    num_agent_steps_trained: 435348\n",
      "    num_env_steps_sampled: 435348\n",
      "    num_env_steps_trained: 435348\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 435348\n",
      "  num_agent_steps_trained: 435348\n",
      "  num_env_steps_sampled: 435348\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 435348\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.06923076923078\n",
      "    ram_util_percent: 37.06153846153846\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11700833885218118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.600996923956141\n",
      "    mean_inference_ms: 10.864782666050985\n",
      "    mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78726960327619\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11700833885218118\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.600996923956141\n",
      "      mean_inference_ms: 10.864782666050985\n",
      "      mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  time_since_restore: 810.157576084137\n",
      "  time_this_iter_s: 8.977426052093506\n",
      "  time_total_s: 810.157576084137\n",
      "  timers:\n",
      "    learn_throughput: 13755.73\n",
      "    learn_time_ms: 363.776\n",
      "    synch_weights_time_ms: 9.109\n",
      "    training_iteration_time_ms: 9340.581\n",
      "  timestamp: 1662330139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435348\n",
      "  training_iteration: 87\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 440352\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 440352\n",
      "    num_agent_steps_trained: 440352\n",
      "    num_env_steps_sampled: 440352\n",
      "    num_env_steps_trained: 440352\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-22-28\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78726960327619\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2924697263734052e-27\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3775547742843628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4409401956072543e-06\n",
      "          policy_loss: -0.032622452825307846\n",
      "          total_loss: -0.03257874771952629\n",
      "          vf_explained_var: 0.2009563446044922\n",
      "          vf_loss: 4.369513044366613e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 440352\n",
      "    num_agent_steps_trained: 440352\n",
      "    num_env_steps_sampled: 440352\n",
      "    num_env_steps_trained: 440352\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 440352\n",
      "  num_agent_steps_trained: 440352\n",
      "  num_env_steps_sampled: 440352\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 440352\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.54166666666667\n",
      "    ram_util_percent: 37.03333333333333\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11700833885218118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.600996923956141\n",
      "    mean_inference_ms: 10.864782666050985\n",
      "    mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78726960327619\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11700833885218118\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.600996923956141\n",
      "      mean_inference_ms: 10.864782666050985\n",
      "      mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  time_since_restore: 819.2250461578369\n",
      "  time_this_iter_s: 9.067470073699951\n",
      "  time_total_s: 819.2250461578369\n",
      "  timers:\n",
      "    learn_throughput: 13861.726\n",
      "    learn_time_ms: 360.994\n",
      "    synch_weights_time_ms: 9.078\n",
      "    training_iteration_time_ms: 9298.271\n",
      "  timestamp: 1662330148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440352\n",
      "  training_iteration: 88\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 445356\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 445356\n",
      "    num_agent_steps_trained: 445356\n",
      "    num_env_steps_sampled: 445356\n",
      "    num_env_steps_trained: 445356\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-22-37\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7974450028322\n",
      "  episode_reward_mean: 102.78726960327619\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 81\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.462348631867026e-28\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3772048950195312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.1193209249759093e-06\n",
      "          policy_loss: 0.06363455206155777\n",
      "          total_loss: 0.0636788159608841\n",
      "          vf_explained_var: 0.2649971544742584\n",
      "          vf_loss: 4.4256652472540736e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 445356\n",
      "    num_agent_steps_trained: 445356\n",
      "    num_env_steps_sampled: 445356\n",
      "    num_env_steps_trained: 445356\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 445356\n",
      "  num_agent_steps_trained: 445356\n",
      "  num_env_steps_sampled: 445356\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 445356\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.45384615384614\n",
      "    ram_util_percent: 36.93846153846154\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11700833885218118\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.600996923956141\n",
      "    mean_inference_ms: 10.864782666050985\n",
      "    mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7974450028322\n",
      "    episode_reward_mean: 102.78726960327619\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11700833885218118\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.600996923956141\n",
      "      mean_inference_ms: 10.864782666050985\n",
      "      mean_raw_obs_processing_ms: 0.205701120206328\n",
      "  time_since_restore: 828.3494982719421\n",
      "  time_this_iter_s: 9.124452114105225\n",
      "  time_total_s: 828.3494982719421\n",
      "  timers:\n",
      "    learn_throughput: 13924.794\n",
      "    learn_time_ms: 359.359\n",
      "    synch_weights_time_ms: 9.081\n",
      "    training_iteration_time_ms: 9217.934\n",
      "  timestamp: 1662330157\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445356\n",
      "  training_iteration: 89\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 450360\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 450360\n",
      "    num_agent_steps_trained: 450360\n",
      "    num_env_steps_sampled: 450360\n",
      "    num_env_steps_trained: 450360\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-22-46\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78710450832294\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 90\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.231174315933513e-28\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3801296949386597\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 6.248546924325638e-07\n",
      "          policy_loss: -0.04394286498427391\n",
      "          total_loss: 0.6601247787475586\n",
      "          vf_explained_var: -0.012735185213387012\n",
      "          vf_loss: 0.704067587852478\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 450360\n",
      "    num_agent_steps_trained: 450360\n",
      "    num_env_steps_sampled: 450360\n",
      "    num_env_steps_trained: 450360\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 450360\n",
      "  num_agent_steps_trained: 450360\n",
      "  num_env_steps_sampled: 450360\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 450360\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.99166666666667\n",
      "    ram_util_percent: 36.916666666666664\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11698568147995243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596854074632382\n",
      "    mean_inference_ms: 10.860534474268622\n",
      "    mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78710450832294\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11698568147995243\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.596854074632382\n",
      "      mean_inference_ms: 10.860534474268622\n",
      "      mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  time_since_restore: 837.359959602356\n",
      "  time_this_iter_s: 9.010461330413818\n",
      "  time_total_s: 837.359959602356\n",
      "  timers:\n",
      "    learn_throughput: 13883.374\n",
      "    learn_time_ms: 360.431\n",
      "    synch_weights_time_ms: 9.114\n",
      "    training_iteration_time_ms: 9161.202\n",
      "  timestamp: 1662330166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450360\n",
      "  training_iteration: 90\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=5010.10.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=5010.30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 455364\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 455364\n",
      "    num_agent_steps_trained: 455364\n",
      "    num_env_steps_sampled: 455364\n",
      "    num_env_steps_trained: 455364\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-22-55\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78710450832294\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 90\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6155871579667565e-28\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3957000970840454\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.954198862425983e-06\n",
      "          policy_loss: -0.09041589498519897\n",
      "          total_loss: 5.5765533447265625\n",
      "          vf_explained_var: -0.0014264106284826994\n",
      "          vf_loss: 5.6669697761535645\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 455364\n",
      "    num_agent_steps_trained: 455364\n",
      "    num_env_steps_sampled: 455364\n",
      "    num_env_steps_trained: 455364\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 455364\n",
      "  num_agent_steps_trained: 455364\n",
      "  num_env_steps_sampled: 455364\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 455364\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.98333333333332\n",
      "    ram_util_percent: 37.09166666666666\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11698568147995243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596854074632382\n",
      "    mean_inference_ms: 10.860534474268622\n",
      "    mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78710450832294\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11698568147995243\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.596854074632382\n",
      "      mean_inference_ms: 10.860534474268622\n",
      "      mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  time_since_restore: 846.1399354934692\n",
      "  time_this_iter_s: 8.779975891113281\n",
      "  time_total_s: 846.1399354934692\n",
      "  timers:\n",
      "    learn_throughput: 13997.037\n",
      "    learn_time_ms: 357.504\n",
      "    synch_weights_time_ms: 9.07\n",
      "    training_iteration_time_ms: 9129.095\n",
      "  timestamp: 1662330175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455364\n",
      "  training_iteration: 91\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 460368\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 460368\n",
      "    num_agent_steps_trained: 460368\n",
      "    num_env_steps_sampled: 460368\n",
      "    num_env_steps_trained: 460368\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-23-04\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78710450832294\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 90\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 8.077935789833782e-29\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.372504472732544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3008854011786752e-06\n",
      "          policy_loss: 0.017848383635282516\n",
      "          total_loss: 0.018097756430506706\n",
      "          vf_explained_var: 0.3434789776802063\n",
      "          vf_loss: 0.0002493679930921644\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 460368\n",
      "    num_agent_steps_trained: 460368\n",
      "    num_env_steps_sampled: 460368\n",
      "    num_env_steps_trained: 460368\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 460368\n",
      "  num_agent_steps_trained: 460368\n",
      "  num_env_steps_sampled: 460368\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 460368\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 73.47692307692307\n",
      "    ram_util_percent: 37.04615384615386\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11698568147995243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596854074632382\n",
      "    mean_inference_ms: 10.860534474268622\n",
      "    mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78710450832294\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11698568147995243\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.596854074632382\n",
      "      mean_inference_ms: 10.860534474268622\n",
      "      mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  time_since_restore: 855.3196804523468\n",
      "  time_this_iter_s: 9.179744958877563\n",
      "  time_total_s: 855.3196804523468\n",
      "  timers:\n",
      "    learn_throughput: 13948.286\n",
      "    learn_time_ms: 358.754\n",
      "    synch_weights_time_ms: 9.063\n",
      "    training_iteration_time_ms: 9144.628\n",
      "  timestamp: 1662330184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460368\n",
      "  training_iteration: 92\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 465372\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 465372\n",
      "    num_agent_steps_trained: 465372\n",
      "    num_env_steps_sampled: 465372\n",
      "    num_env_steps_trained: 465372\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-23-13\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78710450832294\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 90\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.038967894916891e-29\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3709620237350464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.971510634277365e-07\n",
      "          policy_loss: 0.02591947838664055\n",
      "          total_loss: 0.025989120826125145\n",
      "          vf_explained_var: 0.1958158314228058\n",
      "          vf_loss: 6.964623753447086e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 465372\n",
      "    num_agent_steps_trained: 465372\n",
      "    num_env_steps_sampled: 465372\n",
      "    num_env_steps_trained: 465372\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 465372\n",
      "  num_agent_steps_trained: 465372\n",
      "  num_env_steps_sampled: 465372\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 465372\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.83846153846153\n",
      "    ram_util_percent: 36.96153846153846\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11698568147995243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596854074632382\n",
      "    mean_inference_ms: 10.860534474268622\n",
      "    mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78710450832294\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11698568147995243\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.596854074632382\n",
      "      mean_inference_ms: 10.860534474268622\n",
      "      mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  time_since_restore: 864.2588822841644\n",
      "  time_this_iter_s: 8.939201831817627\n",
      "  time_total_s: 864.2588822841644\n",
      "  timers:\n",
      "    learn_throughput: 13878.625\n",
      "    learn_time_ms: 360.554\n",
      "    synch_weights_time_ms: 7.735\n",
      "    training_iteration_time_ms: 9073.598\n",
      "  timestamp: 1662330193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465372\n",
      "  training_iteration: 93\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 470376\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 470376\n",
      "    num_agent_steps_trained: 470376\n",
      "    num_env_steps_sampled: 470376\n",
      "    num_env_steps_trained: 470376\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-23-22\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78710450832294\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 90\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.0194839474584456e-29\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.369969367980957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.3654369013238465e-06\n",
      "          policy_loss: -0.013390721753239632\n",
      "          total_loss: -0.013330903835594654\n",
      "          vf_explained_var: 0.16786286234855652\n",
      "          vf_loss: 5.98208062001504e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 470376\n",
      "    num_agent_steps_trained: 470376\n",
      "    num_env_steps_sampled: 470376\n",
      "    num_env_steps_trained: 470376\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 470376\n",
      "  num_agent_steps_trained: 470376\n",
      "  num_env_steps_sampled: 470376\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 470376\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 71.94166666666668\n",
      "    ram_util_percent: 37.10833333333334\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11698568147995243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596854074632382\n",
      "    mean_inference_ms: 10.860534474268622\n",
      "    mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78710450832294\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11698568147995243\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.596854074632382\n",
      "      mean_inference_ms: 10.860534474268622\n",
      "      mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  time_since_restore: 872.9398090839386\n",
      "  time_this_iter_s: 8.68092679977417\n",
      "  time_total_s: 872.9398090839386\n",
      "  timers:\n",
      "    learn_throughput: 14020.699\n",
      "    learn_time_ms: 356.901\n",
      "    synch_weights_time_ms: 7.699\n",
      "    training_iteration_time_ms: 8998.67\n",
      "  timestamp: 1662330202\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 470376\n",
      "  training_iteration: 94\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 475380\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 475380\n",
      "    num_agent_steps_trained: 475380\n",
      "    num_env_steps_sampled: 475380\n",
      "    num_env_steps_trained: 475380\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78710450832294\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 90\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0097419737292228e-29\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3702690601348877\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.584564159884394e-07\n",
      "          policy_loss: -0.01652398332953453\n",
      "          total_loss: -0.01646508276462555\n",
      "          vf_explained_var: 0.33372005820274353\n",
      "          vf_loss: 5.890794636798091e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 475380\n",
      "    num_agent_steps_trained: 475380\n",
      "    num_env_steps_sampled: 475380\n",
      "    num_env_steps_trained: 475380\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 475380\n",
      "  num_agent_steps_trained: 475380\n",
      "  num_env_steps_sampled: 475380\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 475380\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.11666666666667\n",
      "    ram_util_percent: 36.79166666666668\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11698568147995243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596854074632382\n",
      "    mean_inference_ms: 10.860534474268622\n",
      "    mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78710450832294\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11698568147995243\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.596854074632382\n",
      "      mean_inference_ms: 10.860534474268622\n",
      "      mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  time_since_restore: 881.9185123443604\n",
      "  time_this_iter_s: 8.978703260421753\n",
      "  time_total_s: 881.9185123443604\n",
      "  timers:\n",
      "    learn_throughput: 14075.355\n",
      "    learn_time_ms: 355.515\n",
      "    synch_weights_time_ms: 7.699\n",
      "    training_iteration_time_ms: 8969.432\n",
      "  timestamp: 1662330211\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 475380\n",
      "  training_iteration: 95\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 480384\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 480384\n",
      "    num_agent_steps_trained: 480384\n",
      "    num_env_steps_sampled: 480384\n",
      "    num_env_steps_trained: 480384\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-23-40\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78710450832294\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 90\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.048709868646114e-30\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.37002432346344\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.456943093420705e-06\n",
      "          policy_loss: 0.04697737842798233\n",
      "          total_loss: 0.04703628271818161\n",
      "          vf_explained_var: 0.3375099301338196\n",
      "          vf_loss: 5.892046101507731e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 480384\n",
      "    num_agent_steps_trained: 480384\n",
      "    num_env_steps_sampled: 480384\n",
      "    num_env_steps_trained: 480384\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 480384\n",
      "  num_agent_steps_trained: 480384\n",
      "  num_env_steps_sampled: 480384\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 480384\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 72.36923076923077\n",
      "    ram_util_percent: 37.06923076923076\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11698568147995243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596854074632382\n",
      "    mean_inference_ms: 10.860534474268622\n",
      "    mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78710450832294\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11698568147995243\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.596854074632382\n",
      "      mean_inference_ms: 10.860534474268622\n",
      "      mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  time_since_restore: 890.9433164596558\n",
      "  time_this_iter_s: 9.02480411529541\n",
      "  time_total_s: 890.9433164596558\n",
      "  timers:\n",
      "    learn_throughput: 14069.831\n",
      "    learn_time_ms: 355.655\n",
      "    synch_weights_time_ms: 7.728\n",
      "    training_iteration_time_ms: 8968.681\n",
      "  timestamp: 1662330220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480384\n",
      "  training_iteration: 96\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 485388\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 485388\n",
      "    num_agent_steps_trained: 485388\n",
      "    num_env_steps_sampled: 485388\n",
      "    num_env_steps_trained: 485388\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-23-49\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78710450832294\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 90\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.524354934323057e-30\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3692351579666138\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.399075927110971e-06\n",
      "          policy_loss: -0.0347633957862854\n",
      "          total_loss: -0.03470645472407341\n",
      "          vf_explained_var: 0.09957696497440338\n",
      "          vf_loss: 5.693996718036942e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 485388\n",
      "    num_agent_steps_trained: 485388\n",
      "    num_env_steps_sampled: 485388\n",
      "    num_env_steps_trained: 485388\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 485388\n",
      "  num_agent_steps_trained: 485388\n",
      "  num_env_steps_sampled: 485388\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 485388\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.13333333333334\n",
      "    ram_util_percent: 36.83333333333334\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11698568147995243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596854074632382\n",
      "    mean_inference_ms: 10.860534474268622\n",
      "    mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78710450832294\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11698568147995243\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.596854074632382\n",
      "      mean_inference_ms: 10.860534474268622\n",
      "      mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  time_since_restore: 900.0215315818787\n",
      "  time_this_iter_s: 9.0782151222229\n",
      "  time_total_s: 900.0215315818787\n",
      "  timers:\n",
      "    learn_throughput: 14107.2\n",
      "    learn_time_ms: 354.712\n",
      "    synch_weights_time_ms: 7.686\n",
      "    training_iteration_time_ms: 8978.796\n",
      "  timestamp: 1662330229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485388\n",
      "  training_iteration: 97\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 490392\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 490392\n",
      "    num_agent_steps_trained: 490392\n",
      "    num_env_steps_sampled: 490392\n",
      "    num_env_steps_trained: 490392\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-23-58\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78710450832294\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 90\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2621774671615285e-30\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3707340955734253\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.261517574377649e-07\n",
      "          policy_loss: 0.02009091153740883\n",
      "          total_loss: 0.02014758810400963\n",
      "          vf_explained_var: 0.05465231090784073\n",
      "          vf_loss: 5.667984441970475e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 490392\n",
      "    num_agent_steps_trained: 490392\n",
      "    num_env_steps_sampled: 490392\n",
      "    num_env_steps_trained: 490392\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 490392\n",
      "  num_agent_steps_trained: 490392\n",
      "  num_env_steps_sampled: 490392\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 490392\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 72.11538461538461\n",
      "    ram_util_percent: 37.130769230769225\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11698568147995243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.596854074632382\n",
      "    mean_inference_ms: 10.860534474268622\n",
      "    mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78710450832294\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11698568147995243\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.596854074632382\n",
      "      mean_inference_ms: 10.860534474268622\n",
      "      mean_raw_obs_processing_ms: 0.20610228705974187\n",
      "  time_since_restore: 909.101372718811\n",
      "  time_this_iter_s: 9.079841136932373\n",
      "  time_total_s: 909.101372718811\n",
      "  timers:\n",
      "    learn_throughput: 14127.998\n",
      "    learn_time_ms: 354.19\n",
      "    synch_weights_time_ms: 7.717\n",
      "    training_iteration_time_ms: 8980.072\n",
      "  timestamp: 1662330238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 490392\n",
      "  training_iteration: 98\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 495396\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 495396\n",
      "    num_agent_steps_trained: 495396\n",
      "    num_env_steps_sampled: 495396\n",
      "    num_env_steps_trained: 495396\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-24-07\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78692805512081\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.3108873358076425e-31\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3734405040740967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.085704287921544e-06\n",
      "          policy_loss: 0.07545636594295502\n",
      "          total_loss: 0.5539571046829224\n",
      "          vf_explained_var: -0.027751192450523376\n",
      "          vf_loss: 0.4785006642341614\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 495396\n",
      "    num_agent_steps_trained: 495396\n",
      "    num_env_steps_sampled: 495396\n",
      "    num_env_steps_trained: 495396\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 495396\n",
      "  num_agent_steps_trained: 495396\n",
      "  num_env_steps_sampled: 495396\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 495396\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.475\n",
      "    ram_util_percent: 36.82500000000001\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695994076281536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.591788442107194\n",
      "    mean_inference_ms: 10.85438011396727\n",
      "    mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78692805512081\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11695994076281536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.591788442107194\n",
      "      mean_inference_ms: 10.85438011396727\n",
      "      mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  time_since_restore: 918.051456451416\n",
      "  time_this_iter_s: 8.95008373260498\n",
      "  time_total_s: 918.051456451416\n",
      "  timers:\n",
      "    learn_throughput: 14175.726\n",
      "    learn_time_ms: 352.998\n",
      "    synch_weights_time_ms: 7.635\n",
      "    training_iteration_time_ms: 8962.558\n",
      "  timestamp: 1662330247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495396\n",
      "  training_iteration: 99\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=5510.20.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=5510.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 500400\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 500400\n",
      "    num_agent_steps_trained: 500400\n",
      "    num_env_steps_sampled: 500400\n",
      "    num_env_steps_trained: 500400\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-24-16\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78692805512081\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.1554436679038213e-31\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.385683298110962\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1629770597210154e-05\n",
      "          policy_loss: 0.08761271834373474\n",
      "          total_loss: 4.692473411560059\n",
      "          vf_explained_var: -0.001133221434429288\n",
      "          vf_loss: 4.604859828948975\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 500400\n",
      "    num_agent_steps_trained: 500400\n",
      "    num_env_steps_sampled: 500400\n",
      "    num_env_steps_trained: 500400\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 500400\n",
      "  num_agent_steps_trained: 500400\n",
      "  num_env_steps_sampled: 500400\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 500400\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.55384615384615\n",
      "    ram_util_percent: 36.892307692307696\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695994076281536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.591788442107194\n",
      "    mean_inference_ms: 10.85438011396727\n",
      "    mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78692805512081\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11695994076281536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.591788442107194\n",
      "      mean_inference_ms: 10.85438011396727\n",
      "      mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  time_since_restore: 926.9973204135895\n",
      "  time_this_iter_s: 8.945863962173462\n",
      "  time_total_s: 926.9973204135895\n",
      "  timers:\n",
      "    learn_throughput: 14113.272\n",
      "    learn_time_ms: 354.56\n",
      "    synch_weights_time_ms: 7.648\n",
      "    training_iteration_time_ms: 8956.129\n",
      "  timestamp: 1662330256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500400\n",
      "  training_iteration: 100\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 505404\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 505404\n",
      "    num_agent_steps_trained: 505404\n",
      "    num_env_steps_sampled: 505404\n",
      "    num_env_steps_trained: 505404\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-24-25\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78692805512081\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5777218339519106e-31\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.356920838356018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.888470069388859e-05\n",
      "          policy_loss: 0.0672929435968399\n",
      "          total_loss: 0.06761350482702255\n",
      "          vf_explained_var: 0.32810795307159424\n",
      "          vf_loss: 0.00032056582858785987\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 505404\n",
      "    num_agent_steps_trained: 505404\n",
      "    num_env_steps_sampled: 505404\n",
      "    num_env_steps_trained: 505404\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 505404\n",
      "  num_agent_steps_trained: 505404\n",
      "  num_env_steps_sampled: 505404\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 505404\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.91666666666667\n",
      "    ram_util_percent: 36.949999999999996\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695994076281536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.591788442107194\n",
      "    mean_inference_ms: 10.85438011396727\n",
      "    mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78692805512081\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11695994076281536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.591788442107194\n",
      "      mean_inference_ms: 10.85438011396727\n",
      "      mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  time_since_restore: 936.1637794971466\n",
      "  time_this_iter_s: 9.166459083557129\n",
      "  time_total_s: 936.1637794971466\n",
      "  timers:\n",
      "    learn_throughput: 13992.24\n",
      "    learn_time_ms: 357.627\n",
      "    synch_weights_time_ms: 7.679\n",
      "    training_iteration_time_ms: 8994.875\n",
      "  timestamp: 1662330265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 505404\n",
      "  training_iteration: 101\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 510408\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 510408\n",
      "    num_agent_steps_trained: 510408\n",
      "    num_env_steps_sampled: 510408\n",
      "    num_env_steps_trained: 510408\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-24-34\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78692805512081\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.888609169759553e-32\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3540408611297607\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0311507594451541e-06\n",
      "          policy_loss: -0.07457618415355682\n",
      "          total_loss: -0.07449262589216232\n",
      "          vf_explained_var: 0.14187496900558472\n",
      "          vf_loss: 8.356152102351189e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 510408\n",
      "    num_agent_steps_trained: 510408\n",
      "    num_env_steps_sampled: 510408\n",
      "    num_env_steps_trained: 510408\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 510408\n",
      "  num_agent_steps_trained: 510408\n",
      "  num_env_steps_sampled: 510408\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 510408\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 72.26923076923077\n",
      "    ram_util_percent: 37.0076923076923\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695994076281536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.591788442107194\n",
      "    mean_inference_ms: 10.85438011396727\n",
      "    mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78692805512081\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11695994076281536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.591788442107194\n",
      "      mean_inference_ms: 10.85438011396727\n",
      "      mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  time_since_restore: 945.2064075469971\n",
      "  time_this_iter_s: 9.042628049850464\n",
      "  time_total_s: 945.2064075469971\n",
      "  timers:\n",
      "    learn_throughput: 14013.573\n",
      "    learn_time_ms: 357.082\n",
      "    synch_weights_time_ms: 7.733\n",
      "    training_iteration_time_ms: 8981.131\n",
      "  timestamp: 1662330274\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510408\n",
      "  training_iteration: 102\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 515412\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 515412\n",
      "    num_agent_steps_trained: 515412\n",
      "    num_env_steps_sampled: 515412\n",
      "    num_env_steps_trained: 515412\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-24-43\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78692805512081\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.9443045848797766e-32\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3556016683578491\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.4374251122717396e-06\n",
      "          policy_loss: 0.039514727890491486\n",
      "          total_loss: 0.03958914428949356\n",
      "          vf_explained_var: 0.14674130082130432\n",
      "          vf_loss: 7.441093475790694e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 515412\n",
      "    num_agent_steps_trained: 515412\n",
      "    num_env_steps_sampled: 515412\n",
      "    num_env_steps_trained: 515412\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 515412\n",
      "  num_agent_steps_trained: 515412\n",
      "  num_env_steps_sampled: 515412\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 515412\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 71.83076923076923\n",
      "    ram_util_percent: 37.03846153846153\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695994076281536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.591788442107194\n",
      "    mean_inference_ms: 10.85438011396727\n",
      "    mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78692805512081\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11695994076281536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.591788442107194\n",
      "      mean_inference_ms: 10.85438011396727\n",
      "      mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  time_since_restore: 954.3052315711975\n",
      "  time_this_iter_s: 9.09882402420044\n",
      "  time_total_s: 954.3052315711975\n",
      "  timers:\n",
      "    learn_throughput: 14033.782\n",
      "    learn_time_ms: 356.568\n",
      "    synch_weights_time_ms: 7.725\n",
      "    training_iteration_time_ms: 8997.213\n",
      "  timestamp: 1662330283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 515412\n",
      "  training_iteration: 103\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 520416\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 520416\n",
      "    num_agent_steps_trained: 520416\n",
      "    num_env_steps_sampled: 520416\n",
      "    num_env_steps_trained: 520416\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-24-52\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78692805512081\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9721522924398883e-32\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3557062149047852\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.990966193778149e-07\n",
      "          policy_loss: -0.04466712474822998\n",
      "          total_loss: -0.0445956215262413\n",
      "          vf_explained_var: 0.19790738821029663\n",
      "          vf_loss: 7.150382589315996e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 520416\n",
      "    num_agent_steps_trained: 520416\n",
      "    num_env_steps_sampled: 520416\n",
      "    num_env_steps_trained: 520416\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 520416\n",
      "  num_agent_steps_trained: 520416\n",
      "  num_env_steps_sampled: 520416\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 520416\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.8\n",
      "    ram_util_percent: 36.883333333333326\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695994076281536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.591788442107194\n",
      "    mean_inference_ms: 10.85438011396727\n",
      "    mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78692805512081\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11695994076281536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.591788442107194\n",
      "      mean_inference_ms: 10.85438011396727\n",
      "      mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  time_since_restore: 963.1985716819763\n",
      "  time_this_iter_s: 8.893340110778809\n",
      "  time_total_s: 963.1985716819763\n",
      "  timers:\n",
      "    learn_throughput: 13962.635\n",
      "    learn_time_ms: 358.385\n",
      "    synch_weights_time_ms: 7.687\n",
      "    training_iteration_time_ms: 9018.441\n",
      "  timestamp: 1662330292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520416\n",
      "  training_iteration: 104\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 525420\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 525420\n",
      "    num_agent_steps_trained: 525420\n",
      "    num_env_steps_sampled: 525420\n",
      "    num_env_steps_trained: 525420\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-25-01\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78692805512081\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.860761462199441e-33\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3563380241394043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.601086741080508e-07\n",
      "          policy_loss: -0.0009260461665689945\n",
      "          total_loss: -0.0008543433505110443\n",
      "          vf_explained_var: 0.25167325139045715\n",
      "          vf_loss: 7.171978359110653e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 525420\n",
      "    num_agent_steps_trained: 525420\n",
      "    num_env_steps_sampled: 525420\n",
      "    num_env_steps_trained: 525420\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 525420\n",
      "  num_agent_steps_trained: 525420\n",
      "  num_env_steps_sampled: 525420\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 525420\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.74166666666666\n",
      "    ram_util_percent: 36.99166666666667\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695994076281536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.591788442107194\n",
      "    mean_inference_ms: 10.85438011396727\n",
      "    mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78692805512081\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11695994076281536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.591788442107194\n",
      "      mean_inference_ms: 10.85438011396727\n",
      "      mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  time_since_restore: 972.1541726589203\n",
      "  time_this_iter_s: 8.95560097694397\n",
      "  time_total_s: 972.1541726589203\n",
      "  timers:\n",
      "    learn_throughput: 13922.94\n",
      "    learn_time_ms: 359.407\n",
      "    synch_weights_time_ms: 7.647\n",
      "    training_iteration_time_ms: 9016.098\n",
      "  timestamp: 1662330301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525420\n",
      "  training_iteration: 105\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 530424\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 530424\n",
      "    num_agent_steps_trained: 530424\n",
      "    num_env_steps_sampled: 530424\n",
      "    num_env_steps_trained: 530424\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-25-10\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78692805512081\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.930380731099721e-33\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3582193851470947\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.0746982601122e-06\n",
      "          policy_loss: -0.11463803052902222\n",
      "          total_loss: -0.1145697832107544\n",
      "          vf_explained_var: 0.04995061457157135\n",
      "          vf_loss: 6.824377487646416e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 530424\n",
      "    num_agent_steps_trained: 530424\n",
      "    num_env_steps_sampled: 530424\n",
      "    num_env_steps_trained: 530424\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 530424\n",
      "  num_agent_steps_trained: 530424\n",
      "  num_env_steps_sampled: 530424\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 530424\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.4076923076923\n",
      "    ram_util_percent: 36.91538461538461\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695994076281536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.591788442107194\n",
      "    mean_inference_ms: 10.85438011396727\n",
      "    mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78692805512081\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11695994076281536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.591788442107194\n",
      "      mean_inference_ms: 10.85438011396727\n",
      "      mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  time_since_restore: 981.0840547084808\n",
      "  time_this_iter_s: 8.929882049560547\n",
      "  time_total_s: 981.0840547084808\n",
      "  timers:\n",
      "    learn_throughput: 13959.821\n",
      "    learn_time_ms: 358.457\n",
      "    synch_weights_time_ms: 7.635\n",
      "    training_iteration_time_ms: 9006.59\n",
      "  timestamp: 1662330310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 530424\n",
      "  training_iteration: 106\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 535428\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 535428\n",
      "    num_agent_steps_trained: 535428\n",
      "    num_env_steps_sampled: 535428\n",
      "    num_env_steps_trained: 535428\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-25-19\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.7975513424916\n",
      "  episode_reward_mean: 102.78692805512081\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 99\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4651903655498604e-33\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3595997095108032\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.446101805413491e-06\n",
      "          policy_loss: 0.06544522941112518\n",
      "          total_loss: 0.06551645696163177\n",
      "          vf_explained_var: 0.34180131554603577\n",
      "          vf_loss: 7.122532406356186e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 535428\n",
      "    num_agent_steps_trained: 535428\n",
      "    num_env_steps_sampled: 535428\n",
      "    num_env_steps_trained: 535428\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 535428\n",
      "  num_agent_steps_trained: 535428\n",
      "  num_env_steps_sampled: 535428\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 535428\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.77500000000002\n",
      "    ram_util_percent: 37.099999999999994\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695994076281536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.591788442107194\n",
      "    mean_inference_ms: 10.85438011396727\n",
      "    mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.7975513424916\n",
      "    episode_reward_mean: 102.78692805512081\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [103.7974450028322, 103.79158961501464, 102.46316869718756, 102.64823463091926,\n",
      "        102.64875150822446, 102.51930550939997, 103.08866539989138, 101.94032551804062,\n",
      "        102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11695994076281536\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.591788442107194\n",
      "      mean_inference_ms: 10.85438011396727\n",
      "      mean_raw_obs_processing_ms: 0.2064370042291052\n",
      "  time_since_restore: 989.9222617149353\n",
      "  time_this_iter_s: 8.838207006454468\n",
      "  time_total_s: 989.9222617149353\n",
      "  timers:\n",
      "    learn_throughput: 13993.514\n",
      "    learn_time_ms: 357.594\n",
      "    synch_weights_time_ms: 7.619\n",
      "    training_iteration_time_ms: 8982.613\n",
      "  timestamp: 1662330319\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535428\n",
      "  training_iteration: 107\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 540432\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 540432\n",
      "    num_agent_steps_trained: 540432\n",
      "    num_env_steps_sampled: 540432\n",
      "    num_env_steps_trained: 540432\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78093730025607\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 108\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2325951827749302e-33\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3621454238891602\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.5482939892972354e-06\n",
      "          policy_loss: 0.06534817069768906\n",
      "          total_loss: 0.703900933265686\n",
      "          vf_explained_var: 0.022803068161010742\n",
      "          vf_loss: 0.638552725315094\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 540432\n",
      "    num_agent_steps_trained: 540432\n",
      "    num_env_steps_sampled: 540432\n",
      "    num_env_steps_trained: 540432\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 540432\n",
      "  num_agent_steps_trained: 540432\n",
      "  num_env_steps_sampled: 540432\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 540432\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.10000000000001\n",
      "    ram_util_percent: 37.008333333333326\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11681467798941786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.5857894085454936\n",
      "    mean_inference_ms: 10.844609655180085\n",
      "    mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78093730025607\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11681467798941786\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.5857894085454936\n",
      "      mean_inference_ms: 10.844609655180085\n",
      "      mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  time_since_restore: 998.8223848342896\n",
      "  time_this_iter_s: 8.900123119354248\n",
      "  time_total_s: 998.8223848342896\n",
      "  timers:\n",
      "    learn_throughput: 14023.479\n",
      "    learn_time_ms: 356.83\n",
      "    synch_weights_time_ms: 7.538\n",
      "    training_iteration_time_ms: 8964.587\n",
      "  timestamp: 1662330328\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540432\n",
      "  training_iteration: 108\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=6010.30.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=6010.50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 545436\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 545436\n",
      "    num_agent_steps_trained: 545436\n",
      "    num_env_steps_sampled: 545436\n",
      "    num_env_steps_trained: 545436\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-25-37\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78093730025607\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 108\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.162975913874651e-34\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3788559436798096\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.374132807541173e-05\n",
      "          policy_loss: 0.07014212757349014\n",
      "          total_loss: 4.598898887634277\n",
      "          vf_explained_var: -0.0013582974206656218\n",
      "          vf_loss: 4.528756618499756\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 545436\n",
      "    num_agent_steps_trained: 545436\n",
      "    num_env_steps_sampled: 545436\n",
      "    num_env_steps_trained: 545436\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 545436\n",
      "  num_agent_steps_trained: 545436\n",
      "  num_env_steps_sampled: 545436\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 545436\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.5076923076923\n",
      "    ram_util_percent: 37.153846153846146\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11681467798941786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.5857894085454936\n",
      "    mean_inference_ms: 10.844609655180085\n",
      "    mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78093730025607\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11681467798941786\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.5857894085454936\n",
      "      mean_inference_ms: 10.844609655180085\n",
      "      mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  time_since_restore: 1007.7765765190125\n",
      "  time_this_iter_s: 8.9541916847229\n",
      "  time_total_s: 1007.7765765190125\n",
      "  timers:\n",
      "    learn_throughput: 13972.333\n",
      "    learn_time_ms: 358.136\n",
      "    synch_weights_time_ms: 7.62\n",
      "    training_iteration_time_ms: 8965.085\n",
      "  timestamp: 1662330337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 545436\n",
      "  training_iteration: 109\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 550440\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 550440\n",
      "    num_agent_steps_trained: 550440\n",
      "    num_env_steps_sampled: 550440\n",
      "    num_env_steps_trained: 550440\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-25-46\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78093730025607\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 108\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.0814879569373254e-34\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.344756007194519\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.029461393249221e-05\n",
      "          policy_loss: 0.09443002939224243\n",
      "          total_loss: 0.094673290848732\n",
      "          vf_explained_var: 0.2647874355316162\n",
      "          vf_loss: 0.0002432721375953406\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 550440\n",
      "    num_agent_steps_trained: 550440\n",
      "    num_env_steps_sampled: 550440\n",
      "    num_env_steps_trained: 550440\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 550440\n",
      "  num_agent_steps_trained: 550440\n",
      "  num_env_steps_sampled: 550440\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 550440\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.56666666666665\n",
      "    ram_util_percent: 37.025\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11681467798941786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.5857894085454936\n",
      "    mean_inference_ms: 10.844609655180085\n",
      "    mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78093730025607\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11681467798941786\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.5857894085454936\n",
      "      mean_inference_ms: 10.844609655180085\n",
      "      mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  time_since_restore: 1016.6980583667755\n",
      "  time_this_iter_s: 8.921481847763062\n",
      "  time_total_s: 1016.6980583667755\n",
      "  timers:\n",
      "    learn_throughput: 13977.476\n",
      "    learn_time_ms: 358.005\n",
      "    synch_weights_time_ms: 7.6\n",
      "    training_iteration_time_ms: 8962.642\n",
      "  timestamp: 1662330346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 550440\n",
      "  training_iteration: 110\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 555444\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 555444\n",
      "    num_agent_steps_trained: 555444\n",
      "    num_env_steps_sampled: 555444\n",
      "    num_env_steps_trained: 555444\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-25-55\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78093730025607\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 108\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5407439784686627e-34\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3391709327697754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0036465027951635e-05\n",
      "          policy_loss: 0.043683670461177826\n",
      "          total_loss: 0.04377259686589241\n",
      "          vf_explained_var: 0.20344796776771545\n",
      "          vf_loss: 8.892973710317165e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 555444\n",
      "    num_agent_steps_trained: 555444\n",
      "    num_env_steps_sampled: 555444\n",
      "    num_env_steps_trained: 555444\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 555444\n",
      "  num_agent_steps_trained: 555444\n",
      "  num_env_steps_sampled: 555444\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 555444\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 71.78461538461538\n",
      "    ram_util_percent: 37.061538461538454\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11681467798941786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.5857894085454936\n",
      "    mean_inference_ms: 10.844609655180085\n",
      "    mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78093730025607\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11681467798941786\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.5857894085454936\n",
      "      mean_inference_ms: 10.844609655180085\n",
      "      mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  time_since_restore: 1025.8203403949738\n",
      "  time_this_iter_s: 9.122282028198242\n",
      "  time_total_s: 1025.8203403949738\n",
      "  timers:\n",
      "    learn_throughput: 13972.906\n",
      "    learn_time_ms: 358.122\n",
      "    synch_weights_time_ms: 7.621\n",
      "    training_iteration_time_ms: 8958.189\n",
      "  timestamp: 1662330355\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555444\n",
      "  training_iteration: 111\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 560448\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 560448\n",
      "    num_agent_steps_trained: 560448\n",
      "    num_env_steps_sampled: 560448\n",
      "    num_env_steps_trained: 560448\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-26-04\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78093730025607\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 108\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.703719892343314e-35\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.337382435798645\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.422516313861706e-07\n",
      "          policy_loss: -0.10650881379842758\n",
      "          total_loss: -0.10643088817596436\n",
      "          vf_explained_var: 0.166751891374588\n",
      "          vf_loss: 7.795249985065311e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 560448\n",
      "    num_agent_steps_trained: 560448\n",
      "    num_env_steps_sampled: 560448\n",
      "    num_env_steps_trained: 560448\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 560448\n",
      "  num_agent_steps_trained: 560448\n",
      "  num_env_steps_sampled: 560448\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 560448\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.88333333333334\n",
      "    ram_util_percent: 36.99999999999999\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11681467798941786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.5857894085454936\n",
      "    mean_inference_ms: 10.844609655180085\n",
      "    mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78093730025607\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11681467798941786\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.5857894085454936\n",
      "      mean_inference_ms: 10.844609655180085\n",
      "      mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  time_since_restore: 1034.8167951107025\n",
      "  time_this_iter_s: 8.99645471572876\n",
      "  time_total_s: 1034.8167951107025\n",
      "  timers:\n",
      "    learn_throughput: 14000.812\n",
      "    learn_time_ms: 357.408\n",
      "    synch_weights_time_ms: 7.578\n",
      "    training_iteration_time_ms: 8953.568\n",
      "  timestamp: 1662330364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560448\n",
      "  training_iteration: 112\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 565452\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 565452\n",
      "    num_agent_steps_trained: 565452\n",
      "    num_env_steps_sampled: 565452\n",
      "    num_env_steps_trained: 565452\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-26-13\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78093730025607\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 108\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.851859946171657e-35\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3389627933502197\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.242180804998497e-07\n",
      "          policy_loss: 0.09725301712751389\n",
      "          total_loss: 0.09733302891254425\n",
      "          vf_explained_var: 0.29552748799324036\n",
      "          vf_loss: 8.001269452506676e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 565452\n",
      "    num_agent_steps_trained: 565452\n",
      "    num_env_steps_sampled: 565452\n",
      "    num_env_steps_trained: 565452\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 565452\n",
      "  num_agent_steps_trained: 565452\n",
      "  num_env_steps_sampled: 565452\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 565452\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.96153846153847\n",
      "    ram_util_percent: 37.19230769230769\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11681467798941786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.5857894085454936\n",
      "    mean_inference_ms: 10.844609655180085\n",
      "    mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78093730025607\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11681467798941786\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.5857894085454936\n",
      "      mean_inference_ms: 10.844609655180085\n",
      "      mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  time_since_restore: 1043.7474029064178\n",
      "  time_this_iter_s: 8.930607795715332\n",
      "  time_total_s: 1043.7474029064178\n",
      "  timers:\n",
      "    learn_throughput: 13972.164\n",
      "    learn_time_ms: 358.141\n",
      "    synch_weights_time_ms: 7.587\n",
      "    training_iteration_time_ms: 8936.768\n",
      "  timestamp: 1662330373\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 565452\n",
      "  training_iteration: 113\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 570456\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 570456\n",
      "    num_agent_steps_trained: 570456\n",
      "    num_env_steps_sampled: 570456\n",
      "    num_env_steps_trained: 570456\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-26-22\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78093730025607\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 108\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.9259299730858284e-35\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3368539810180664\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8903261889136047e-06\n",
      "          policy_loss: 0.010431955568492413\n",
      "          total_loss: 0.010509165935218334\n",
      "          vf_explained_var: 0.16679859161376953\n",
      "          vf_loss: 7.720647408859804e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 570456\n",
      "    num_agent_steps_trained: 570456\n",
      "    num_env_steps_sampled: 570456\n",
      "    num_env_steps_trained: 570456\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 570456\n",
      "  num_agent_steps_trained: 570456\n",
      "  num_env_steps_sampled: 570456\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 570456\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.59999999999998\n",
      "    ram_util_percent: 37.17499999999999\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11681467798941786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.5857894085454936\n",
      "    mean_inference_ms: 10.844609655180085\n",
      "    mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78093730025607\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11681467798941786\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.5857894085454936\n",
      "      mean_inference_ms: 10.844609655180085\n",
      "      mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  time_since_restore: 1052.6549808979034\n",
      "  time_this_iter_s: 8.907577991485596\n",
      "  time_total_s: 1052.6549808979034\n",
      "  timers:\n",
      "    learn_throughput: 13969.563\n",
      "    learn_time_ms: 358.207\n",
      "    synch_weights_time_ms: 7.563\n",
      "    training_iteration_time_ms: 8938.157\n",
      "  timestamp: 1662330382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 570456\n",
      "  training_iteration: 114\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 575460\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 575460\n",
      "    num_agent_steps_trained: 575460\n",
      "    num_env_steps_sampled: 575460\n",
      "    num_env_steps_trained: 575460\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-26-31\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78093730025607\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 108\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 9.629649865429142e-36\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3363335132598877\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.6290325289864995e-08\n",
      "          policy_loss: -0.07404571771621704\n",
      "          total_loss: -0.07397054880857468\n",
      "          vf_explained_var: 0.23082523047924042\n",
      "          vf_loss: 7.516550249420106e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 575460\n",
      "    num_agent_steps_trained: 575460\n",
      "    num_env_steps_sampled: 575460\n",
      "    num_env_steps_trained: 575460\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 575460\n",
      "  num_agent_steps_trained: 575460\n",
      "  num_env_steps_sampled: 575460\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 575460\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 71.5923076923077\n",
      "    ram_util_percent: 37.23846153846153\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11681467798941786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.5857894085454936\n",
      "    mean_inference_ms: 10.844609655180085\n",
      "    mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78093730025607\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11681467798941786\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.5857894085454936\n",
      "      mean_inference_ms: 10.844609655180085\n",
      "      mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  time_since_restore: 1061.7036581039429\n",
      "  time_this_iter_s: 9.048677206039429\n",
      "  time_total_s: 1061.7036581039429\n",
      "  timers:\n",
      "    learn_throughput: 13875.15\n",
      "    learn_time_ms: 360.645\n",
      "    synch_weights_time_ms: 7.602\n",
      "    training_iteration_time_ms: 8947.442\n",
      "  timestamp: 1662330391\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 575460\n",
      "  training_iteration: 115\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 580464\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 580464\n",
      "    num_agent_steps_trained: 580464\n",
      "    num_env_steps_sampled: 580464\n",
      "    num_env_steps_trained: 580464\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-26-40\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78093730025607\n",
      "  episode_reward_min: 101.93794751526434\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 108\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 4.814824932714571e-36\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3391456604003906\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1233099940000102e-05\n",
      "          policy_loss: 0.016333268955349922\n",
      "          total_loss: 0.016408739611506462\n",
      "          vf_explained_var: 0.06291957199573517\n",
      "          vf_loss: 7.547980203526095e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 580464\n",
      "    num_agent_steps_trained: 580464\n",
      "    num_env_steps_sampled: 580464\n",
      "    num_env_steps_trained: 580464\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 580464\n",
      "  num_agent_steps_trained: 580464\n",
      "  num_env_steps_sampled: 580464\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 580464\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 72.53333333333332\n",
      "    ram_util_percent: 37.14166666666666\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11681467798941786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.5857894085454936\n",
      "    mean_inference_ms: 10.844609655180085\n",
      "    mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78093730025607\n",
      "    episode_reward_min: 101.93794751526434\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.22090486647183, 103.797003971608, 103.76953883410536, 102.46081713537805,\n",
      "        102.6458359165362, 102.64647988370697, 102.51712553244901, 103.08633824428215,\n",
      "        101.93794751526434, 102.19875448442272, 103.79687406851185, 103.77135753401382,\n",
      "        102.46263616939216, 102.64753991526167, 102.64811418259029, 102.51874561896078,\n",
      "        103.0880851629313, 101.93975173061912, 102.20038822865497, 103.79625847951847,\n",
      "        103.77073504509276, 102.46204510235188, 102.6469371365317, 102.64756943425583,\n",
      "        102.51821540542055, 103.0875192390335, 101.93912031605522, 102.19980944928844,\n",
      "        103.79682060964069, 103.76936051660124, 102.4606451383536, 102.64557724996303,\n",
      "        102.64621198307653, 102.5169109108498, 103.08613407616481, 101.9436697277808,\n",
      "        102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11681467798941786\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.5857894085454936\n",
      "      mean_inference_ms: 10.844609655180085\n",
      "      mean_raw_obs_processing_ms: 0.20727857455515522\n",
      "  time_since_restore: 1070.6543691158295\n",
      "  time_this_iter_s: 8.950711011886597\n",
      "  time_total_s: 1070.6543691158295\n",
      "  timers:\n",
      "    learn_throughput: 13966.444\n",
      "    learn_time_ms: 358.287\n",
      "    synch_weights_time_ms: 7.544\n",
      "    training_iteration_time_ms: 8949.537\n",
      "  timestamp: 1662330400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580464\n",
      "  training_iteration: 116\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 585468\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 585468\n",
      "    num_agent_steps_trained: 585468\n",
      "    num_env_steps_sampled: 585468\n",
      "    num_env_steps_trained: 585468\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-26-49\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78085139889768\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 117\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.4074124663572855e-36\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3473047018051147\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.4377223932533525e-06\n",
      "          policy_loss: 0.021087102591991425\n",
      "          total_loss: 0.8949927091598511\n",
      "          vf_explained_var: -0.009328583255410194\n",
      "          vf_loss: 0.8739057779312134\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 585468\n",
      "    num_agent_steps_trained: 585468\n",
      "    num_env_steps_sampled: 585468\n",
      "    num_env_steps_trained: 585468\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 585468\n",
      "  num_agent_steps_trained: 585468\n",
      "  num_env_steps_sampled: 585468\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 585468\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 71.54166666666667\n",
      "    ram_util_percent: 37.008333333333326\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11671662512777013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.576311036211172\n",
      "    mean_inference_ms: 10.834992703559188\n",
      "    mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78085139889768\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19875448442272, 103.79687406851185, 103.77135753401382, 102.46263616939216,\n",
      "        102.64753991526167, 102.64811418259029, 102.51874561896078, 103.0880851629313,\n",
      "        101.93975173061912, 102.20038822865497, 103.79625847951847, 103.77073504509276,\n",
      "        102.46204510235188, 102.6469371365317, 102.64756943425583, 102.51821540542055,\n",
      "        103.0875192390335, 101.93912031605522, 102.19980944928844, 103.79682060964069,\n",
      "        103.76936051660124, 102.4606451383536, 102.64557724996303, 102.64621198307653,\n",
      "        102.5169109108498, 103.08613407616481, 101.9436697277808, 102.19841162553591,\n",
      "        103.79614241723428, 103.77064000820155, 102.4618886587304, 102.646880882052,\n",
      "        102.64750124916208, 102.51810285984993, 103.08738079080894, 101.93897894630204,\n",
      "        102.19973215059784, 103.79732238534635, 103.76980686111476, 102.46114650815524,\n",
      "        102.64602645801769, 102.64668308160944, 102.51737039893044, 103.08662327344526,\n",
      "        101.93821971981195, 102.19889060005423, 103.79676209772195, 103.76928762724583,\n",
      "        102.4605848614072, 102.64549554932918, 102.64614825929499, 102.51686374187916,\n",
      "        103.08606478052307, 101.94362155570418, 102.19836109768045, 103.7968226749796,\n",
      "        103.77068989422041, 102.51700274093768, 102.64780285788922, 102.64806596239531,\n",
      "        102.5186024471645, 103.08811951417884, 101.94498233858293, 102.2605462926252,\n",
      "        103.7975513424916, 103.77024914103386, 102.4613630396782, 102.64646441571935,\n",
      "        102.64691369312487, 102.51755289982283, 103.0868646049081, 101.94444673847279,\n",
      "        102.19916200844175, 103.79696950272854, 103.77107623251682, 102.4607642551226,\n",
      "        102.64569098491162, 102.64631875318773, 102.5170526312697, 103.08624823474777,\n",
      "        101.9438032285913, 102.19854788482233, 103.79902259922984, 103.77333039131061,\n",
      "        102.46284084530903, 102.64810243215321, 102.6486015244451, 102.51908321756594,\n",
      "        103.08841024987291, 101.93988269596869, 102.2060644942997, 103.79600335781976,\n",
      "        103.77745901303042, 102.46178942377085, 102.64686816008972, 102.64750815163215,\n",
      "        102.517960424526, 103.08724024642771, 101.9390098484461, 102.19956313822217]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11671662512777013\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.576311036211172\n",
      "      mean_inference_ms: 10.834992703559188\n",
      "      mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  time_since_restore: 1079.4994859695435\n",
      "  time_this_iter_s: 8.84511685371399\n",
      "  time_total_s: 1079.4994859695435\n",
      "  timers:\n",
      "    learn_throughput: 13895.691\n",
      "    learn_time_ms: 360.112\n",
      "    synch_weights_time_ms: 7.608\n",
      "    training_iteration_time_ms: 8950.18\n",
      "  timestamp: 1662330409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 585468\n",
      "  training_iteration: 117\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=6510.40.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=6510.60.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 590472\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 590472\n",
      "    num_agent_steps_trained: 590472\n",
      "    num_env_steps_sampled: 590472\n",
      "    num_env_steps_trained: 590472\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78085139889768\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 117\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2037062331786428e-36\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3665322065353394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8738066501100548e-05\n",
      "          policy_loss: 0.01774388924241066\n",
      "          total_loss: 4.824573993682861\n",
      "          vf_explained_var: -0.0013543546665459871\n",
      "          vf_loss: 4.806830406188965\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 590472\n",
      "    num_agent_steps_trained: 590472\n",
      "    num_env_steps_sampled: 590472\n",
      "    num_env_steps_trained: 590472\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 590472\n",
      "  num_agent_steps_trained: 590472\n",
      "  num_env_steps_sampled: 590472\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 590472\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 73.8923076923077\n",
      "    ram_util_percent: 37.06153846153846\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11671662512777013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.576311036211172\n",
      "    mean_inference_ms: 10.834992703559188\n",
      "    mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78085139889768\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19875448442272, 103.79687406851185, 103.77135753401382, 102.46263616939216,\n",
      "        102.64753991526167, 102.64811418259029, 102.51874561896078, 103.0880851629313,\n",
      "        101.93975173061912, 102.20038822865497, 103.79625847951847, 103.77073504509276,\n",
      "        102.46204510235188, 102.6469371365317, 102.64756943425583, 102.51821540542055,\n",
      "        103.0875192390335, 101.93912031605522, 102.19980944928844, 103.79682060964069,\n",
      "        103.76936051660124, 102.4606451383536, 102.64557724996303, 102.64621198307653,\n",
      "        102.5169109108498, 103.08613407616481, 101.9436697277808, 102.19841162553591,\n",
      "        103.79614241723428, 103.77064000820155, 102.4618886587304, 102.646880882052,\n",
      "        102.64750124916208, 102.51810285984993, 103.08738079080894, 101.93897894630204,\n",
      "        102.19973215059784, 103.79732238534635, 103.76980686111476, 102.46114650815524,\n",
      "        102.64602645801769, 102.64668308160944, 102.51737039893044, 103.08662327344526,\n",
      "        101.93821971981195, 102.19889060005423, 103.79676209772195, 103.76928762724583,\n",
      "        102.4605848614072, 102.64549554932918, 102.64614825929499, 102.51686374187916,\n",
      "        103.08606478052307, 101.94362155570418, 102.19836109768045, 103.7968226749796,\n",
      "        103.77068989422041, 102.51700274093768, 102.64780285788922, 102.64806596239531,\n",
      "        102.5186024471645, 103.08811951417884, 101.94498233858293, 102.2605462926252,\n",
      "        103.7975513424916, 103.77024914103386, 102.4613630396782, 102.64646441571935,\n",
      "        102.64691369312487, 102.51755289982283, 103.0868646049081, 101.94444673847279,\n",
      "        102.19916200844175, 103.79696950272854, 103.77107623251682, 102.4607642551226,\n",
      "        102.64569098491162, 102.64631875318773, 102.5170526312697, 103.08624823474777,\n",
      "        101.9438032285913, 102.19854788482233, 103.79902259922984, 103.77333039131061,\n",
      "        102.46284084530903, 102.64810243215321, 102.6486015244451, 102.51908321756594,\n",
      "        103.08841024987291, 101.93988269596869, 102.2060644942997, 103.79600335781976,\n",
      "        103.77745901303042, 102.46178942377085, 102.64686816008972, 102.64750815163215,\n",
      "        102.517960424526, 103.08724024642771, 101.9390098484461, 102.19956313822217]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11671662512777013\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.576311036211172\n",
      "      mean_inference_ms: 10.834992703559188\n",
      "      mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  time_since_restore: 1088.4035258293152\n",
      "  time_this_iter_s: 8.904039859771729\n",
      "  time_total_s: 1088.4035258293152\n",
      "  timers:\n",
      "    learn_throughput: 13852.124\n",
      "    learn_time_ms: 361.244\n",
      "    synch_weights_time_ms: 7.7\n",
      "    training_iteration_time_ms: 8950.562\n",
      "  timestamp: 1662330418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 590472\n",
      "  training_iteration: 118\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 595476\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 595476\n",
      "    num_agent_steps_trained: 595476\n",
      "    num_env_steps_sampled: 595476\n",
      "    num_env_steps_trained: 595476\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-27-07\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78085139889768\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 117\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 6.018531165893214e-37\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3266712427139282\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.5867223030218156e-06\n",
      "          policy_loss: -0.003174443496391177\n",
      "          total_loss: -0.002746548503637314\n",
      "          vf_explained_var: 0.25235337018966675\n",
      "          vf_loss: 0.00042789947474375367\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 595476\n",
      "    num_agent_steps_trained: 595476\n",
      "    num_env_steps_sampled: 595476\n",
      "    num_env_steps_trained: 595476\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 595476\n",
      "  num_agent_steps_trained: 595476\n",
      "  num_env_steps_sampled: 595476\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 595476\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.41666666666667\n",
      "    ram_util_percent: 36.93333333333333\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11671662512777013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.576311036211172\n",
      "    mean_inference_ms: 10.834992703559188\n",
      "    mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78085139889768\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19875448442272, 103.79687406851185, 103.77135753401382, 102.46263616939216,\n",
      "        102.64753991526167, 102.64811418259029, 102.51874561896078, 103.0880851629313,\n",
      "        101.93975173061912, 102.20038822865497, 103.79625847951847, 103.77073504509276,\n",
      "        102.46204510235188, 102.6469371365317, 102.64756943425583, 102.51821540542055,\n",
      "        103.0875192390335, 101.93912031605522, 102.19980944928844, 103.79682060964069,\n",
      "        103.76936051660124, 102.4606451383536, 102.64557724996303, 102.64621198307653,\n",
      "        102.5169109108498, 103.08613407616481, 101.9436697277808, 102.19841162553591,\n",
      "        103.79614241723428, 103.77064000820155, 102.4618886587304, 102.646880882052,\n",
      "        102.64750124916208, 102.51810285984993, 103.08738079080894, 101.93897894630204,\n",
      "        102.19973215059784, 103.79732238534635, 103.76980686111476, 102.46114650815524,\n",
      "        102.64602645801769, 102.64668308160944, 102.51737039893044, 103.08662327344526,\n",
      "        101.93821971981195, 102.19889060005423, 103.79676209772195, 103.76928762724583,\n",
      "        102.4605848614072, 102.64549554932918, 102.64614825929499, 102.51686374187916,\n",
      "        103.08606478052307, 101.94362155570418, 102.19836109768045, 103.7968226749796,\n",
      "        103.77068989422041, 102.51700274093768, 102.64780285788922, 102.64806596239531,\n",
      "        102.5186024471645, 103.08811951417884, 101.94498233858293, 102.2605462926252,\n",
      "        103.7975513424916, 103.77024914103386, 102.4613630396782, 102.64646441571935,\n",
      "        102.64691369312487, 102.51755289982283, 103.0868646049081, 101.94444673847279,\n",
      "        102.19916200844175, 103.79696950272854, 103.77107623251682, 102.4607642551226,\n",
      "        102.64569098491162, 102.64631875318773, 102.5170526312697, 103.08624823474777,\n",
      "        101.9438032285913, 102.19854788482233, 103.79902259922984, 103.77333039131061,\n",
      "        102.46284084530903, 102.64810243215321, 102.6486015244451, 102.51908321756594,\n",
      "        103.08841024987291, 101.93988269596869, 102.2060644942997, 103.79600335781976,\n",
      "        103.77745901303042, 102.46178942377085, 102.64686816008972, 102.64750815163215,\n",
      "        102.517960424526, 103.08724024642771, 101.9390098484461, 102.19956313822217]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11671662512777013\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.576311036211172\n",
      "      mean_inference_ms: 10.834992703559188\n",
      "      mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  time_since_restore: 1097.4157779216766\n",
      "  time_this_iter_s: 9.01225209236145\n",
      "  time_total_s: 1097.4157779216766\n",
      "  timers:\n",
      "    learn_throughput: 13930.044\n",
      "    learn_time_ms: 359.224\n",
      "    synch_weights_time_ms: 7.687\n",
      "    training_iteration_time_ms: 8956.414\n",
      "  timestamp: 1662330427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 595476\n",
      "  training_iteration: 119\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 600480\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 600480\n",
      "    num_agent_steps_trained: 600480\n",
      "    num_env_steps_sampled: 600480\n",
      "    num_env_steps_trained: 600480\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-27-16\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78085139889768\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 117\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.009265582946607e-37\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3249062299728394\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.951487883568916e-07\n",
      "          policy_loss: 0.031101468950510025\n",
      "          total_loss: 0.031217128038406372\n",
      "          vf_explained_var: 0.26626425981521606\n",
      "          vf_loss: 0.00011566642206162214\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 600480\n",
      "    num_agent_steps_trained: 600480\n",
      "    num_env_steps_sampled: 600480\n",
      "    num_env_steps_trained: 600480\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 600480\n",
      "  num_agent_steps_trained: 600480\n",
      "  num_env_steps_sampled: 600480\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 600480\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 71.72307692307692\n",
      "    ram_util_percent: 37.03076923076923\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11671662512777013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.576311036211172\n",
      "    mean_inference_ms: 10.834992703559188\n",
      "    mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78085139889768\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19875448442272, 103.79687406851185, 103.77135753401382, 102.46263616939216,\n",
      "        102.64753991526167, 102.64811418259029, 102.51874561896078, 103.0880851629313,\n",
      "        101.93975173061912, 102.20038822865497, 103.79625847951847, 103.77073504509276,\n",
      "        102.46204510235188, 102.6469371365317, 102.64756943425583, 102.51821540542055,\n",
      "        103.0875192390335, 101.93912031605522, 102.19980944928844, 103.79682060964069,\n",
      "        103.76936051660124, 102.4606451383536, 102.64557724996303, 102.64621198307653,\n",
      "        102.5169109108498, 103.08613407616481, 101.9436697277808, 102.19841162553591,\n",
      "        103.79614241723428, 103.77064000820155, 102.4618886587304, 102.646880882052,\n",
      "        102.64750124916208, 102.51810285984993, 103.08738079080894, 101.93897894630204,\n",
      "        102.19973215059784, 103.79732238534635, 103.76980686111476, 102.46114650815524,\n",
      "        102.64602645801769, 102.64668308160944, 102.51737039893044, 103.08662327344526,\n",
      "        101.93821971981195, 102.19889060005423, 103.79676209772195, 103.76928762724583,\n",
      "        102.4605848614072, 102.64549554932918, 102.64614825929499, 102.51686374187916,\n",
      "        103.08606478052307, 101.94362155570418, 102.19836109768045, 103.7968226749796,\n",
      "        103.77068989422041, 102.51700274093768, 102.64780285788922, 102.64806596239531,\n",
      "        102.5186024471645, 103.08811951417884, 101.94498233858293, 102.2605462926252,\n",
      "        103.7975513424916, 103.77024914103386, 102.4613630396782, 102.64646441571935,\n",
      "        102.64691369312487, 102.51755289982283, 103.0868646049081, 101.94444673847279,\n",
      "        102.19916200844175, 103.79696950272854, 103.77107623251682, 102.4607642551226,\n",
      "        102.64569098491162, 102.64631875318773, 102.5170526312697, 103.08624823474777,\n",
      "        101.9438032285913, 102.19854788482233, 103.79902259922984, 103.77333039131061,\n",
      "        102.46284084530903, 102.64810243215321, 102.6486015244451, 102.51908321756594,\n",
      "        103.08841024987291, 101.93988269596869, 102.2060644942997, 103.79600335781976,\n",
      "        103.77745901303042, 102.46178942377085, 102.64686816008972, 102.64750815163215,\n",
      "        102.517960424526, 103.08724024642771, 101.9390098484461, 102.19956313822217]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11671662512777013\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.576311036211172\n",
      "      mean_inference_ms: 10.834992703559188\n",
      "      mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  time_since_restore: 1106.426591873169\n",
      "  time_this_iter_s: 9.01081395149231\n",
      "  time_total_s: 1106.426591873169\n",
      "  timers:\n",
      "    learn_throughput: 13835.877\n",
      "    learn_time_ms: 361.668\n",
      "    synch_weights_time_ms: 7.736\n",
      "    training_iteration_time_ms: 8965.378\n",
      "  timestamp: 1662330436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600480\n",
      "  training_iteration: 120\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 605484\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 605484\n",
      "    num_agent_steps_trained: 605484\n",
      "    num_env_steps_sampled: 605484\n",
      "    num_env_steps_trained: 605484\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-27-25\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78085139889768\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 117\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5046327914733034e-37\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3228137493133545\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.440193611619179e-06\n",
      "          policy_loss: -0.00134878302924335\n",
      "          total_loss: -0.001249551773071289\n",
      "          vf_explained_var: 0.15246638655662537\n",
      "          vf_loss: 9.922750905388966e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 605484\n",
      "    num_agent_steps_trained: 605484\n",
      "    num_env_steps_sampled: 605484\n",
      "    num_env_steps_trained: 605484\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 605484\n",
      "  num_agent_steps_trained: 605484\n",
      "  num_env_steps_sampled: 605484\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 605484\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.24999999999999\n",
      "    ram_util_percent: 37.099999999999994\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11671662512777013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.576311036211172\n",
      "    mean_inference_ms: 10.834992703559188\n",
      "    mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78085139889768\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19875448442272, 103.79687406851185, 103.77135753401382, 102.46263616939216,\n",
      "        102.64753991526167, 102.64811418259029, 102.51874561896078, 103.0880851629313,\n",
      "        101.93975173061912, 102.20038822865497, 103.79625847951847, 103.77073504509276,\n",
      "        102.46204510235188, 102.6469371365317, 102.64756943425583, 102.51821540542055,\n",
      "        103.0875192390335, 101.93912031605522, 102.19980944928844, 103.79682060964069,\n",
      "        103.76936051660124, 102.4606451383536, 102.64557724996303, 102.64621198307653,\n",
      "        102.5169109108498, 103.08613407616481, 101.9436697277808, 102.19841162553591,\n",
      "        103.79614241723428, 103.77064000820155, 102.4618886587304, 102.646880882052,\n",
      "        102.64750124916208, 102.51810285984993, 103.08738079080894, 101.93897894630204,\n",
      "        102.19973215059784, 103.79732238534635, 103.76980686111476, 102.46114650815524,\n",
      "        102.64602645801769, 102.64668308160944, 102.51737039893044, 103.08662327344526,\n",
      "        101.93821971981195, 102.19889060005423, 103.79676209772195, 103.76928762724583,\n",
      "        102.4605848614072, 102.64549554932918, 102.64614825929499, 102.51686374187916,\n",
      "        103.08606478052307, 101.94362155570418, 102.19836109768045, 103.7968226749796,\n",
      "        103.77068989422041, 102.51700274093768, 102.64780285788922, 102.64806596239531,\n",
      "        102.5186024471645, 103.08811951417884, 101.94498233858293, 102.2605462926252,\n",
      "        103.7975513424916, 103.77024914103386, 102.4613630396782, 102.64646441571935,\n",
      "        102.64691369312487, 102.51755289982283, 103.0868646049081, 101.94444673847279,\n",
      "        102.19916200844175, 103.79696950272854, 103.77107623251682, 102.4607642551226,\n",
      "        102.64569098491162, 102.64631875318773, 102.5170526312697, 103.08624823474777,\n",
      "        101.9438032285913, 102.19854788482233, 103.79902259922984, 103.77333039131061,\n",
      "        102.46284084530903, 102.64810243215321, 102.6486015244451, 102.51908321756594,\n",
      "        103.08841024987291, 101.93988269596869, 102.2060644942997, 103.79600335781976,\n",
      "        103.77745901303042, 102.46178942377085, 102.64686816008972, 102.64750815163215,\n",
      "        102.517960424526, 103.08724024642771, 101.9390098484461, 102.19956313822217]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11671662512777013\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.576311036211172\n",
      "      mean_inference_ms: 10.834992703559188\n",
      "      mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  time_since_restore: 1115.362370967865\n",
      "  time_this_iter_s: 8.935779094696045\n",
      "  time_total_s: 1115.362370967865\n",
      "  timers:\n",
      "    learn_throughput: 13917.166\n",
      "    learn_time_ms: 359.556\n",
      "    synch_weights_time_ms: 7.682\n",
      "    training_iteration_time_ms: 8946.763\n",
      "  timestamp: 1662330445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 605484\n",
      "  training_iteration: 121\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 610488\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 610488\n",
      "    num_agent_steps_trained: 610488\n",
      "    num_env_steps_sampled: 610488\n",
      "    num_env_steps_trained: 610488\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-27-34\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78085139889768\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 117\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 7.523163957366517e-38\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3221203088760376\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.781299658887292e-07\n",
      "          policy_loss: 0.05292753502726555\n",
      "          total_loss: 0.05302507430315018\n",
      "          vf_explained_var: 0.13874974846839905\n",
      "          vf_loss: 9.753186895977706e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 610488\n",
      "    num_agent_steps_trained: 610488\n",
      "    num_env_steps_sampled: 610488\n",
      "    num_env_steps_trained: 610488\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 610488\n",
      "  num_agent_steps_trained: 610488\n",
      "  num_env_steps_sampled: 610488\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 610488\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 71.6076923076923\n",
      "    ram_util_percent: 37.23076923076922\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11671662512777013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.576311036211172\n",
      "    mean_inference_ms: 10.834992703559188\n",
      "    mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78085139889768\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19875448442272, 103.79687406851185, 103.77135753401382, 102.46263616939216,\n",
      "        102.64753991526167, 102.64811418259029, 102.51874561896078, 103.0880851629313,\n",
      "        101.93975173061912, 102.20038822865497, 103.79625847951847, 103.77073504509276,\n",
      "        102.46204510235188, 102.6469371365317, 102.64756943425583, 102.51821540542055,\n",
      "        103.0875192390335, 101.93912031605522, 102.19980944928844, 103.79682060964069,\n",
      "        103.76936051660124, 102.4606451383536, 102.64557724996303, 102.64621198307653,\n",
      "        102.5169109108498, 103.08613407616481, 101.9436697277808, 102.19841162553591,\n",
      "        103.79614241723428, 103.77064000820155, 102.4618886587304, 102.646880882052,\n",
      "        102.64750124916208, 102.51810285984993, 103.08738079080894, 101.93897894630204,\n",
      "        102.19973215059784, 103.79732238534635, 103.76980686111476, 102.46114650815524,\n",
      "        102.64602645801769, 102.64668308160944, 102.51737039893044, 103.08662327344526,\n",
      "        101.93821971981195, 102.19889060005423, 103.79676209772195, 103.76928762724583,\n",
      "        102.4605848614072, 102.64549554932918, 102.64614825929499, 102.51686374187916,\n",
      "        103.08606478052307, 101.94362155570418, 102.19836109768045, 103.7968226749796,\n",
      "        103.77068989422041, 102.51700274093768, 102.64780285788922, 102.64806596239531,\n",
      "        102.5186024471645, 103.08811951417884, 101.94498233858293, 102.2605462926252,\n",
      "        103.7975513424916, 103.77024914103386, 102.4613630396782, 102.64646441571935,\n",
      "        102.64691369312487, 102.51755289982283, 103.0868646049081, 101.94444673847279,\n",
      "        102.19916200844175, 103.79696950272854, 103.77107623251682, 102.4607642551226,\n",
      "        102.64569098491162, 102.64631875318773, 102.5170526312697, 103.08624823474777,\n",
      "        101.9438032285913, 102.19854788482233, 103.79902259922984, 103.77333039131061,\n",
      "        102.46284084530903, 102.64810243215321, 102.6486015244451, 102.51908321756594,\n",
      "        103.08841024987291, 101.93988269596869, 102.2060644942997, 103.79600335781976,\n",
      "        103.77745901303042, 102.46178942377085, 102.64686816008972, 102.64750815163215,\n",
      "        102.517960424526, 103.08724024642771, 101.9390098484461, 102.19956313822217]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11671662512777013\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.576311036211172\n",
      "      mean_inference_ms: 10.834992703559188\n",
      "      mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  time_since_restore: 1124.3932869434357\n",
      "  time_this_iter_s: 9.030915975570679\n",
      "  time_total_s: 1124.3932869434357\n",
      "  timers:\n",
      "    learn_throughput: 13819.978\n",
      "    learn_time_ms: 362.085\n",
      "    synch_weights_time_ms: 7.739\n",
      "    training_iteration_time_ms: 8950.274\n",
      "  timestamp: 1662330454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 610488\n",
      "  training_iteration: 122\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 615492\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 615492\n",
      "    num_agent_steps_trained: 615492\n",
      "    num_env_steps_sampled: 615492\n",
      "    num_env_steps_trained: 615492\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-27-43\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78085139889768\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 117\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.7615819786832586e-38\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3197476863861084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.449868472671369e-06\n",
      "          policy_loss: -0.012591248378157616\n",
      "          total_loss: -0.012495914474129677\n",
      "          vf_explained_var: 0.1110834926366806\n",
      "          vf_loss: 9.53320850385353e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 615492\n",
      "    num_agent_steps_trained: 615492\n",
      "    num_env_steps_sampled: 615492\n",
      "    num_env_steps_trained: 615492\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 615492\n",
      "  num_agent_steps_trained: 615492\n",
      "  num_env_steps_sampled: 615492\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 615492\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 63.13333333333333\n",
      "    ram_util_percent: 37.14166666666666\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11671662512777013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.576311036211172\n",
      "    mean_inference_ms: 10.834992703559188\n",
      "    mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78085139889768\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19875448442272, 103.79687406851185, 103.77135753401382, 102.46263616939216,\n",
      "        102.64753991526167, 102.64811418259029, 102.51874561896078, 103.0880851629313,\n",
      "        101.93975173061912, 102.20038822865497, 103.79625847951847, 103.77073504509276,\n",
      "        102.46204510235188, 102.6469371365317, 102.64756943425583, 102.51821540542055,\n",
      "        103.0875192390335, 101.93912031605522, 102.19980944928844, 103.79682060964069,\n",
      "        103.76936051660124, 102.4606451383536, 102.64557724996303, 102.64621198307653,\n",
      "        102.5169109108498, 103.08613407616481, 101.9436697277808, 102.19841162553591,\n",
      "        103.79614241723428, 103.77064000820155, 102.4618886587304, 102.646880882052,\n",
      "        102.64750124916208, 102.51810285984993, 103.08738079080894, 101.93897894630204,\n",
      "        102.19973215059784, 103.79732238534635, 103.76980686111476, 102.46114650815524,\n",
      "        102.64602645801769, 102.64668308160944, 102.51737039893044, 103.08662327344526,\n",
      "        101.93821971981195, 102.19889060005423, 103.79676209772195, 103.76928762724583,\n",
      "        102.4605848614072, 102.64549554932918, 102.64614825929499, 102.51686374187916,\n",
      "        103.08606478052307, 101.94362155570418, 102.19836109768045, 103.7968226749796,\n",
      "        103.77068989422041, 102.51700274093768, 102.64780285788922, 102.64806596239531,\n",
      "        102.5186024471645, 103.08811951417884, 101.94498233858293, 102.2605462926252,\n",
      "        103.7975513424916, 103.77024914103386, 102.4613630396782, 102.64646441571935,\n",
      "        102.64691369312487, 102.51755289982283, 103.0868646049081, 101.94444673847279,\n",
      "        102.19916200844175, 103.79696950272854, 103.77107623251682, 102.4607642551226,\n",
      "        102.64569098491162, 102.64631875318773, 102.5170526312697, 103.08624823474777,\n",
      "        101.9438032285913, 102.19854788482233, 103.79902259922984, 103.77333039131061,\n",
      "        102.46284084530903, 102.64810243215321, 102.6486015244451, 102.51908321756594,\n",
      "        103.08841024987291, 101.93988269596869, 102.2060644942997, 103.79600335781976,\n",
      "        103.77745901303042, 102.46178942377085, 102.64686816008972, 102.64750815163215,\n",
      "        102.517960424526, 103.08724024642771, 101.9390098484461, 102.19956313822217]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11671662512777013\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.576311036211172\n",
      "      mean_inference_ms: 10.834992703559188\n",
      "      mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  time_since_restore: 1133.5078887939453\n",
      "  time_this_iter_s: 9.114601850509644\n",
      "  time_total_s: 1133.5078887939453\n",
      "  timers:\n",
      "    learn_throughput: 13895.705\n",
      "    learn_time_ms: 360.111\n",
      "    synch_weights_time_ms: 7.706\n",
      "    training_iteration_time_ms: 8968.671\n",
      "  timestamp: 1662330463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615492\n",
      "  training_iteration: 123\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 620496\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 620496\n",
      "    num_agent_steps_trained: 620496\n",
      "    num_env_steps_sampled: 620496\n",
      "    num_env_steps_trained: 620496\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-27-52\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78085139889768\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 117\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.8807909893416293e-38\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3194808959960938\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.728569582672208e-07\n",
      "          policy_loss: 0.044510748237371445\n",
      "          total_loss: 0.04460570961236954\n",
      "          vf_explained_var: 0.13416323065757751\n",
      "          vf_loss: 9.496165148448199e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 620496\n",
      "    num_agent_steps_trained: 620496\n",
      "    num_env_steps_sampled: 620496\n",
      "    num_env_steps_trained: 620496\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 620496\n",
      "  num_agent_steps_trained: 620496\n",
      "  num_env_steps_sampled: 620496\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 620496\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.29230769230769\n",
      "    ram_util_percent: 37.130769230769225\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11671662512777013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.576311036211172\n",
      "    mean_inference_ms: 10.834992703559188\n",
      "    mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78085139889768\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19875448442272, 103.79687406851185, 103.77135753401382, 102.46263616939216,\n",
      "        102.64753991526167, 102.64811418259029, 102.51874561896078, 103.0880851629313,\n",
      "        101.93975173061912, 102.20038822865497, 103.79625847951847, 103.77073504509276,\n",
      "        102.46204510235188, 102.6469371365317, 102.64756943425583, 102.51821540542055,\n",
      "        103.0875192390335, 101.93912031605522, 102.19980944928844, 103.79682060964069,\n",
      "        103.76936051660124, 102.4606451383536, 102.64557724996303, 102.64621198307653,\n",
      "        102.5169109108498, 103.08613407616481, 101.9436697277808, 102.19841162553591,\n",
      "        103.79614241723428, 103.77064000820155, 102.4618886587304, 102.646880882052,\n",
      "        102.64750124916208, 102.51810285984993, 103.08738079080894, 101.93897894630204,\n",
      "        102.19973215059784, 103.79732238534635, 103.76980686111476, 102.46114650815524,\n",
      "        102.64602645801769, 102.64668308160944, 102.51737039893044, 103.08662327344526,\n",
      "        101.93821971981195, 102.19889060005423, 103.79676209772195, 103.76928762724583,\n",
      "        102.4605848614072, 102.64549554932918, 102.64614825929499, 102.51686374187916,\n",
      "        103.08606478052307, 101.94362155570418, 102.19836109768045, 103.7968226749796,\n",
      "        103.77068989422041, 102.51700274093768, 102.64780285788922, 102.64806596239531,\n",
      "        102.5186024471645, 103.08811951417884, 101.94498233858293, 102.2605462926252,\n",
      "        103.7975513424916, 103.77024914103386, 102.4613630396782, 102.64646441571935,\n",
      "        102.64691369312487, 102.51755289982283, 103.0868646049081, 101.94444673847279,\n",
      "        102.19916200844175, 103.79696950272854, 103.77107623251682, 102.4607642551226,\n",
      "        102.64569098491162, 102.64631875318773, 102.5170526312697, 103.08624823474777,\n",
      "        101.9438032285913, 102.19854788482233, 103.79902259922984, 103.77333039131061,\n",
      "        102.46284084530903, 102.64810243215321, 102.6486015244451, 102.51908321756594,\n",
      "        103.08841024987291, 101.93988269596869, 102.2060644942997, 103.79600335781976,\n",
      "        103.77745901303042, 102.46178942377085, 102.64686816008972, 102.64750815163215,\n",
      "        102.517960424526, 103.08724024642771, 101.9390098484461, 102.19956313822217]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11671662512777013\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.576311036211172\n",
      "      mean_inference_ms: 10.834992703559188\n",
      "      mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  time_since_restore: 1142.4552838802338\n",
      "  time_this_iter_s: 8.947395086288452\n",
      "  time_total_s: 1142.4552838802338\n",
      "  timers:\n",
      "    learn_throughput: 13871.453\n",
      "    learn_time_ms: 360.741\n",
      "    synch_weights_time_ms: 7.733\n",
      "    training_iteration_time_ms: 8972.713\n",
      "  timestamp: 1662330472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620496\n",
      "  training_iteration: 124\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 625500\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 625500\n",
      "    num_agent_steps_trained: 625500\n",
      "    num_env_steps_sampled: 625500\n",
      "    num_env_steps_trained: 625500\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-28-01\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78085139889768\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 117\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3173996210098267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.137635642109672e-06\n",
      "          policy_loss: 0.04309656471014023\n",
      "          total_loss: 0.043190546333789825\n",
      "          vf_explained_var: 0.2643967270851135\n",
      "          vf_loss: 9.398259135195985e-05\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 625500\n",
      "    num_agent_steps_trained: 625500\n",
      "    num_env_steps_sampled: 625500\n",
      "    num_env_steps_trained: 625500\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 625500\n",
      "  num_agent_steps_trained: 625500\n",
      "  num_env_steps_sampled: 625500\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 625500\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 72.0\n",
      "    ram_util_percent: 36.824999999999996\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11671662512777013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.576311036211172\n",
      "    mean_inference_ms: 10.834992703559188\n",
      "    mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78085139889768\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19875448442272, 103.79687406851185, 103.77135753401382, 102.46263616939216,\n",
      "        102.64753991526167, 102.64811418259029, 102.51874561896078, 103.0880851629313,\n",
      "        101.93975173061912, 102.20038822865497, 103.79625847951847, 103.77073504509276,\n",
      "        102.46204510235188, 102.6469371365317, 102.64756943425583, 102.51821540542055,\n",
      "        103.0875192390335, 101.93912031605522, 102.19980944928844, 103.79682060964069,\n",
      "        103.76936051660124, 102.4606451383536, 102.64557724996303, 102.64621198307653,\n",
      "        102.5169109108498, 103.08613407616481, 101.9436697277808, 102.19841162553591,\n",
      "        103.79614241723428, 103.77064000820155, 102.4618886587304, 102.646880882052,\n",
      "        102.64750124916208, 102.51810285984993, 103.08738079080894, 101.93897894630204,\n",
      "        102.19973215059784, 103.79732238534635, 103.76980686111476, 102.46114650815524,\n",
      "        102.64602645801769, 102.64668308160944, 102.51737039893044, 103.08662327344526,\n",
      "        101.93821971981195, 102.19889060005423, 103.79676209772195, 103.76928762724583,\n",
      "        102.4605848614072, 102.64549554932918, 102.64614825929499, 102.51686374187916,\n",
      "        103.08606478052307, 101.94362155570418, 102.19836109768045, 103.7968226749796,\n",
      "        103.77068989422041, 102.51700274093768, 102.64780285788922, 102.64806596239531,\n",
      "        102.5186024471645, 103.08811951417884, 101.94498233858293, 102.2605462926252,\n",
      "        103.7975513424916, 103.77024914103386, 102.4613630396782, 102.64646441571935,\n",
      "        102.64691369312487, 102.51755289982283, 103.0868646049081, 101.94444673847279,\n",
      "        102.19916200844175, 103.79696950272854, 103.77107623251682, 102.4607642551226,\n",
      "        102.64569098491162, 102.64631875318773, 102.5170526312697, 103.08624823474777,\n",
      "        101.9438032285913, 102.19854788482233, 103.79902259922984, 103.77333039131061,\n",
      "        102.46284084530903, 102.64810243215321, 102.6486015244451, 102.51908321756594,\n",
      "        103.08841024987291, 101.93988269596869, 102.2060644942997, 103.79600335781976,\n",
      "        103.77745901303042, 102.46178942377085, 102.64686816008972, 102.64750815163215,\n",
      "        102.517960424526, 103.08724024642771, 101.9390098484461, 102.19956313822217]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11671662512777013\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.576311036211172\n",
      "      mean_inference_ms: 10.834992703559188\n",
      "      mean_raw_obs_processing_ms: 0.20806534686564826\n",
      "  time_since_restore: 1151.3212840557098\n",
      "  time_this_iter_s: 8.866000175476074\n",
      "  time_total_s: 1151.3212840557098\n",
      "  timers:\n",
      "    learn_throughput: 13990.342\n",
      "    learn_time_ms: 357.675\n",
      "    synch_weights_time_ms: 7.685\n",
      "    training_iteration_time_ms: 8954.394\n",
      "  timestamp: 1662330481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 625500\n",
      "  training_iteration: 125\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 630504\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 630504\n",
      "    num_agent_steps_trained: 630504\n",
      "    num_env_steps_sampled: 630504\n",
      "    num_env_steps_trained: 630504\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-28-10\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78075484189979\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 126\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.327752709388733\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.63435037797899e-06\n",
      "          policy_loss: 0.007514937315136194\n",
      "          total_loss: 0.9951915740966797\n",
      "          vf_explained_var: 0.09886007755994797\n",
      "          vf_loss: 0.9876765012741089\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 630504\n",
      "    num_agent_steps_trained: 630504\n",
      "    num_env_steps_sampled: 630504\n",
      "    num_env_steps_trained: 630504\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 630504\n",
      "  num_agent_steps_trained: 630504\n",
      "  num_env_steps_sampled: 630504\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 630504\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 72.44615384615385\n",
      "    ram_util_percent: 37.069230769230764\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11664928014770717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.562925009898623\n",
      "    mean_inference_ms: 10.81884408506865\n",
      "    mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78075484189979\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.20038822865497, 103.79625847951847, 103.77073504509276, 102.46204510235188,\n",
      "        102.6469371365317, 102.64756943425583, 102.51821540542055, 103.0875192390335,\n",
      "        101.93912031605522, 102.19980944928844, 103.79682060964069, 103.76936051660124,\n",
      "        102.4606451383536, 102.64557724996303, 102.64621198307653, 102.5169109108498,\n",
      "        103.08613407616481, 101.9436697277808, 102.19841162553591, 103.79614241723428,\n",
      "        103.77064000820155, 102.4618886587304, 102.646880882052, 102.64750124916208,\n",
      "        102.51810285984993, 103.08738079080894, 101.93897894630204, 102.19973215059784,\n",
      "        103.79732238534635, 103.76980686111476, 102.46114650815524, 102.64602645801769,\n",
      "        102.64668308160944, 102.51737039893044, 103.08662327344526, 101.93821971981195,\n",
      "        102.19889060005423, 103.79676209772195, 103.76928762724583, 102.4605848614072,\n",
      "        102.64549554932918, 102.64614825929499, 102.51686374187916, 103.08606478052307,\n",
      "        101.94362155570418, 102.19836109768045, 103.7968226749796, 103.77068989422041,\n",
      "        102.51700274093768, 102.64780285788922, 102.64806596239531, 102.5186024471645,\n",
      "        103.08811951417884, 101.94498233858293, 102.2605462926252, 103.7975513424916,\n",
      "        103.77024914103386, 102.4613630396782, 102.64646441571935, 102.64691369312487,\n",
      "        102.51755289982283, 103.0868646049081, 101.94444673847279, 102.19916200844175,\n",
      "        103.79696950272854, 103.77107623251682, 102.4607642551226, 102.64569098491162,\n",
      "        102.64631875318773, 102.5170526312697, 103.08624823474777, 101.9438032285913,\n",
      "        102.19854788482233, 103.79902259922984, 103.77333039131061, 102.46284084530903,\n",
      "        102.64810243215321, 102.6486015244451, 102.51908321756594, 103.08841024987291,\n",
      "        101.93988269596869, 102.2060644942997, 103.79600335781976, 103.77745901303042,\n",
      "        102.46178942377085, 102.64686816008972, 102.64750815163215, 102.517960424526,\n",
      "        103.08724024642771, 101.9390098484461, 102.19956313822217, 103.79490998874452,\n",
      "        103.76939198188575, 102.46069190095136, 102.64560728616925, 102.64626063030612,\n",
      "        102.51695863948956, 103.08617925879146, 101.94372626370294, 102.19847721687437]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11664928014770717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.562925009898623\n",
      "      mean_inference_ms: 10.81884408506865\n",
      "      mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  time_since_restore: 1160.3162591457367\n",
      "  time_this_iter_s: 8.994975090026855\n",
      "  time_total_s: 1160.3162591457367\n",
      "  timers:\n",
      "    learn_throughput: 13948.058\n",
      "    learn_time_ms: 358.76\n",
      "    synch_weights_time_ms: 9.12\n",
      "    training_iteration_time_ms: 8958.911\n",
      "  timestamp: 1662330490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 630504\n",
      "  training_iteration: 126\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=7010.70.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=7010.50.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 635508\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 635508\n",
      "    num_agent_steps_trained: 635508\n",
      "    num_env_steps_sampled: 635508\n",
      "    num_env_steps_trained: 635508\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-28-19\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78075484189979\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 126\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3473856449127197\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.473773176665418e-05\n",
      "          policy_loss: 0.0671953409910202\n",
      "          total_loss: 4.490694999694824\n",
      "          vf_explained_var: -0.0014346539974212646\n",
      "          vf_loss: 4.42349910736084\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 635508\n",
      "    num_agent_steps_trained: 635508\n",
      "    num_env_steps_sampled: 635508\n",
      "    num_env_steps_trained: 635508\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 635508\n",
      "  num_agent_steps_trained: 635508\n",
      "  num_env_steps_sampled: 635508\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 635508\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 80.61666666666667\n",
      "    ram_util_percent: 37.099999999999994\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11664928014770717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.562925009898623\n",
      "    mean_inference_ms: 10.81884408506865\n",
      "    mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78075484189979\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.20038822865497, 103.79625847951847, 103.77073504509276, 102.46204510235188,\n",
      "        102.6469371365317, 102.64756943425583, 102.51821540542055, 103.0875192390335,\n",
      "        101.93912031605522, 102.19980944928844, 103.79682060964069, 103.76936051660124,\n",
      "        102.4606451383536, 102.64557724996303, 102.64621198307653, 102.5169109108498,\n",
      "        103.08613407616481, 101.9436697277808, 102.19841162553591, 103.79614241723428,\n",
      "        103.77064000820155, 102.4618886587304, 102.646880882052, 102.64750124916208,\n",
      "        102.51810285984993, 103.08738079080894, 101.93897894630204, 102.19973215059784,\n",
      "        103.79732238534635, 103.76980686111476, 102.46114650815524, 102.64602645801769,\n",
      "        102.64668308160944, 102.51737039893044, 103.08662327344526, 101.93821971981195,\n",
      "        102.19889060005423, 103.79676209772195, 103.76928762724583, 102.4605848614072,\n",
      "        102.64549554932918, 102.64614825929499, 102.51686374187916, 103.08606478052307,\n",
      "        101.94362155570418, 102.19836109768045, 103.7968226749796, 103.77068989422041,\n",
      "        102.51700274093768, 102.64780285788922, 102.64806596239531, 102.5186024471645,\n",
      "        103.08811951417884, 101.94498233858293, 102.2605462926252, 103.7975513424916,\n",
      "        103.77024914103386, 102.4613630396782, 102.64646441571935, 102.64691369312487,\n",
      "        102.51755289982283, 103.0868646049081, 101.94444673847279, 102.19916200844175,\n",
      "        103.79696950272854, 103.77107623251682, 102.4607642551226, 102.64569098491162,\n",
      "        102.64631875318773, 102.5170526312697, 103.08624823474777, 101.9438032285913,\n",
      "        102.19854788482233, 103.79902259922984, 103.77333039131061, 102.46284084530903,\n",
      "        102.64810243215321, 102.6486015244451, 102.51908321756594, 103.08841024987291,\n",
      "        101.93988269596869, 102.2060644942997, 103.79600335781976, 103.77745901303042,\n",
      "        102.46178942377085, 102.64686816008972, 102.64750815163215, 102.517960424526,\n",
      "        103.08724024642771, 101.9390098484461, 102.19956313822217, 103.79490998874452,\n",
      "        103.76939198188575, 102.46069190095136, 102.64560728616925, 102.64626063030612,\n",
      "        102.51695863948956, 103.08617925879146, 101.94372626370294, 102.19847721687437]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11664928014770717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.562925009898623\n",
      "      mean_inference_ms: 10.81884408506865\n",
      "      mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  time_since_restore: 1169.2383172512054\n",
      "  time_this_iter_s: 8.92205810546875\n",
      "  time_total_s: 1169.2383172512054\n",
      "  timers:\n",
      "    learn_throughput: 13945.237\n",
      "    learn_time_ms: 358.832\n",
      "    synch_weights_time_ms: 9.095\n",
      "    training_iteration_time_ms: 8966.64\n",
      "  timestamp: 1662330499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 635508\n",
      "  training_iteration: 127\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 640512\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 640512\n",
      "    num_agent_steps_trained: 640512\n",
      "    num_env_steps_sampled: 640512\n",
      "    num_env_steps_trained: 640512\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-28-28\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78075484189979\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 126\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2999942302703857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.9969531826500315e-06\n",
      "          policy_loss: -0.21003088355064392\n",
      "          total_loss: -0.20971262454986572\n",
      "          vf_explained_var: 0.10397730022668839\n",
      "          vf_loss: 0.000318242673529312\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 640512\n",
      "    num_agent_steps_trained: 640512\n",
      "    num_env_steps_sampled: 640512\n",
      "    num_env_steps_trained: 640512\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 640512\n",
      "  num_agent_steps_trained: 640512\n",
      "  num_env_steps_sampled: 640512\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 640512\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.0\n",
      "    ram_util_percent: 37.10833333333333\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11664928014770717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.562925009898623\n",
      "    mean_inference_ms: 10.81884408506865\n",
      "    mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78075484189979\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.20038822865497, 103.79625847951847, 103.77073504509276, 102.46204510235188,\n",
      "        102.6469371365317, 102.64756943425583, 102.51821540542055, 103.0875192390335,\n",
      "        101.93912031605522, 102.19980944928844, 103.79682060964069, 103.76936051660124,\n",
      "        102.4606451383536, 102.64557724996303, 102.64621198307653, 102.5169109108498,\n",
      "        103.08613407616481, 101.9436697277808, 102.19841162553591, 103.79614241723428,\n",
      "        103.77064000820155, 102.4618886587304, 102.646880882052, 102.64750124916208,\n",
      "        102.51810285984993, 103.08738079080894, 101.93897894630204, 102.19973215059784,\n",
      "        103.79732238534635, 103.76980686111476, 102.46114650815524, 102.64602645801769,\n",
      "        102.64668308160944, 102.51737039893044, 103.08662327344526, 101.93821971981195,\n",
      "        102.19889060005423, 103.79676209772195, 103.76928762724583, 102.4605848614072,\n",
      "        102.64549554932918, 102.64614825929499, 102.51686374187916, 103.08606478052307,\n",
      "        101.94362155570418, 102.19836109768045, 103.7968226749796, 103.77068989422041,\n",
      "        102.51700274093768, 102.64780285788922, 102.64806596239531, 102.5186024471645,\n",
      "        103.08811951417884, 101.94498233858293, 102.2605462926252, 103.7975513424916,\n",
      "        103.77024914103386, 102.4613630396782, 102.64646441571935, 102.64691369312487,\n",
      "        102.51755289982283, 103.0868646049081, 101.94444673847279, 102.19916200844175,\n",
      "        103.79696950272854, 103.77107623251682, 102.4607642551226, 102.64569098491162,\n",
      "        102.64631875318773, 102.5170526312697, 103.08624823474777, 101.9438032285913,\n",
      "        102.19854788482233, 103.79902259922984, 103.77333039131061, 102.46284084530903,\n",
      "        102.64810243215321, 102.6486015244451, 102.51908321756594, 103.08841024987291,\n",
      "        101.93988269596869, 102.2060644942997, 103.79600335781976, 103.77745901303042,\n",
      "        102.46178942377085, 102.64686816008972, 102.64750815163215, 102.517960424526,\n",
      "        103.08724024642771, 101.9390098484461, 102.19956313822217, 103.79490998874452,\n",
      "        103.76939198188575, 102.46069190095136, 102.64560728616925, 102.64626063030612,\n",
      "        102.51695863948956, 103.08617925879146, 101.94372626370294, 102.19847721687437]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11664928014770717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.562925009898623\n",
      "      mean_inference_ms: 10.81884408506865\n",
      "      mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  time_since_restore: 1178.1259672641754\n",
      "  time_this_iter_s: 8.88765001296997\n",
      "  time_total_s: 1178.1259672641754\n",
      "  timers:\n",
      "    learn_throughput: 14011.325\n",
      "    learn_time_ms: 357.14\n",
      "    synch_weights_time_ms: 9.057\n",
      "    training_iteration_time_ms: 8964.98\n",
      "  timestamp: 1662330508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640512\n",
      "  training_iteration: 128\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 645516\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 645516\n",
      "    num_agent_steps_trained: 645516\n",
      "    num_env_steps_sampled: 645516\n",
      "    num_env_steps_trained: 645516\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-28-37\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78075484189979\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 126\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3043447732925415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.0936405487882439e-05\n",
      "          policy_loss: 0.08586958795785904\n",
      "          total_loss: 0.08599600195884705\n",
      "          vf_explained_var: 0.1897093951702118\n",
      "          vf_loss: 0.00012641269131563604\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 645516\n",
      "    num_agent_steps_trained: 645516\n",
      "    num_env_steps_sampled: 645516\n",
      "    num_env_steps_trained: 645516\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 645516\n",
      "  num_agent_steps_trained: 645516\n",
      "  num_env_steps_sampled: 645516\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 645516\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.22307692307693\n",
      "    ram_util_percent: 36.94615384615384\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11664928014770717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.562925009898623\n",
      "    mean_inference_ms: 10.81884408506865\n",
      "    mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78075484189979\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.20038822865497, 103.79625847951847, 103.77073504509276, 102.46204510235188,\n",
      "        102.6469371365317, 102.64756943425583, 102.51821540542055, 103.0875192390335,\n",
      "        101.93912031605522, 102.19980944928844, 103.79682060964069, 103.76936051660124,\n",
      "        102.4606451383536, 102.64557724996303, 102.64621198307653, 102.5169109108498,\n",
      "        103.08613407616481, 101.9436697277808, 102.19841162553591, 103.79614241723428,\n",
      "        103.77064000820155, 102.4618886587304, 102.646880882052, 102.64750124916208,\n",
      "        102.51810285984993, 103.08738079080894, 101.93897894630204, 102.19973215059784,\n",
      "        103.79732238534635, 103.76980686111476, 102.46114650815524, 102.64602645801769,\n",
      "        102.64668308160944, 102.51737039893044, 103.08662327344526, 101.93821971981195,\n",
      "        102.19889060005423, 103.79676209772195, 103.76928762724583, 102.4605848614072,\n",
      "        102.64549554932918, 102.64614825929499, 102.51686374187916, 103.08606478052307,\n",
      "        101.94362155570418, 102.19836109768045, 103.7968226749796, 103.77068989422041,\n",
      "        102.51700274093768, 102.64780285788922, 102.64806596239531, 102.5186024471645,\n",
      "        103.08811951417884, 101.94498233858293, 102.2605462926252, 103.7975513424916,\n",
      "        103.77024914103386, 102.4613630396782, 102.64646441571935, 102.64691369312487,\n",
      "        102.51755289982283, 103.0868646049081, 101.94444673847279, 102.19916200844175,\n",
      "        103.79696950272854, 103.77107623251682, 102.4607642551226, 102.64569098491162,\n",
      "        102.64631875318773, 102.5170526312697, 103.08624823474777, 101.9438032285913,\n",
      "        102.19854788482233, 103.79902259922984, 103.77333039131061, 102.46284084530903,\n",
      "        102.64810243215321, 102.6486015244451, 102.51908321756594, 103.08841024987291,\n",
      "        101.93988269596869, 102.2060644942997, 103.79600335781976, 103.77745901303042,\n",
      "        102.46178942377085, 102.64686816008972, 102.64750815163215, 102.517960424526,\n",
      "        103.08724024642771, 101.9390098484461, 102.19956313822217, 103.79490998874452,\n",
      "        103.76939198188575, 102.46069190095136, 102.64560728616925, 102.64626063030612,\n",
      "        102.51695863948956, 103.08617925879146, 101.94372626370294, 102.19847721687437]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11664928014770717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.562925009898623\n",
      "      mean_inference_ms: 10.81884408506865\n",
      "      mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  time_since_restore: 1187.349181175232\n",
      "  time_this_iter_s: 9.223213911056519\n",
      "  time_total_s: 1187.349181175232\n",
      "  timers:\n",
      "    learn_throughput: 14038.644\n",
      "    learn_time_ms: 356.445\n",
      "    synch_weights_time_ms: 9.023\n",
      "    training_iteration_time_ms: 8986.058\n",
      "  timestamp: 1662330517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 645516\n",
      "  training_iteration: 129\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 650520\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 650520\n",
      "    num_agent_steps_trained: 650520\n",
      "    num_env_steps_sampled: 650520\n",
      "    num_env_steps_trained: 650520\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-28-46\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78075484189979\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 126\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3031021356582642\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 8.011198588064872e-06\n",
      "          policy_loss: 0.06878890842199326\n",
      "          total_loss: 0.06890201568603516\n",
      "          vf_explained_var: 0.0378979817032814\n",
      "          vf_loss: 0.00011311184061923996\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 650520\n",
      "    num_agent_steps_trained: 650520\n",
      "    num_env_steps_sampled: 650520\n",
      "    num_env_steps_trained: 650520\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 650520\n",
      "  num_agent_steps_trained: 650520\n",
      "  num_env_steps_sampled: 650520\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 650520\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 71.19999999999999\n",
      "    ram_util_percent: 36.99999999999999\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11664928014770717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.562925009898623\n",
      "    mean_inference_ms: 10.81884408506865\n",
      "    mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78075484189979\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.20038822865497, 103.79625847951847, 103.77073504509276, 102.46204510235188,\n",
      "        102.6469371365317, 102.64756943425583, 102.51821540542055, 103.0875192390335,\n",
      "        101.93912031605522, 102.19980944928844, 103.79682060964069, 103.76936051660124,\n",
      "        102.4606451383536, 102.64557724996303, 102.64621198307653, 102.5169109108498,\n",
      "        103.08613407616481, 101.9436697277808, 102.19841162553591, 103.79614241723428,\n",
      "        103.77064000820155, 102.4618886587304, 102.646880882052, 102.64750124916208,\n",
      "        102.51810285984993, 103.08738079080894, 101.93897894630204, 102.19973215059784,\n",
      "        103.79732238534635, 103.76980686111476, 102.46114650815524, 102.64602645801769,\n",
      "        102.64668308160944, 102.51737039893044, 103.08662327344526, 101.93821971981195,\n",
      "        102.19889060005423, 103.79676209772195, 103.76928762724583, 102.4605848614072,\n",
      "        102.64549554932918, 102.64614825929499, 102.51686374187916, 103.08606478052307,\n",
      "        101.94362155570418, 102.19836109768045, 103.7968226749796, 103.77068989422041,\n",
      "        102.51700274093768, 102.64780285788922, 102.64806596239531, 102.5186024471645,\n",
      "        103.08811951417884, 101.94498233858293, 102.2605462926252, 103.7975513424916,\n",
      "        103.77024914103386, 102.4613630396782, 102.64646441571935, 102.64691369312487,\n",
      "        102.51755289982283, 103.0868646049081, 101.94444673847279, 102.19916200844175,\n",
      "        103.79696950272854, 103.77107623251682, 102.4607642551226, 102.64569098491162,\n",
      "        102.64631875318773, 102.5170526312697, 103.08624823474777, 101.9438032285913,\n",
      "        102.19854788482233, 103.79902259922984, 103.77333039131061, 102.46284084530903,\n",
      "        102.64810243215321, 102.6486015244451, 102.51908321756594, 103.08841024987291,\n",
      "        101.93988269596869, 102.2060644942997, 103.79600335781976, 103.77745901303042,\n",
      "        102.46178942377085, 102.64686816008972, 102.64750815163215, 102.517960424526,\n",
      "        103.08724024642771, 101.9390098484461, 102.19956313822217, 103.79490998874452,\n",
      "        103.76939198188575, 102.46069190095136, 102.64560728616925, 102.64626063030612,\n",
      "        102.51695863948956, 103.08617925879146, 101.94372626370294, 102.19847721687437]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11664928014770717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.562925009898623\n",
      "      mean_inference_ms: 10.81884408506865\n",
      "      mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  time_since_restore: 1196.3816301822662\n",
      "  time_this_iter_s: 9.032449007034302\n",
      "  time_total_s: 1196.3816301822662\n",
      "  timers:\n",
      "    learn_throughput: 14155.006\n",
      "    learn_time_ms: 353.515\n",
      "    synch_weights_time_ms: 9.016\n",
      "    training_iteration_time_ms: 8988.23\n",
      "  timestamp: 1662330526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 650520\n",
      "  training_iteration: 130\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 655524\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 655524\n",
      "    num_agent_steps_trained: 655524\n",
      "    num_env_steps_sampled: 655524\n",
      "    num_env_steps_trained: 655524\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-28-55\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78075484189979\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 126\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3001335859298706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.897264837680268e-06\n",
      "          policy_loss: 0.07145343720912933\n",
      "          total_loss: 0.07156486809253693\n",
      "          vf_explained_var: 0.22393798828125\n",
      "          vf_loss: 0.00011144114978378639\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 655524\n",
      "    num_agent_steps_trained: 655524\n",
      "    num_env_steps_sampled: 655524\n",
      "    num_env_steps_trained: 655524\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 655524\n",
      "  num_agent_steps_trained: 655524\n",
      "  num_env_steps_sampled: 655524\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 655524\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 78.74166666666666\n",
      "    ram_util_percent: 37.05\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11664928014770717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.562925009898623\n",
      "    mean_inference_ms: 10.81884408506865\n",
      "    mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78075484189979\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.20038822865497, 103.79625847951847, 103.77073504509276, 102.46204510235188,\n",
      "        102.6469371365317, 102.64756943425583, 102.51821540542055, 103.0875192390335,\n",
      "        101.93912031605522, 102.19980944928844, 103.79682060964069, 103.76936051660124,\n",
      "        102.4606451383536, 102.64557724996303, 102.64621198307653, 102.5169109108498,\n",
      "        103.08613407616481, 101.9436697277808, 102.19841162553591, 103.79614241723428,\n",
      "        103.77064000820155, 102.4618886587304, 102.646880882052, 102.64750124916208,\n",
      "        102.51810285984993, 103.08738079080894, 101.93897894630204, 102.19973215059784,\n",
      "        103.79732238534635, 103.76980686111476, 102.46114650815524, 102.64602645801769,\n",
      "        102.64668308160944, 102.51737039893044, 103.08662327344526, 101.93821971981195,\n",
      "        102.19889060005423, 103.79676209772195, 103.76928762724583, 102.4605848614072,\n",
      "        102.64549554932918, 102.64614825929499, 102.51686374187916, 103.08606478052307,\n",
      "        101.94362155570418, 102.19836109768045, 103.7968226749796, 103.77068989422041,\n",
      "        102.51700274093768, 102.64780285788922, 102.64806596239531, 102.5186024471645,\n",
      "        103.08811951417884, 101.94498233858293, 102.2605462926252, 103.7975513424916,\n",
      "        103.77024914103386, 102.4613630396782, 102.64646441571935, 102.64691369312487,\n",
      "        102.51755289982283, 103.0868646049081, 101.94444673847279, 102.19916200844175,\n",
      "        103.79696950272854, 103.77107623251682, 102.4607642551226, 102.64569098491162,\n",
      "        102.64631875318773, 102.5170526312697, 103.08624823474777, 101.9438032285913,\n",
      "        102.19854788482233, 103.79902259922984, 103.77333039131061, 102.46284084530903,\n",
      "        102.64810243215321, 102.6486015244451, 102.51908321756594, 103.08841024987291,\n",
      "        101.93988269596869, 102.2060644942997, 103.79600335781976, 103.77745901303042,\n",
      "        102.46178942377085, 102.64686816008972, 102.64750815163215, 102.517960424526,\n",
      "        103.08724024642771, 101.9390098484461, 102.19956313822217, 103.79490998874452,\n",
      "        103.76939198188575, 102.46069190095136, 102.64560728616925, 102.64626063030612,\n",
      "        102.51695863948956, 103.08617925879146, 101.94372626370294, 102.19847721687437]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11664928014770717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.562925009898623\n",
      "      mean_inference_ms: 10.81884408506865\n",
      "      mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  time_since_restore: 1205.4273948669434\n",
      "  time_this_iter_s: 9.045764684677124\n",
      "  time_total_s: 1205.4273948669434\n",
      "  timers:\n",
      "    learn_throughput: 14143.008\n",
      "    learn_time_ms: 353.814\n",
      "    synch_weights_time_ms: 9.048\n",
      "    training_iteration_time_ms: 8999.193\n",
      "  timestamp: 1662330535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 655524\n",
      "  training_iteration: 131\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 660528\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 660528\n",
      "    num_agent_steps_trained: 660528\n",
      "    num_env_steps_sampled: 660528\n",
      "    num_env_steps_trained: 660528\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-29-04\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78075484189979\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 126\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2966525554656982\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 4.639930466510123e-06\n",
      "          policy_loss: 0.012892154976725578\n",
      "          total_loss: 0.01300014741718769\n",
      "          vf_explained_var: 0.2894945740699768\n",
      "          vf_loss: 0.00010799315350595862\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 660528\n",
      "    num_agent_steps_trained: 660528\n",
      "    num_env_steps_sampled: 660528\n",
      "    num_env_steps_trained: 660528\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 660528\n",
      "  num_agent_steps_trained: 660528\n",
      "  num_env_steps_sampled: 660528\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 660528\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.00833333333334\n",
      "    ram_util_percent: 37.10833333333333\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11664928014770717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.562925009898623\n",
      "    mean_inference_ms: 10.81884408506865\n",
      "    mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78075484189979\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.20038822865497, 103.79625847951847, 103.77073504509276, 102.46204510235188,\n",
      "        102.6469371365317, 102.64756943425583, 102.51821540542055, 103.0875192390335,\n",
      "        101.93912031605522, 102.19980944928844, 103.79682060964069, 103.76936051660124,\n",
      "        102.4606451383536, 102.64557724996303, 102.64621198307653, 102.5169109108498,\n",
      "        103.08613407616481, 101.9436697277808, 102.19841162553591, 103.79614241723428,\n",
      "        103.77064000820155, 102.4618886587304, 102.646880882052, 102.64750124916208,\n",
      "        102.51810285984993, 103.08738079080894, 101.93897894630204, 102.19973215059784,\n",
      "        103.79732238534635, 103.76980686111476, 102.46114650815524, 102.64602645801769,\n",
      "        102.64668308160944, 102.51737039893044, 103.08662327344526, 101.93821971981195,\n",
      "        102.19889060005423, 103.79676209772195, 103.76928762724583, 102.4605848614072,\n",
      "        102.64549554932918, 102.64614825929499, 102.51686374187916, 103.08606478052307,\n",
      "        101.94362155570418, 102.19836109768045, 103.7968226749796, 103.77068989422041,\n",
      "        102.51700274093768, 102.64780285788922, 102.64806596239531, 102.5186024471645,\n",
      "        103.08811951417884, 101.94498233858293, 102.2605462926252, 103.7975513424916,\n",
      "        103.77024914103386, 102.4613630396782, 102.64646441571935, 102.64691369312487,\n",
      "        102.51755289982283, 103.0868646049081, 101.94444673847279, 102.19916200844175,\n",
      "        103.79696950272854, 103.77107623251682, 102.4607642551226, 102.64569098491162,\n",
      "        102.64631875318773, 102.5170526312697, 103.08624823474777, 101.9438032285913,\n",
      "        102.19854788482233, 103.79902259922984, 103.77333039131061, 102.46284084530903,\n",
      "        102.64810243215321, 102.6486015244451, 102.51908321756594, 103.08841024987291,\n",
      "        101.93988269596869, 102.2060644942997, 103.79600335781976, 103.77745901303042,\n",
      "        102.46178942377085, 102.64686816008972, 102.64750815163215, 102.517960424526,\n",
      "        103.08724024642771, 101.9390098484461, 102.19956313822217, 103.79490998874452,\n",
      "        103.76939198188575, 102.46069190095136, 102.64560728616925, 102.64626063030612,\n",
      "        102.51695863948956, 103.08617925879146, 101.94372626370294, 102.19847721687437]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11664928014770717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.562925009898623\n",
      "      mean_inference_ms: 10.81884408506865\n",
      "      mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  time_since_restore: 1214.2914357185364\n",
      "  time_this_iter_s: 8.864040851593018\n",
      "  time_total_s: 1214.2914357185364\n",
      "  timers:\n",
      "    learn_throughput: 14246.481\n",
      "    learn_time_ms: 351.245\n",
      "    synch_weights_time_ms: 8.997\n",
      "    training_iteration_time_ms: 8982.519\n",
      "  timestamp: 1662330544\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660528\n",
      "  training_iteration: 132\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 665532\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 665532\n",
      "    num_agent_steps_trained: 665532\n",
      "    num_env_steps_sampled: 665532\n",
      "    num_env_steps_trained: 665532\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-29-13\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78075484189979\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 126\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2950818538665771\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.2782401174481492e-06\n",
      "          policy_loss: 0.037184759974479675\n",
      "          total_loss: 0.03729239106178284\n",
      "          vf_explained_var: 0.0520976297557354\n",
      "          vf_loss: 0.00010763745376607403\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 665532\n",
      "    num_agent_steps_trained: 665532\n",
      "    num_env_steps_sampled: 665532\n",
      "    num_env_steps_trained: 665532\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 665532\n",
      "  num_agent_steps_trained: 665532\n",
      "  num_env_steps_sampled: 665532\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 665532\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.76923076923077\n",
      "    ram_util_percent: 37.153846153846146\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11664928014770717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.562925009898623\n",
      "    mean_inference_ms: 10.81884408506865\n",
      "    mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78075484189979\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.20038822865497, 103.79625847951847, 103.77073504509276, 102.46204510235188,\n",
      "        102.6469371365317, 102.64756943425583, 102.51821540542055, 103.0875192390335,\n",
      "        101.93912031605522, 102.19980944928844, 103.79682060964069, 103.76936051660124,\n",
      "        102.4606451383536, 102.64557724996303, 102.64621198307653, 102.5169109108498,\n",
      "        103.08613407616481, 101.9436697277808, 102.19841162553591, 103.79614241723428,\n",
      "        103.77064000820155, 102.4618886587304, 102.646880882052, 102.64750124916208,\n",
      "        102.51810285984993, 103.08738079080894, 101.93897894630204, 102.19973215059784,\n",
      "        103.79732238534635, 103.76980686111476, 102.46114650815524, 102.64602645801769,\n",
      "        102.64668308160944, 102.51737039893044, 103.08662327344526, 101.93821971981195,\n",
      "        102.19889060005423, 103.79676209772195, 103.76928762724583, 102.4605848614072,\n",
      "        102.64549554932918, 102.64614825929499, 102.51686374187916, 103.08606478052307,\n",
      "        101.94362155570418, 102.19836109768045, 103.7968226749796, 103.77068989422041,\n",
      "        102.51700274093768, 102.64780285788922, 102.64806596239531, 102.5186024471645,\n",
      "        103.08811951417884, 101.94498233858293, 102.2605462926252, 103.7975513424916,\n",
      "        103.77024914103386, 102.4613630396782, 102.64646441571935, 102.64691369312487,\n",
      "        102.51755289982283, 103.0868646049081, 101.94444673847279, 102.19916200844175,\n",
      "        103.79696950272854, 103.77107623251682, 102.4607642551226, 102.64569098491162,\n",
      "        102.64631875318773, 102.5170526312697, 103.08624823474777, 101.9438032285913,\n",
      "        102.19854788482233, 103.79902259922984, 103.77333039131061, 102.46284084530903,\n",
      "        102.64810243215321, 102.6486015244451, 102.51908321756594, 103.08841024987291,\n",
      "        101.93988269596869, 102.2060644942997, 103.79600335781976, 103.77745901303042,\n",
      "        102.46178942377085, 102.64686816008972, 102.64750815163215, 102.517960424526,\n",
      "        103.08724024642771, 101.9390098484461, 102.19956313822217, 103.79490998874452,\n",
      "        103.76939198188575, 102.46069190095136, 102.64560728616925, 102.64626063030612,\n",
      "        102.51695863948956, 103.08617925879146, 101.94372626370294, 102.19847721687437]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11664928014770717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.562925009898623\n",
      "      mean_inference_ms: 10.81884408506865\n",
      "      mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  time_since_restore: 1223.2313277721405\n",
      "  time_this_iter_s: 8.939892053604126\n",
      "  time_total_s: 1223.2313277721405\n",
      "  timers:\n",
      "    learn_throughput: 14195.069\n",
      "    learn_time_ms: 352.517\n",
      "    synch_weights_time_ms: 9.05\n",
      "    training_iteration_time_ms: 8965.023\n",
      "  timestamp: 1662330553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 665532\n",
      "  training_iteration: 133\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 670536\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 670536\n",
      "    num_agent_steps_trained: 670536\n",
      "    num_env_steps_sampled: 670536\n",
      "    num_env_steps_trained: 670536\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-29-22\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78075484189979\n",
      "  episode_reward_min: 101.93821971981195\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 126\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2929733991622925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.073877451242879e-06\n",
      "          policy_loss: 0.06834651529788971\n",
      "          total_loss: 0.06845389306545258\n",
      "          vf_explained_var: 0.2656170725822449\n",
      "          vf_loss: 0.0001073895618901588\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 670536\n",
      "    num_agent_steps_trained: 670536\n",
      "    num_env_steps_sampled: 670536\n",
      "    num_env_steps_trained: 670536\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 670536\n",
      "  num_agent_steps_trained: 670536\n",
      "  num_env_steps_sampled: 670536\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 670536\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.2\n",
      "    ram_util_percent: 37.14166666666666\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11664928014770717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.562925009898623\n",
      "    mean_inference_ms: 10.81884408506865\n",
      "    mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78075484189979\n",
      "    episode_reward_min: 101.93821971981195\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.20038822865497, 103.79625847951847, 103.77073504509276, 102.46204510235188,\n",
      "        102.6469371365317, 102.64756943425583, 102.51821540542055, 103.0875192390335,\n",
      "        101.93912031605522, 102.19980944928844, 103.79682060964069, 103.76936051660124,\n",
      "        102.4606451383536, 102.64557724996303, 102.64621198307653, 102.5169109108498,\n",
      "        103.08613407616481, 101.9436697277808, 102.19841162553591, 103.79614241723428,\n",
      "        103.77064000820155, 102.4618886587304, 102.646880882052, 102.64750124916208,\n",
      "        102.51810285984993, 103.08738079080894, 101.93897894630204, 102.19973215059784,\n",
      "        103.79732238534635, 103.76980686111476, 102.46114650815524, 102.64602645801769,\n",
      "        102.64668308160944, 102.51737039893044, 103.08662327344526, 101.93821971981195,\n",
      "        102.19889060005423, 103.79676209772195, 103.76928762724583, 102.4605848614072,\n",
      "        102.64549554932918, 102.64614825929499, 102.51686374187916, 103.08606478052307,\n",
      "        101.94362155570418, 102.19836109768045, 103.7968226749796, 103.77068989422041,\n",
      "        102.51700274093768, 102.64780285788922, 102.64806596239531, 102.5186024471645,\n",
      "        103.08811951417884, 101.94498233858293, 102.2605462926252, 103.7975513424916,\n",
      "        103.77024914103386, 102.4613630396782, 102.64646441571935, 102.64691369312487,\n",
      "        102.51755289982283, 103.0868646049081, 101.94444673847279, 102.19916200844175,\n",
      "        103.79696950272854, 103.77107623251682, 102.4607642551226, 102.64569098491162,\n",
      "        102.64631875318773, 102.5170526312697, 103.08624823474777, 101.9438032285913,\n",
      "        102.19854788482233, 103.79902259922984, 103.77333039131061, 102.46284084530903,\n",
      "        102.64810243215321, 102.6486015244451, 102.51908321756594, 103.08841024987291,\n",
      "        101.93988269596869, 102.2060644942997, 103.79600335781976, 103.77745901303042,\n",
      "        102.46178942377085, 102.64686816008972, 102.64750815163215, 102.517960424526,\n",
      "        103.08724024642771, 101.9390098484461, 102.19956313822217, 103.79490998874452,\n",
      "        103.76939198188575, 102.46069190095136, 102.64560728616925, 102.64626063030612,\n",
      "        102.51695863948956, 103.08617925879146, 101.94372626370294, 102.19847721687437]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11664928014770717\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.562925009898623\n",
      "      mean_inference_ms: 10.81884408506865\n",
      "      mean_raw_obs_processing_ms: 0.20846493403960495\n",
      "  time_since_restore: 1232.168182849884\n",
      "  time_this_iter_s: 8.93685507774353\n",
      "  time_total_s: 1232.168182849884\n",
      "  timers:\n",
      "    learn_throughput: 14220.953\n",
      "    learn_time_ms: 351.875\n",
      "    synch_weights_time_ms: 9.009\n",
      "    training_iteration_time_ms: 8963.928\n",
      "  timestamp: 1662330562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 670536\n",
      "  training_iteration: 134\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 675540\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 675540\n",
      "    num_agent_steps_trained: 675540\n",
      "    num_env_steps_sampled: 675540\n",
      "    num_env_steps_trained: 675540\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78063993278272\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 135\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3012454509735107\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.576766175683588e-05\n",
      "          policy_loss: 0.020358877256512642\n",
      "          total_loss: 0.9120896458625793\n",
      "          vf_explained_var: -0.05571538954973221\n",
      "          vf_loss: 0.8917306661605835\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 675540\n",
      "    num_agent_steps_trained: 675540\n",
      "    num_env_steps_sampled: 675540\n",
      "    num_env_steps_trained: 675540\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 675540\n",
      "  num_agent_steps_trained: 675540\n",
      "  num_env_steps_sampled: 675540\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 675540\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 79.575\n",
      "    ram_util_percent: 37.09166666666666\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11660293036061158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.549083321525236\n",
      "    mean_inference_ms: 10.804083736174004\n",
      "    mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78063993278272\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19980944928844, 103.79682060964069, 103.76936051660124, 102.4606451383536,\n",
      "        102.64557724996303, 102.64621198307653, 102.5169109108498, 103.08613407616481,\n",
      "        101.9436697277808, 102.19841162553591, 103.79614241723428, 103.77064000820155,\n",
      "        102.4618886587304, 102.646880882052, 102.64750124916208, 102.51810285984993,\n",
      "        103.08738079080894, 101.93897894630204, 102.19973215059784, 103.79732238534635,\n",
      "        103.76980686111476, 102.46114650815524, 102.64602645801769, 102.64668308160944,\n",
      "        102.51737039893044, 103.08662327344526, 101.93821971981195, 102.19889060005423,\n",
      "        103.79676209772195, 103.76928762724583, 102.4605848614072, 102.64549554932918,\n",
      "        102.64614825929499, 102.51686374187916, 103.08606478052307, 101.94362155570418,\n",
      "        102.19836109768045, 103.7968226749796, 103.77068989422041, 102.51700274093768,\n",
      "        102.64780285788922, 102.64806596239531, 102.5186024471645, 103.08811951417884,\n",
      "        101.94498233858293, 102.2605462926252, 103.7975513424916, 103.77024914103386,\n",
      "        102.4613630396782, 102.64646441571935, 102.64691369312487, 102.51755289982283,\n",
      "        103.0868646049081, 101.94444673847279, 102.19916200844175, 103.79696950272854,\n",
      "        103.77107623251682, 102.4607642551226, 102.64569098491162, 102.64631875318773,\n",
      "        102.5170526312697, 103.08624823474777, 101.9438032285913, 102.19854788482233,\n",
      "        103.79902259922984, 103.77333039131061, 102.46284084530903, 102.64810243215321,\n",
      "        102.6486015244451, 102.51908321756594, 103.08841024987291, 101.93988269596869,\n",
      "        102.2060644942997, 103.79600335781976, 103.77745901303042, 102.46178942377085,\n",
      "        102.64686816008972, 102.64750815163215, 102.517960424526, 103.08724024642771,\n",
      "        101.9390098484461, 102.19956313822217, 103.79490998874452, 103.76939198188575,\n",
      "        102.46069190095136, 102.64560728616925, 102.64626063030612, 102.51695863948956,\n",
      "        103.08617925879146, 101.94372626370294, 102.19847721687437, 103.79499948904999,\n",
      "        103.76955518246866, 102.46080021822279, 102.6457450113279, 102.64637082543501,\n",
      "        102.51703578921665, 103.08627999088272, 101.93793493358653, 102.19857603501494]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11660293036061158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.549083321525236\n",
      "      mean_inference_ms: 10.804083736174004\n",
      "      mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  time_since_restore: 1240.8887877464294\n",
      "  time_this_iter_s: 8.72060489654541\n",
      "  time_total_s: 1240.8887877464294\n",
      "  timers:\n",
      "    learn_throughput: 14201.838\n",
      "    learn_time_ms: 352.349\n",
      "    synch_weights_time_ms: 9.002\n",
      "    training_iteration_time_ms: 8949.384\n",
      "  timestamp: 1662330571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 675540\n",
      "  training_iteration: 135\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=7510.60.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=7510.80.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 680544\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 680544\n",
      "    num_agent_steps_trained: 680544\n",
      "    num_env_steps_sampled: 680544\n",
      "    num_env_steps_trained: 680544\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-29-40\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78063993278272\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 135\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.3334676027297974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.8772328985505737e-05\n",
      "          policy_loss: -0.04187304526567459\n",
      "          total_loss: 5.258872985839844\n",
      "          vf_explained_var: -0.0014101117849349976\n",
      "          vf_loss: 5.300745964050293\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 680544\n",
      "    num_agent_steps_trained: 680544\n",
      "    num_env_steps_sampled: 680544\n",
      "    num_env_steps_trained: 680544\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 680544\n",
      "  num_agent_steps_trained: 680544\n",
      "  num_env_steps_sampled: 680544\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 680544\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 73.1\n",
      "    ram_util_percent: 37.1\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11660293036061158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.549083321525236\n",
      "    mean_inference_ms: 10.804083736174004\n",
      "    mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78063993278272\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19980944928844, 103.79682060964069, 103.76936051660124, 102.4606451383536,\n",
      "        102.64557724996303, 102.64621198307653, 102.5169109108498, 103.08613407616481,\n",
      "        101.9436697277808, 102.19841162553591, 103.79614241723428, 103.77064000820155,\n",
      "        102.4618886587304, 102.646880882052, 102.64750124916208, 102.51810285984993,\n",
      "        103.08738079080894, 101.93897894630204, 102.19973215059784, 103.79732238534635,\n",
      "        103.76980686111476, 102.46114650815524, 102.64602645801769, 102.64668308160944,\n",
      "        102.51737039893044, 103.08662327344526, 101.93821971981195, 102.19889060005423,\n",
      "        103.79676209772195, 103.76928762724583, 102.4605848614072, 102.64549554932918,\n",
      "        102.64614825929499, 102.51686374187916, 103.08606478052307, 101.94362155570418,\n",
      "        102.19836109768045, 103.7968226749796, 103.77068989422041, 102.51700274093768,\n",
      "        102.64780285788922, 102.64806596239531, 102.5186024471645, 103.08811951417884,\n",
      "        101.94498233858293, 102.2605462926252, 103.7975513424916, 103.77024914103386,\n",
      "        102.4613630396782, 102.64646441571935, 102.64691369312487, 102.51755289982283,\n",
      "        103.0868646049081, 101.94444673847279, 102.19916200844175, 103.79696950272854,\n",
      "        103.77107623251682, 102.4607642551226, 102.64569098491162, 102.64631875318773,\n",
      "        102.5170526312697, 103.08624823474777, 101.9438032285913, 102.19854788482233,\n",
      "        103.79902259922984, 103.77333039131061, 102.46284084530903, 102.64810243215321,\n",
      "        102.6486015244451, 102.51908321756594, 103.08841024987291, 101.93988269596869,\n",
      "        102.2060644942997, 103.79600335781976, 103.77745901303042, 102.46178942377085,\n",
      "        102.64686816008972, 102.64750815163215, 102.517960424526, 103.08724024642771,\n",
      "        101.9390098484461, 102.19956313822217, 103.79490998874452, 103.76939198188575,\n",
      "        102.46069190095136, 102.64560728616925, 102.64626063030612, 102.51695863948956,\n",
      "        103.08617925879146, 101.94372626370294, 102.19847721687437, 103.79499948904999,\n",
      "        103.76955518246866, 102.46080021822279, 102.6457450113279, 102.64637082543501,\n",
      "        102.51703578921665, 103.08627999088272, 101.93793493358653, 102.19857603501494]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11660293036061158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.549083321525236\n",
      "      mean_inference_ms: 10.804083736174004\n",
      "      mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  time_since_restore: 1249.9338610172272\n",
      "  time_this_iter_s: 9.04507327079773\n",
      "  time_total_s: 1249.9338610172272\n",
      "  timers:\n",
      "    learn_throughput: 14193.967\n",
      "    learn_time_ms: 352.544\n",
      "    synch_weights_time_ms: 8.809\n",
      "    training_iteration_time_ms: 8954.487\n",
      "  timestamp: 1662330580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 680544\n",
      "  training_iteration: 136\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 685548\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 685548\n",
      "    num_agent_steps_trained: 685548\n",
      "    num_env_steps_sampled: 685548\n",
      "    num_env_steps_trained: 685548\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-29-49\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78063993278272\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 135\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.272881269454956\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2268958016647957e-05\n",
      "          policy_loss: 0.08280573040246964\n",
      "          total_loss: 0.08328567445278168\n",
      "          vf_explained_var: 0.3433585464954376\n",
      "          vf_loss: 0.00047993194311857224\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 685548\n",
      "    num_agent_steps_trained: 685548\n",
      "    num_env_steps_sampled: 685548\n",
      "    num_env_steps_trained: 685548\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 685548\n",
      "  num_agent_steps_trained: 685548\n",
      "  num_env_steps_sampled: 685548\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 685548\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 81.63333333333334\n",
      "    ram_util_percent: 36.94166666666666\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11660293036061158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.549083321525236\n",
      "    mean_inference_ms: 10.804083736174004\n",
      "    mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78063993278272\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19980944928844, 103.79682060964069, 103.76936051660124, 102.4606451383536,\n",
      "        102.64557724996303, 102.64621198307653, 102.5169109108498, 103.08613407616481,\n",
      "        101.9436697277808, 102.19841162553591, 103.79614241723428, 103.77064000820155,\n",
      "        102.4618886587304, 102.646880882052, 102.64750124916208, 102.51810285984993,\n",
      "        103.08738079080894, 101.93897894630204, 102.19973215059784, 103.79732238534635,\n",
      "        103.76980686111476, 102.46114650815524, 102.64602645801769, 102.64668308160944,\n",
      "        102.51737039893044, 103.08662327344526, 101.93821971981195, 102.19889060005423,\n",
      "        103.79676209772195, 103.76928762724583, 102.4605848614072, 102.64549554932918,\n",
      "        102.64614825929499, 102.51686374187916, 103.08606478052307, 101.94362155570418,\n",
      "        102.19836109768045, 103.7968226749796, 103.77068989422041, 102.51700274093768,\n",
      "        102.64780285788922, 102.64806596239531, 102.5186024471645, 103.08811951417884,\n",
      "        101.94498233858293, 102.2605462926252, 103.7975513424916, 103.77024914103386,\n",
      "        102.4613630396782, 102.64646441571935, 102.64691369312487, 102.51755289982283,\n",
      "        103.0868646049081, 101.94444673847279, 102.19916200844175, 103.79696950272854,\n",
      "        103.77107623251682, 102.4607642551226, 102.64569098491162, 102.64631875318773,\n",
      "        102.5170526312697, 103.08624823474777, 101.9438032285913, 102.19854788482233,\n",
      "        103.79902259922984, 103.77333039131061, 102.46284084530903, 102.64810243215321,\n",
      "        102.6486015244451, 102.51908321756594, 103.08841024987291, 101.93988269596869,\n",
      "        102.2060644942997, 103.79600335781976, 103.77745901303042, 102.46178942377085,\n",
      "        102.64686816008972, 102.64750815163215, 102.517960424526, 103.08724024642771,\n",
      "        101.9390098484461, 102.19956313822217, 103.79490998874452, 103.76939198188575,\n",
      "        102.46069190095136, 102.64560728616925, 102.64626063030612, 102.51695863948956,\n",
      "        103.08617925879146, 101.94372626370294, 102.19847721687437, 103.79499948904999,\n",
      "        103.76955518246866, 102.46080021822279, 102.6457450113279, 102.64637082543501,\n",
      "        102.51703578921665, 103.08627999088272, 101.93793493358653, 102.19857603501494]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11660293036061158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.549083321525236\n",
      "      mean_inference_ms: 10.804083736174004\n",
      "      mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  time_since_restore: 1259.2221291065216\n",
      "  time_this_iter_s: 9.288268089294434\n",
      "  time_total_s: 1259.2221291065216\n",
      "  timers:\n",
      "    learn_throughput: 14156.013\n",
      "    learn_time_ms: 353.489\n",
      "    synch_weights_time_ms: 8.774\n",
      "    training_iteration_time_ms: 8991.053\n",
      "  timestamp: 1662330589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 685548\n",
      "  training_iteration: 137\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 690552\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 690552\n",
      "    num_agent_steps_trained: 690552\n",
      "    num_env_steps_sampled: 690552\n",
      "    num_env_steps_trained: 690552\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78063993278272\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 135\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.269349455833435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.029246257341583e-07\n",
      "          policy_loss: -0.0065207937732338905\n",
      "          total_loss: -0.006370247341692448\n",
      "          vf_explained_var: 0.19426468014717102\n",
      "          vf_loss: 0.00015053903916850686\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 690552\n",
      "    num_agent_steps_trained: 690552\n",
      "    num_env_steps_sampled: 690552\n",
      "    num_env_steps_trained: 690552\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 690552\n",
      "  num_agent_steps_trained: 690552\n",
      "  num_env_steps_sampled: 690552\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 690552\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.15384615384616\n",
      "    ram_util_percent: 37.230769230769226\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11660293036061158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.549083321525236\n",
      "    mean_inference_ms: 10.804083736174004\n",
      "    mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78063993278272\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19980944928844, 103.79682060964069, 103.76936051660124, 102.4606451383536,\n",
      "        102.64557724996303, 102.64621198307653, 102.5169109108498, 103.08613407616481,\n",
      "        101.9436697277808, 102.19841162553591, 103.79614241723428, 103.77064000820155,\n",
      "        102.4618886587304, 102.646880882052, 102.64750124916208, 102.51810285984993,\n",
      "        103.08738079080894, 101.93897894630204, 102.19973215059784, 103.79732238534635,\n",
      "        103.76980686111476, 102.46114650815524, 102.64602645801769, 102.64668308160944,\n",
      "        102.51737039893044, 103.08662327344526, 101.93821971981195, 102.19889060005423,\n",
      "        103.79676209772195, 103.76928762724583, 102.4605848614072, 102.64549554932918,\n",
      "        102.64614825929499, 102.51686374187916, 103.08606478052307, 101.94362155570418,\n",
      "        102.19836109768045, 103.7968226749796, 103.77068989422041, 102.51700274093768,\n",
      "        102.64780285788922, 102.64806596239531, 102.5186024471645, 103.08811951417884,\n",
      "        101.94498233858293, 102.2605462926252, 103.7975513424916, 103.77024914103386,\n",
      "        102.4613630396782, 102.64646441571935, 102.64691369312487, 102.51755289982283,\n",
      "        103.0868646049081, 101.94444673847279, 102.19916200844175, 103.79696950272854,\n",
      "        103.77107623251682, 102.4607642551226, 102.64569098491162, 102.64631875318773,\n",
      "        102.5170526312697, 103.08624823474777, 101.9438032285913, 102.19854788482233,\n",
      "        103.79902259922984, 103.77333039131061, 102.46284084530903, 102.64810243215321,\n",
      "        102.6486015244451, 102.51908321756594, 103.08841024987291, 101.93988269596869,\n",
      "        102.2060644942997, 103.79600335781976, 103.77745901303042, 102.46178942377085,\n",
      "        102.64686816008972, 102.64750815163215, 102.517960424526, 103.08724024642771,\n",
      "        101.9390098484461, 102.19956313822217, 103.79490998874452, 103.76939198188575,\n",
      "        102.46069190095136, 102.64560728616925, 102.64626063030612, 102.51695863948956,\n",
      "        103.08617925879146, 101.94372626370294, 102.19847721687437, 103.79499948904999,\n",
      "        103.76955518246866, 102.46080021822279, 102.6457450113279, 102.64637082543501,\n",
      "        102.51703578921665, 103.08627999088272, 101.93793493358653, 102.19857603501494]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11660293036061158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.549083321525236\n",
      "      mean_inference_ms: 10.804083736174004\n",
      "      mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  time_since_restore: 1268.3503453731537\n",
      "  time_this_iter_s: 9.12821626663208\n",
      "  time_total_s: 1268.3503453731537\n",
      "  timers:\n",
      "    learn_throughput: 14189.697\n",
      "    learn_time_ms: 352.65\n",
      "    synch_weights_time_ms: 8.707\n",
      "    training_iteration_time_ms: 9015.103\n",
      "  timestamp: 1662330598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 690552\n",
      "  training_iteration: 138\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 695556\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 695556\n",
      "    num_agent_steps_trained: 695556\n",
      "    num_env_steps_sampled: 695556\n",
      "    num_env_steps_trained: 695556\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-30-07\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78063993278272\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 135\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2668157815933228\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 9.306127140007447e-06\n",
      "          policy_loss: 0.07561234384775162\n",
      "          total_loss: 0.07574675977230072\n",
      "          vf_explained_var: 0.10315108299255371\n",
      "          vf_loss: 0.00013442736235447228\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 695556\n",
      "    num_agent_steps_trained: 695556\n",
      "    num_env_steps_sampled: 695556\n",
      "    num_env_steps_trained: 695556\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 695556\n",
      "  num_agent_steps_trained: 695556\n",
      "  num_env_steps_sampled: 695556\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 695556\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.01538461538463\n",
      "    ram_util_percent: 37.215384615384615\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11660293036061158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.549083321525236\n",
      "    mean_inference_ms: 10.804083736174004\n",
      "    mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78063993278272\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19980944928844, 103.79682060964069, 103.76936051660124, 102.4606451383536,\n",
      "        102.64557724996303, 102.64621198307653, 102.5169109108498, 103.08613407616481,\n",
      "        101.9436697277808, 102.19841162553591, 103.79614241723428, 103.77064000820155,\n",
      "        102.4618886587304, 102.646880882052, 102.64750124916208, 102.51810285984993,\n",
      "        103.08738079080894, 101.93897894630204, 102.19973215059784, 103.79732238534635,\n",
      "        103.76980686111476, 102.46114650815524, 102.64602645801769, 102.64668308160944,\n",
      "        102.51737039893044, 103.08662327344526, 101.93821971981195, 102.19889060005423,\n",
      "        103.79676209772195, 103.76928762724583, 102.4605848614072, 102.64549554932918,\n",
      "        102.64614825929499, 102.51686374187916, 103.08606478052307, 101.94362155570418,\n",
      "        102.19836109768045, 103.7968226749796, 103.77068989422041, 102.51700274093768,\n",
      "        102.64780285788922, 102.64806596239531, 102.5186024471645, 103.08811951417884,\n",
      "        101.94498233858293, 102.2605462926252, 103.7975513424916, 103.77024914103386,\n",
      "        102.4613630396782, 102.64646441571935, 102.64691369312487, 102.51755289982283,\n",
      "        103.0868646049081, 101.94444673847279, 102.19916200844175, 103.79696950272854,\n",
      "        103.77107623251682, 102.4607642551226, 102.64569098491162, 102.64631875318773,\n",
      "        102.5170526312697, 103.08624823474777, 101.9438032285913, 102.19854788482233,\n",
      "        103.79902259922984, 103.77333039131061, 102.46284084530903, 102.64810243215321,\n",
      "        102.6486015244451, 102.51908321756594, 103.08841024987291, 101.93988269596869,\n",
      "        102.2060644942997, 103.79600335781976, 103.77745901303042, 102.46178942377085,\n",
      "        102.64686816008972, 102.64750815163215, 102.517960424526, 103.08724024642771,\n",
      "        101.9390098484461, 102.19956313822217, 103.79490998874452, 103.76939198188575,\n",
      "        102.46069190095136, 102.64560728616925, 102.64626063030612, 102.51695863948956,\n",
      "        103.08617925879146, 101.94372626370294, 102.19847721687437, 103.79499948904999,\n",
      "        103.76955518246866, 102.46080021822279, 102.6457450113279, 102.64637082543501,\n",
      "        102.51703578921665, 103.08627999088272, 101.93793493358653, 102.19857603501494]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11660293036061158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.549083321525236\n",
      "      mean_inference_ms: 10.804083736174004\n",
      "      mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  time_since_restore: 1277.533896446228\n",
      "  time_this_iter_s: 9.18355107307434\n",
      "  time_total_s: 1277.533896446228\n",
      "  timers:\n",
      "    learn_throughput: 14064.502\n",
      "    learn_time_ms: 355.789\n",
      "    synch_weights_time_ms: 8.698\n",
      "    training_iteration_time_ms: 9011.172\n",
      "  timestamp: 1662330607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 695556\n",
      "  training_iteration: 139\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 700560\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 700560\n",
      "    num_agent_steps_trained: 700560\n",
      "    num_env_steps_sampled: 700560\n",
      "    num_env_steps_trained: 700560\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-30-16\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78063993278272\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 135\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.262718915939331\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.94620700617088e-06\n",
      "          policy_loss: -0.04634539783000946\n",
      "          total_loss: -0.04621649533510208\n",
      "          vf_explained_var: 0.2357814610004425\n",
      "          vf_loss: 0.00012890122889075428\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 700560\n",
      "    num_agent_steps_trained: 700560\n",
      "    num_env_steps_sampled: 700560\n",
      "    num_env_steps_trained: 700560\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 700560\n",
      "  num_agent_steps_trained: 700560\n",
      "  num_env_steps_sampled: 700560\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 700560\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 76.8\n",
      "    ram_util_percent: 37.066666666666656\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11660293036061158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.549083321525236\n",
      "    mean_inference_ms: 10.804083736174004\n",
      "    mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78063993278272\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19980944928844, 103.79682060964069, 103.76936051660124, 102.4606451383536,\n",
      "        102.64557724996303, 102.64621198307653, 102.5169109108498, 103.08613407616481,\n",
      "        101.9436697277808, 102.19841162553591, 103.79614241723428, 103.77064000820155,\n",
      "        102.4618886587304, 102.646880882052, 102.64750124916208, 102.51810285984993,\n",
      "        103.08738079080894, 101.93897894630204, 102.19973215059784, 103.79732238534635,\n",
      "        103.76980686111476, 102.46114650815524, 102.64602645801769, 102.64668308160944,\n",
      "        102.51737039893044, 103.08662327344526, 101.93821971981195, 102.19889060005423,\n",
      "        103.79676209772195, 103.76928762724583, 102.4605848614072, 102.64549554932918,\n",
      "        102.64614825929499, 102.51686374187916, 103.08606478052307, 101.94362155570418,\n",
      "        102.19836109768045, 103.7968226749796, 103.77068989422041, 102.51700274093768,\n",
      "        102.64780285788922, 102.64806596239531, 102.5186024471645, 103.08811951417884,\n",
      "        101.94498233858293, 102.2605462926252, 103.7975513424916, 103.77024914103386,\n",
      "        102.4613630396782, 102.64646441571935, 102.64691369312487, 102.51755289982283,\n",
      "        103.0868646049081, 101.94444673847279, 102.19916200844175, 103.79696950272854,\n",
      "        103.77107623251682, 102.4607642551226, 102.64569098491162, 102.64631875318773,\n",
      "        102.5170526312697, 103.08624823474777, 101.9438032285913, 102.19854788482233,\n",
      "        103.79902259922984, 103.77333039131061, 102.46284084530903, 102.64810243215321,\n",
      "        102.6486015244451, 102.51908321756594, 103.08841024987291, 101.93988269596869,\n",
      "        102.2060644942997, 103.79600335781976, 103.77745901303042, 102.46178942377085,\n",
      "        102.64686816008972, 102.64750815163215, 102.517960424526, 103.08724024642771,\n",
      "        101.9390098484461, 102.19956313822217, 103.79490998874452, 103.76939198188575,\n",
      "        102.46069190095136, 102.64560728616925, 102.64626063030612, 102.51695863948956,\n",
      "        103.08617925879146, 101.94372626370294, 102.19847721687437, 103.79499948904999,\n",
      "        103.76955518246866, 102.46080021822279, 102.6457450113279, 102.64637082543501,\n",
      "        102.51703578921665, 103.08627999088272, 101.93793493358653, 102.19857603501494]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11660293036061158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.549083321525236\n",
      "      mean_inference_ms: 10.804083736174004\n",
      "      mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  time_since_restore: 1286.5825164318085\n",
      "  time_this_iter_s: 9.048619985580444\n",
      "  time_total_s: 1286.5825164318085\n",
      "  timers:\n",
      "    learn_throughput: 14093.372\n",
      "    learn_time_ms: 355.061\n",
      "    synch_weights_time_ms: 8.668\n",
      "    training_iteration_time_ms: 9012.788\n",
      "  timestamp: 1662330616\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 700560\n",
      "  training_iteration: 140\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 705564\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 705564\n",
      "    num_agent_steps_trained: 705564\n",
      "    num_env_steps_sampled: 705564\n",
      "    num_env_steps_trained: 705564\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78063993278272\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 135\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2637301683425903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.889740128215635e-06\n",
      "          policy_loss: 0.0045827217400074005\n",
      "          total_loss: 0.004712230991572142\n",
      "          vf_explained_var: 0.13869783282279968\n",
      "          vf_loss: 0.00012951352982781827\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 705564\n",
      "    num_agent_steps_trained: 705564\n",
      "    num_env_steps_sampled: 705564\n",
      "    num_env_steps_trained: 705564\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 705564\n",
      "  num_agent_steps_trained: 705564\n",
      "  num_env_steps_sampled: 705564\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 705564\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 72.60769230769232\n",
      "    ram_util_percent: 37.07692307692308\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11660293036061158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.549083321525236\n",
      "    mean_inference_ms: 10.804083736174004\n",
      "    mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78063993278272\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19980944928844, 103.79682060964069, 103.76936051660124, 102.4606451383536,\n",
      "        102.64557724996303, 102.64621198307653, 102.5169109108498, 103.08613407616481,\n",
      "        101.9436697277808, 102.19841162553591, 103.79614241723428, 103.77064000820155,\n",
      "        102.4618886587304, 102.646880882052, 102.64750124916208, 102.51810285984993,\n",
      "        103.08738079080894, 101.93897894630204, 102.19973215059784, 103.79732238534635,\n",
      "        103.76980686111476, 102.46114650815524, 102.64602645801769, 102.64668308160944,\n",
      "        102.51737039893044, 103.08662327344526, 101.93821971981195, 102.19889060005423,\n",
      "        103.79676209772195, 103.76928762724583, 102.4605848614072, 102.64549554932918,\n",
      "        102.64614825929499, 102.51686374187916, 103.08606478052307, 101.94362155570418,\n",
      "        102.19836109768045, 103.7968226749796, 103.77068989422041, 102.51700274093768,\n",
      "        102.64780285788922, 102.64806596239531, 102.5186024471645, 103.08811951417884,\n",
      "        101.94498233858293, 102.2605462926252, 103.7975513424916, 103.77024914103386,\n",
      "        102.4613630396782, 102.64646441571935, 102.64691369312487, 102.51755289982283,\n",
      "        103.0868646049081, 101.94444673847279, 102.19916200844175, 103.79696950272854,\n",
      "        103.77107623251682, 102.4607642551226, 102.64569098491162, 102.64631875318773,\n",
      "        102.5170526312697, 103.08624823474777, 101.9438032285913, 102.19854788482233,\n",
      "        103.79902259922984, 103.77333039131061, 102.46284084530903, 102.64810243215321,\n",
      "        102.6486015244451, 102.51908321756594, 103.08841024987291, 101.93988269596869,\n",
      "        102.2060644942997, 103.79600335781976, 103.77745901303042, 102.46178942377085,\n",
      "        102.64686816008972, 102.64750815163215, 102.517960424526, 103.08724024642771,\n",
      "        101.9390098484461, 102.19956313822217, 103.79490998874452, 103.76939198188575,\n",
      "        102.46069190095136, 102.64560728616925, 102.64626063030612, 102.51695863948956,\n",
      "        103.08617925879146, 101.94372626370294, 102.19847721687437, 103.79499948904999,\n",
      "        103.76955518246866, 102.46080021822279, 102.6457450113279, 102.64637082543501,\n",
      "        102.51703578921665, 103.08627999088272, 101.93793493358653, 102.19857603501494]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11660293036061158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.549083321525236\n",
      "      mean_inference_ms: 10.804083736174004\n",
      "      mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  time_since_restore: 1295.665507555008\n",
      "  time_this_iter_s: 9.082991123199463\n",
      "  time_total_s: 1295.665507555008\n",
      "  timers:\n",
      "    learn_throughput: 14117.264\n",
      "    learn_time_ms: 354.46\n",
      "    synch_weights_time_ms: 8.657\n",
      "    training_iteration_time_ms: 9016.504\n",
      "  timestamp: 1662330625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 705564\n",
      "  training_iteration: 141\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 710568\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 710568\n",
      "    num_agent_steps_trained: 710568\n",
      "    num_env_steps_sampled: 710568\n",
      "    num_env_steps_trained: 710568\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-30-34\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78063993278272\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 135\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.263139247894287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.2036536645609885e-05\n",
      "          policy_loss: 0.12212973833084106\n",
      "          total_loss: 0.12226029485464096\n",
      "          vf_explained_var: 0.2772773802280426\n",
      "          vf_loss: 0.00013055464660283178\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 710568\n",
      "    num_agent_steps_trained: 710568\n",
      "    num_env_steps_sampled: 710568\n",
      "    num_env_steps_trained: 710568\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 710568\n",
      "  num_agent_steps_trained: 710568\n",
      "  num_env_steps_sampled: 710568\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 710568\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.44999999999999\n",
      "    ram_util_percent: 37.475\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11660293036061158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.549083321525236\n",
      "    mean_inference_ms: 10.804083736174004\n",
      "    mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78063993278272\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19980944928844, 103.79682060964069, 103.76936051660124, 102.4606451383536,\n",
      "        102.64557724996303, 102.64621198307653, 102.5169109108498, 103.08613407616481,\n",
      "        101.9436697277808, 102.19841162553591, 103.79614241723428, 103.77064000820155,\n",
      "        102.4618886587304, 102.646880882052, 102.64750124916208, 102.51810285984993,\n",
      "        103.08738079080894, 101.93897894630204, 102.19973215059784, 103.79732238534635,\n",
      "        103.76980686111476, 102.46114650815524, 102.64602645801769, 102.64668308160944,\n",
      "        102.51737039893044, 103.08662327344526, 101.93821971981195, 102.19889060005423,\n",
      "        103.79676209772195, 103.76928762724583, 102.4605848614072, 102.64549554932918,\n",
      "        102.64614825929499, 102.51686374187916, 103.08606478052307, 101.94362155570418,\n",
      "        102.19836109768045, 103.7968226749796, 103.77068989422041, 102.51700274093768,\n",
      "        102.64780285788922, 102.64806596239531, 102.5186024471645, 103.08811951417884,\n",
      "        101.94498233858293, 102.2605462926252, 103.7975513424916, 103.77024914103386,\n",
      "        102.4613630396782, 102.64646441571935, 102.64691369312487, 102.51755289982283,\n",
      "        103.0868646049081, 101.94444673847279, 102.19916200844175, 103.79696950272854,\n",
      "        103.77107623251682, 102.4607642551226, 102.64569098491162, 102.64631875318773,\n",
      "        102.5170526312697, 103.08624823474777, 101.9438032285913, 102.19854788482233,\n",
      "        103.79902259922984, 103.77333039131061, 102.46284084530903, 102.64810243215321,\n",
      "        102.6486015244451, 102.51908321756594, 103.08841024987291, 101.93988269596869,\n",
      "        102.2060644942997, 103.79600335781976, 103.77745901303042, 102.46178942377085,\n",
      "        102.64686816008972, 102.64750815163215, 102.517960424526, 103.08724024642771,\n",
      "        101.9390098484461, 102.19956313822217, 103.79490998874452, 103.76939198188575,\n",
      "        102.46069190095136, 102.64560728616925, 102.64626063030612, 102.51695863948956,\n",
      "        103.08617925879146, 101.94372626370294, 102.19847721687437, 103.79499948904999,\n",
      "        103.76955518246866, 102.46080021822279, 102.6457450113279, 102.64637082543501,\n",
      "        102.51703578921665, 103.08627999088272, 101.93793493358653, 102.19857603501494]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11660293036061158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.549083321525236\n",
      "      mean_inference_ms: 10.804083736174004\n",
      "      mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  time_since_restore: 1304.548772573471\n",
      "  time_this_iter_s: 8.883265018463135\n",
      "  time_total_s: 1304.548772573471\n",
      "  timers:\n",
      "    learn_throughput: 14091.86\n",
      "    learn_time_ms: 355.099\n",
      "    synch_weights_time_ms: 8.686\n",
      "    training_iteration_time_ms: 9018.413\n",
      "  timestamp: 1662330634\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 710568\n",
      "  training_iteration: 142\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 715572\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 715572\n",
      "    num_agent_steps_trained: 715572\n",
      "    num_env_steps_sampled: 715572\n",
      "    num_env_steps_trained: 715572\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-30-43\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79902259922984\n",
      "  episode_reward_mean: 102.78063993278272\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 135\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2568095922470093\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 2.3175682144938037e-05\n",
      "          policy_loss: 0.1246851459145546\n",
      "          total_loss: 0.12481416761875153\n",
      "          vf_explained_var: 0.3344089090824127\n",
      "          vf_loss: 0.00012902729213237762\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 715572\n",
      "    num_agent_steps_trained: 715572\n",
      "    num_env_steps_sampled: 715572\n",
      "    num_env_steps_trained: 715572\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 715572\n",
      "  num_agent_steps_trained: 715572\n",
      "  num_env_steps_sampled: 715572\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 715572\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 73.43076923076923\n",
      "    ram_util_percent: 37.30769230769231\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11660293036061158\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.549083321525236\n",
      "    mean_inference_ms: 10.804083736174004\n",
      "    mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79902259922984\n",
      "    episode_reward_mean: 102.78063993278272\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19980944928844, 103.79682060964069, 103.76936051660124, 102.4606451383536,\n",
      "        102.64557724996303, 102.64621198307653, 102.5169109108498, 103.08613407616481,\n",
      "        101.9436697277808, 102.19841162553591, 103.79614241723428, 103.77064000820155,\n",
      "        102.4618886587304, 102.646880882052, 102.64750124916208, 102.51810285984993,\n",
      "        103.08738079080894, 101.93897894630204, 102.19973215059784, 103.79732238534635,\n",
      "        103.76980686111476, 102.46114650815524, 102.64602645801769, 102.64668308160944,\n",
      "        102.51737039893044, 103.08662327344526, 101.93821971981195, 102.19889060005423,\n",
      "        103.79676209772195, 103.76928762724583, 102.4605848614072, 102.64549554932918,\n",
      "        102.64614825929499, 102.51686374187916, 103.08606478052307, 101.94362155570418,\n",
      "        102.19836109768045, 103.7968226749796, 103.77068989422041, 102.51700274093768,\n",
      "        102.64780285788922, 102.64806596239531, 102.5186024471645, 103.08811951417884,\n",
      "        101.94498233858293, 102.2605462926252, 103.7975513424916, 103.77024914103386,\n",
      "        102.4613630396782, 102.64646441571935, 102.64691369312487, 102.51755289982283,\n",
      "        103.0868646049081, 101.94444673847279, 102.19916200844175, 103.79696950272854,\n",
      "        103.77107623251682, 102.4607642551226, 102.64569098491162, 102.64631875318773,\n",
      "        102.5170526312697, 103.08624823474777, 101.9438032285913, 102.19854788482233,\n",
      "        103.79902259922984, 103.77333039131061, 102.46284084530903, 102.64810243215321,\n",
      "        102.6486015244451, 102.51908321756594, 103.08841024987291, 101.93988269596869,\n",
      "        102.2060644942997, 103.79600335781976, 103.77745901303042, 102.46178942377085,\n",
      "        102.64686816008972, 102.64750815163215, 102.517960424526, 103.08724024642771,\n",
      "        101.9390098484461, 102.19956313822217, 103.79490998874452, 103.76939198188575,\n",
      "        102.46069190095136, 102.64560728616925, 102.64626063030612, 102.51695863948956,\n",
      "        103.08617925879146, 101.94372626370294, 102.19847721687437, 103.79499948904999,\n",
      "        103.76955518246866, 102.46080021822279, 102.6457450113279, 102.64637082543501,\n",
      "        102.51703578921665, 103.08627999088272, 101.93793493358653, 102.19857603501494]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.11660293036061158\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.549083321525236\n",
      "      mean_inference_ms: 10.804083736174004\n",
      "      mean_raw_obs_processing_ms: 0.20881808514480693\n",
      "  time_since_restore: 1313.5620987415314\n",
      "  time_this_iter_s: 9.013326168060303\n",
      "  time_total_s: 1313.5620987415314\n",
      "  timers:\n",
      "    learn_throughput: 14118.041\n",
      "    learn_time_ms: 354.44\n",
      "    synch_weights_time_ms: 8.615\n",
      "    training_iteration_time_ms: 9025.749\n",
      "  timestamp: 1662330643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 715572\n",
      "  training_iteration: 143\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 720576\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 720576\n",
      "    num_agent_steps_trained: 720576\n",
      "    num_env_steps_sampled: 720576\n",
      "    num_env_steps_trained: 720576\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-30-52\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79945888203886\n",
      "  episode_reward_mean: 102.78080550188763\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 144\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2707653045654297\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.2246316550299525e-05\n",
      "          policy_loss: 0.008471730165183544\n",
      "          total_loss: 1.097357153892517\n",
      "          vf_explained_var: -0.00369492475874722\n",
      "          vf_loss: 1.0888854265213013\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 720576\n",
      "    num_agent_steps_trained: 720576\n",
      "    num_env_steps_sampled: 720576\n",
      "    num_env_steps_trained: 720576\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 720576\n",
      "  num_agent_steps_trained: 720576\n",
      "  num_env_steps_sampled: 720576\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 720576\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.98333333333333\n",
      "    ram_util_percent: 36.93333333333334\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1165672455162569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.536804389035105\n",
      "    mean_inference_ms: 10.790049657825104\n",
      "    mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79945888203886\n",
      "    episode_reward_mean: 102.78080550188763\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 9\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997,\n",
      "        103.79600335781976, 103.77745901303042, 102.46178942377085, 102.64686816008972,\n",
      "        102.64750815163215, 102.517960424526, 103.08724024642771, 101.9390098484461,\n",
      "        102.19956313822217, 103.79490998874452, 103.76939198188575, 102.46069190095136,\n",
      "        102.64560728616925, 102.64626063030612, 102.51695863948956, 103.08617925879146,\n",
      "        101.94372626370294, 102.19847721687437, 103.79499948904999, 103.76955518246866,\n",
      "        102.46080021822279, 102.6457450113279, 102.64637082543501, 102.51703578921665,\n",
      "        103.08627999088272, 101.93793493358653, 102.19857603501494, 103.79945888203886,\n",
      "        103.77210464944618, 102.46336520707935, 102.64818313853746, 102.6488286381259,\n",
      "        102.51953937389558, 103.08875934887419, 101.94035724867749, 102.20110008553476]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1165672455162569\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.536804389035105\n",
      "      mean_inference_ms: 10.790049657825104\n",
      "      mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  time_since_restore: 1322.5882935523987\n",
      "  time_this_iter_s: 9.02619481086731\n",
      "  time_total_s: 1322.5882935523987\n",
      "  timers:\n",
      "    learn_throughput: 14246.81\n",
      "    learn_time_ms: 351.237\n",
      "    synch_weights_time_ms: 8.571\n",
      "    training_iteration_time_ms: 9034.603\n",
      "  timestamp: 1662330652\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 720576\n",
      "  training_iteration: 144\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.27, wished=4.50, severity=1.28, time=8010.70.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Warning: Vehicle 'human_8' performs emergency braking on lane 'right_0' with decel=10.45, wished=4.50, severity=1.32, time=8010.90.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 725580\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 725580\n",
      "    num_agent_steps_trained: 725580\n",
      "    num_env_steps_sampled: 725580\n",
      "    num_env_steps_trained: 725580\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-31-02\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79945888203886\n",
      "  episode_reward_mean: 102.78080550188763\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 144\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.303743600845337\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 3.9257480239029974e-05\n",
      "          policy_loss: -0.03199172765016556\n",
      "          total_loss: 4.8065996170043945\n",
      "          vf_explained_var: 0.0009902060264721513\n",
      "          vf_loss: 4.838591575622559\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 725580\n",
      "    num_agent_steps_trained: 725580\n",
      "    num_env_steps_sampled: 725580\n",
      "    num_env_steps_trained: 725580\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 725580\n",
      "  num_agent_steps_trained: 725580\n",
      "  num_env_steps_sampled: 725580\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 725580\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.07692307692307\n",
      "    ram_util_percent: 36.96153846153846\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1165672455162569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.536804389035105\n",
      "    mean_inference_ms: 10.790049657825104\n",
      "    mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79945888203886\n",
      "    episode_reward_mean: 102.78080550188763\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997,\n",
      "        103.79600335781976, 103.77745901303042, 102.46178942377085, 102.64686816008972,\n",
      "        102.64750815163215, 102.517960424526, 103.08724024642771, 101.9390098484461,\n",
      "        102.19956313822217, 103.79490998874452, 103.76939198188575, 102.46069190095136,\n",
      "        102.64560728616925, 102.64626063030612, 102.51695863948956, 103.08617925879146,\n",
      "        101.94372626370294, 102.19847721687437, 103.79499948904999, 103.76955518246866,\n",
      "        102.46080021822279, 102.6457450113279, 102.64637082543501, 102.51703578921665,\n",
      "        103.08627999088272, 101.93793493358653, 102.19857603501494, 103.79945888203886,\n",
      "        103.77210464944618, 102.46336520707935, 102.64818313853746, 102.6488286381259,\n",
      "        102.51953937389558, 103.08875934887419, 101.94035724867749, 102.20110008553476]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1165672455162569\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.536804389035105\n",
      "      mean_inference_ms: 10.790049657825104\n",
      "      mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  time_since_restore: 1331.6203603744507\n",
      "  time_this_iter_s: 9.032066822052002\n",
      "  time_total_s: 1331.6203603744507\n",
      "  timers:\n",
      "    learn_throughput: 14139.944\n",
      "    learn_time_ms: 353.891\n",
      "    synch_weights_time_ms: 8.621\n",
      "    training_iteration_time_ms: 9065.793\n",
      "  timestamp: 1662330662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 725580\n",
      "  training_iteration: 145\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 730584\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 730584\n",
      "    num_agent_steps_trained: 730584\n",
      "    num_env_steps_sampled: 730584\n",
      "    num_env_steps_trained: 730584\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79945888203886\n",
      "  episode_reward_mean: 102.78080550188763\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 144\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2308242321014404\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.146956937096547e-06\n",
      "          policy_loss: -0.11037759482860565\n",
      "          total_loss: -0.1099022775888443\n",
      "          vf_explained_var: 0.030062973499298096\n",
      "          vf_loss: 0.00047532180906273425\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 730584\n",
      "    num_agent_steps_trained: 730584\n",
      "    num_env_steps_sampled: 730584\n",
      "    num_env_steps_trained: 730584\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 730584\n",
      "  num_agent_steps_trained: 730584\n",
      "  num_env_steps_sampled: 730584\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 730584\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 75.81666666666668\n",
      "    ram_util_percent: 37.00833333333333\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1165672455162569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.536804389035105\n",
      "    mean_inference_ms: 10.790049657825104\n",
      "    mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79945888203886\n",
      "    episode_reward_mean: 102.78080550188763\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997,\n",
      "        103.79600335781976, 103.77745901303042, 102.46178942377085, 102.64686816008972,\n",
      "        102.64750815163215, 102.517960424526, 103.08724024642771, 101.9390098484461,\n",
      "        102.19956313822217, 103.79490998874452, 103.76939198188575, 102.46069190095136,\n",
      "        102.64560728616925, 102.64626063030612, 102.51695863948956, 103.08617925879146,\n",
      "        101.94372626370294, 102.19847721687437, 103.79499948904999, 103.76955518246866,\n",
      "        102.46080021822279, 102.6457450113279, 102.64637082543501, 102.51703578921665,\n",
      "        103.08627999088272, 101.93793493358653, 102.19857603501494, 103.79945888203886,\n",
      "        103.77210464944618, 102.46336520707935, 102.64818313853746, 102.6488286381259,\n",
      "        102.51953937389558, 103.08875934887419, 101.94035724867749, 102.20110008553476]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1165672455162569\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.536804389035105\n",
      "      mean_inference_ms: 10.790049657825104\n",
      "      mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  time_since_restore: 1340.6128902435303\n",
      "  time_this_iter_s: 8.99252986907959\n",
      "  time_total_s: 1340.6128902435303\n",
      "  timers:\n",
      "    learn_throughput: 14140.738\n",
      "    learn_time_ms: 353.871\n",
      "    synch_weights_time_ms: 7.387\n",
      "    training_iteration_time_ms: 9060.338\n",
      "  timestamp: 1662330671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 730584\n",
      "  training_iteration: 146\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 735588\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 735588\n",
      "    num_agent_steps_trained: 735588\n",
      "    num_env_steps_sampled: 735588\n",
      "    num_env_steps_trained: 735588\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-31-19\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79945888203886\n",
      "  episode_reward_mean: 102.78080550188763\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 144\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2343677282333374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 5.915780093346257e-06\n",
      "          policy_loss: 0.0031263858545571566\n",
      "          total_loss: 0.003293758723884821\n",
      "          vf_explained_var: 0.15774357318878174\n",
      "          vf_loss: 0.0001673771330388263\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 735588\n",
      "    num_agent_steps_trained: 735588\n",
      "    num_env_steps_sampled: 735588\n",
      "    num_env_steps_trained: 735588\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 735588\n",
      "  num_agent_steps_trained: 735588\n",
      "  num_env_steps_sampled: 735588\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 735588\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 74.25384615384615\n",
      "    ram_util_percent: 37.161538461538456\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1165672455162569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.536804389035105\n",
      "    mean_inference_ms: 10.790049657825104\n",
      "    mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79945888203886\n",
      "    episode_reward_mean: 102.78080550188763\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997,\n",
      "        103.79600335781976, 103.77745901303042, 102.46178942377085, 102.64686816008972,\n",
      "        102.64750815163215, 102.517960424526, 103.08724024642771, 101.9390098484461,\n",
      "        102.19956313822217, 103.79490998874452, 103.76939198188575, 102.46069190095136,\n",
      "        102.64560728616925, 102.64626063030612, 102.51695863948956, 103.08617925879146,\n",
      "        101.94372626370294, 102.19847721687437, 103.79499948904999, 103.76955518246866,\n",
      "        102.46080021822279, 102.6457450113279, 102.64637082543501, 102.51703578921665,\n",
      "        103.08627999088272, 101.93793493358653, 102.19857603501494, 103.79945888203886,\n",
      "        103.77210464944618, 102.46336520707935, 102.64818313853746, 102.6488286381259,\n",
      "        102.51953937389558, 103.08875934887419, 101.94035724867749, 102.20110008553476]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1165672455162569\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.536804389035105\n",
      "      mean_inference_ms: 10.790049657825104\n",
      "      mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  time_since_restore: 1349.557035446167\n",
      "  time_this_iter_s: 8.944145202636719\n",
      "  time_total_s: 1349.557035446167\n",
      "  timers:\n",
      "    learn_throughput: 14162.522\n",
      "    learn_time_ms: 353.327\n",
      "    synch_weights_time_ms: 7.448\n",
      "    training_iteration_time_ms: 9025.947\n",
      "  timestamp: 1662330679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 735588\n",
      "  training_iteration: 147\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 740592\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 740592\n",
      "    num_agent_steps_trained: 740592\n",
      "    num_env_steps_sampled: 740592\n",
      "    num_env_steps_trained: 740592\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-31-28\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79945888203886\n",
      "  episode_reward_mean: 102.78080550188763\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 144\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2363712787628174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.1350317436154e-05\n",
      "          policy_loss: -0.01322888769209385\n",
      "          total_loss: -0.013081476092338562\n",
      "          vf_explained_var: -0.008112946525216103\n",
      "          vf_loss: 0.00014740333426743746\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 740592\n",
      "    num_agent_steps_trained: 740592\n",
      "    num_env_steps_sampled: 740592\n",
      "    num_env_steps_trained: 740592\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 740592\n",
      "  num_agent_steps_trained: 740592\n",
      "  num_env_steps_sampled: 740592\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 740592\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 77.15833333333335\n",
      "    ram_util_percent: 36.93333333333334\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1165672455162569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.536804389035105\n",
      "    mean_inference_ms: 10.790049657825104\n",
      "    mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79945888203886\n",
      "    episode_reward_mean: 102.78080550188763\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997,\n",
      "        103.79600335781976, 103.77745901303042, 102.46178942377085, 102.64686816008972,\n",
      "        102.64750815163215, 102.517960424526, 103.08724024642771, 101.9390098484461,\n",
      "        102.19956313822217, 103.79490998874452, 103.76939198188575, 102.46069190095136,\n",
      "        102.64560728616925, 102.64626063030612, 102.51695863948956, 103.08617925879146,\n",
      "        101.94372626370294, 102.19847721687437, 103.79499948904999, 103.76955518246866,\n",
      "        102.46080021822279, 102.6457450113279, 102.64637082543501, 102.51703578921665,\n",
      "        103.08627999088272, 101.93793493358653, 102.19857603501494, 103.79945888203886,\n",
      "        103.77210464944618, 102.46336520707935, 102.64818313853746, 102.6488286381259,\n",
      "        102.51953937389558, 103.08875934887419, 101.94035724867749, 102.20110008553476]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1165672455162569\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.536804389035105\n",
      "      mean_inference_ms: 10.790049657825104\n",
      "      mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  time_since_restore: 1358.5253353118896\n",
      "  time_this_iter_s: 8.968299865722656\n",
      "  time_total_s: 1358.5253353118896\n",
      "  timers:\n",
      "    learn_throughput: 14187.912\n",
      "    learn_time_ms: 352.695\n",
      "    synch_weights_time_ms: 7.43\n",
      "    training_iteration_time_ms: 9009.983\n",
      "  timestamp: 1662330688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 740592\n",
      "  training_iteration: 148\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 745596\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 745596\n",
      "    num_agent_steps_trained: 745596\n",
      "    num_env_steps_sampled: 745596\n",
      "    num_env_steps_trained: 745596\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79945888203886\n",
      "  episode_reward_mean: 102.78080550188763\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 144\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.2372034788131714\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 7.105199983925559e-07\n",
      "          policy_loss: -0.017036670818924904\n",
      "          total_loss: -0.0168911200016737\n",
      "          vf_explained_var: 0.16445621848106384\n",
      "          vf_loss: 0.00014556183305103332\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 745596\n",
      "    num_agent_steps_trained: 745596\n",
      "    num_env_steps_sampled: 745596\n",
      "    num_env_steps_trained: 745596\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 745596\n",
      "  num_agent_steps_trained: 745596\n",
      "  num_env_steps_sampled: 745596\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 745596\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 73.15384615384616\n",
      "    ram_util_percent: 36.97692307692308\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1165672455162569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.536804389035105\n",
      "    mean_inference_ms: 10.790049657825104\n",
      "    mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79945888203886\n",
      "    episode_reward_mean: 102.78080550188763\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997,\n",
      "        103.79600335781976, 103.77745901303042, 102.46178942377085, 102.64686816008972,\n",
      "        102.64750815163215, 102.517960424526, 103.08724024642771, 101.9390098484461,\n",
      "        102.19956313822217, 103.79490998874452, 103.76939198188575, 102.46069190095136,\n",
      "        102.64560728616925, 102.64626063030612, 102.51695863948956, 103.08617925879146,\n",
      "        101.94372626370294, 102.19847721687437, 103.79499948904999, 103.76955518246866,\n",
      "        102.46080021822279, 102.6457450113279, 102.64637082543501, 102.51703578921665,\n",
      "        103.08627999088272, 101.93793493358653, 102.19857603501494, 103.79945888203886,\n",
      "        103.77210464944618, 102.46336520707935, 102.64818313853746, 102.6488286381259,\n",
      "        102.51953937389558, 103.08875934887419, 101.94035724867749, 102.20110008553476]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1165672455162569\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.536804389035105\n",
      "      mean_inference_ms: 10.790049657825104\n",
      "      mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  time_since_restore: 1367.6992893218994\n",
      "  time_this_iter_s: 9.173954010009766\n",
      "  time_total_s: 1367.6992893218994\n",
      "  timers:\n",
      "    learn_throughput: 14180.687\n",
      "    learn_time_ms: 352.874\n",
      "    synch_weights_time_ms: 7.482\n",
      "    training_iteration_time_ms: 9008.975\n",
      "  timestamp: 1662330698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 745596\n",
      "  training_iteration: 149\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n",
      "Result for PPO_AccelEnv-v0_1fa80_00000:\n",
      "  agent_timesteps_total: 750600\n",
      "  counters:\n",
      "    num_agent_steps_sampled: 750600\n",
      "    num_agent_steps_trained: 750600\n",
      "    num_env_steps_sampled: 750600\n",
      "    num_env_steps_trained: 750600\n",
      "  custom_metrics: {}\n",
      "  date: 2022-09-04_18-31-47\n",
      "  done: true\n",
      "  episode_len_mean: 5000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 103.79945888203886\n",
      "  episode_reward_mean: 102.78080550188763\n",
      "  episode_reward_min: 101.93793493358653\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 144\n",
      "  experiment_id: 0e814ab2ba9c41d8b7aa032a01a2cb23\n",
      "  hostname: Maxs-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 1.237789273262024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 1.468799723625125e-06\n",
      "          policy_loss: -0.061004865914583206\n",
      "          total_loss: -0.06086333468556404\n",
      "          vf_explained_var: 0.14732098579406738\n",
      "          vf_loss: 0.0001415460283169523\n",
      "        num_agent_steps_trained: 2502.0\n",
      "    num_agent_steps_sampled: 750600\n",
      "    num_agent_steps_trained: 750600\n",
      "    num_env_steps_sampled: 750600\n",
      "    num_env_steps_trained: 750600\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 127.0.0.1\n",
      "  num_agent_steps_sampled: 750600\n",
      "  num_agent_steps_trained: 750600\n",
      "  num_env_steps_sampled: 750600\n",
      "  num_env_steps_sampled_this_iter: 5004\n",
      "  num_env_steps_trained: 750600\n",
      "  num_env_steps_trained_this_iter: 5004\n",
      "  num_faulty_episodes: 0\n",
      "  num_healthy_workers: 9\n",
      "  num_recreated_workers: 0\n",
      "  num_steps_trained_this_iter: 5004\n",
      "  perf:\n",
      "    cpu_util_percent: 69.94166666666666\n",
      "    ram_util_percent: 36.975\n",
      "  pid: 84495\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1165672455162569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 4.536804389035105\n",
      "    mean_inference_ms: 10.790049657825104\n",
      "    mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  sampler_results:\n",
      "    custom_metrics: {}\n",
      "    episode_len_mean: 5000.0\n",
      "    episode_media: {}\n",
      "    episode_reward_max: 103.79945888203886\n",
      "    episode_reward_mean: 102.78080550188763\n",
      "    episode_reward_min: 101.93793493358653\n",
      "    episodes_this_iter: 0\n",
      "    hist_stats:\n",
      "      episode_lengths: [5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000,\n",
      "        5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000]\n",
      "      episode_reward: [102.19841162553591, 103.79614241723428, 103.77064000820155, 102.4618886587304,\n",
      "        102.646880882052, 102.64750124916208, 102.51810285984993, 103.08738079080894,\n",
      "        101.93897894630204, 102.19973215059784, 103.79732238534635, 103.76980686111476,\n",
      "        102.46114650815524, 102.64602645801769, 102.64668308160944, 102.51737039893044,\n",
      "        103.08662327344526, 101.93821971981195, 102.19889060005423, 103.79676209772195,\n",
      "        103.76928762724583, 102.4605848614072, 102.64549554932918, 102.64614825929499,\n",
      "        102.51686374187916, 103.08606478052307, 101.94362155570418, 102.19836109768045,\n",
      "        103.7968226749796, 103.77068989422041, 102.51700274093768, 102.64780285788922,\n",
      "        102.64806596239531, 102.5186024471645, 103.08811951417884, 101.94498233858293,\n",
      "        102.2605462926252, 103.7975513424916, 103.77024914103386, 102.4613630396782,\n",
      "        102.64646441571935, 102.64691369312487, 102.51755289982283, 103.0868646049081,\n",
      "        101.94444673847279, 102.19916200844175, 103.79696950272854, 103.77107623251682,\n",
      "        102.4607642551226, 102.64569098491162, 102.64631875318773, 102.5170526312697,\n",
      "        103.08624823474777, 101.9438032285913, 102.19854788482233, 103.79902259922984,\n",
      "        103.77333039131061, 102.46284084530903, 102.64810243215321, 102.6486015244451,\n",
      "        102.51908321756594, 103.08841024987291, 101.93988269596869, 102.2060644942997,\n",
      "        103.79600335781976, 103.77745901303042, 102.46178942377085, 102.64686816008972,\n",
      "        102.64750815163215, 102.517960424526, 103.08724024642771, 101.9390098484461,\n",
      "        102.19956313822217, 103.79490998874452, 103.76939198188575, 102.46069190095136,\n",
      "        102.64560728616925, 102.64626063030612, 102.51695863948956, 103.08617925879146,\n",
      "        101.94372626370294, 102.19847721687437, 103.79499948904999, 103.76955518246866,\n",
      "        102.46080021822279, 102.6457450113279, 102.64637082543501, 102.51703578921665,\n",
      "        103.08627999088272, 101.93793493358653, 102.19857603501494, 103.79945888203886,\n",
      "        103.77210464944618, 102.46336520707935, 102.64818313853746, 102.6488286381259,\n",
      "        102.51953937389558, 103.08875934887419, 101.94035724867749, 102.20110008553476]\n",
      "    num_faulty_episodes: 0\n",
      "    policy_reward_max: {}\n",
      "    policy_reward_mean: {}\n",
      "    policy_reward_min: {}\n",
      "    sampler_perf:\n",
      "      mean_action_processing_ms: 0.1165672455162569\n",
      "      mean_env_render_ms: 0.0\n",
      "      mean_env_wait_ms: 4.536804389035105\n",
      "      mean_inference_ms: 10.790049657825104\n",
      "      mean_raw_obs_processing_ms: 0.20923441903841158\n",
      "  time_since_restore: 1376.7065844535828\n",
      "  time_this_iter_s: 9.00729513168335\n",
      "  time_total_s: 1376.7065844535828\n",
      "  timers:\n",
      "    learn_throughput: 14169.443\n",
      "    learn_time_ms: 353.154\n",
      "    synch_weights_time_ms: 7.443\n",
      "    training_iteration_time_ms: 9004.8\n",
      "  timestamp: 1662330707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 750600\n",
      "  training_iteration: 150\n",
      "  trial_id: 1fa80_00000\n",
      "  warmup_time: 6.6942431926727295\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=84504)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84504)\u001b[0m Quitting (on error).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84503)\u001b[0m Quitting (on error).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84506)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84506)\u001b[0m Quitting (on error).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84499)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84499)\u001b[0m Quitting (on error).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84505)\u001b[0m Quitting (on error).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84498)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84498)\u001b[0m Quitting (on error).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84501)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84501)\u001b[0m Quitting (on error).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84502)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84502)\u001b[0m Quitting (on error).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84500)\u001b[0m Error: tcpip::Socket::recvAndCheck @ recv: peer shutdown\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=84500)\u001b[0m Quitting (on error).\n",
      "2022-09-04 18:31:47,368\tINFO tune.py:758 -- Total run time: 1390.78 seconds (1390.62 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PPO_AccelEnv-v0_1fa80_00000]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials = run_experiments(\n",
    "    {\n",
    "        EXP_NAME: {\n",
    "            \"run\": RL_ALG_NAME,\n",
    "            \"env\": gym_name,\n",
    "            \"config\": config.copy(),\n",
    "            \"checkpoint_freq\": 10,  # number of iterations between checkpoints\n",
    "            \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "            \"max_failures\": 999,\n",
    "            \"stop\": {  # stopping conditions\n",
    "                \"training_iteration\": 150,  # number of iterations to stop after\n",
    "            },\n",
    "        },\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trials' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/maxdumas/cornell/spec-project/flow/av4sg/figure-eight.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maxdumas/cornell/spec-project/flow/av4sg/figure-eight.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(trials[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlogdir)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxdumas/cornell/spec-project/flow/av4sg/figure-eight.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpython ../flow/visualize/visualizer_rllib.py $trials[0].logdir 10\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trials' is not defined"
     ]
    }
   ],
   "source": [
    "print(trials[0].logdir)\n",
    "!python ../flow/visualize/visualizer_rllib.py ${trials[0].logdir} 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('flow-m1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f9dffbe09ab657a5bcf2360e8f820761f031bf90cb86617d92b1245916784d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
